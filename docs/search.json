[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "STA/OPR 9750 - Course Policies & Additional Resources",
    "section": "",
    "text": "R for Data Science (R4DS) is an excellent free textbook covering much of the material of this course.\nThe tidyverse packages used throughout this course have excellent documentation:\n\nreadr\ndplyr\ntidyr\nggplot2\nrvest\n\nThe quarto guide is particularly useful.\nStudents may also benefit from the Unofficial Solutions for R4DS, the Posit R Cheatsheets, Statistical Infernece via Data Science, and the book Elegrant Graphics for Data Analysis.\nThe book Happy Git with R is particularly useful for git usage. General git usage is also covered by the Git Book.\nThe book Veridical Data Science by Yu and Barter has lots of useful advice on applied data analytics that may help with the course project.\nStudents are encouraged to ask the instructor for additional resources as needed.\n\n\nSTA/OPR 9750 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nWritten and oral communication is an important element of this course.\nThe Baruch Writing Center offers free support to all Baruch students; students can meet with a professional writing consultant one-to-one (in person, in NVC 8-185, or online, by video, audio, and text-based chat) and in group workshops. Writing Center consultants will work collaboratively with you to deepen your writing and English language skills. At any step in the process, they’ll help you become a more independent, confident, and versatile writer.1\nBaruch’s Tools for Clear Speech program improves the pronunciation, fluency, and pragmatic abilities of English language learners and non-native English speakers at Baruch College. TfCS participants achieve more effective and intelligible communication, developing skills that empower them to succeed in their classrooms, careers, and beyond. TfCS offers a range of free face-to-face and online services with our professional Speech Consultants, including One-to-One Sessions, small-group Focused Skills Series sessions, large-group Overview Workshops, interview and career preparation, and weekly Conversation Hours.\n\n\n\nAll software used in this course is Free and Open-Source Software that can be installed on your personal machine without cost. Students will need to install, at a minimum,\n\nR\nrstudio Desktop Edition\nquarto\n\nThanks to the Binder project, we are also able to provide free virtual machines equipped with all course software pre-installed:\n\nRStudio\nCommand Line Access\n\nCUNY also provides a Windows-based RStudio virtual machine through Apporto.\nPlease note that these are transient instances and any work saved on these achines may be lost without warning."
  },
  {
    "objectID": "resources.html#course-resources",
    "href": "resources.html#course-resources",
    "title": "STA/OPR 9750 - Course Policies & Additional Resources",
    "section": "",
    "text": "R for Data Science (R4DS) is an excellent free textbook covering much of the material of this course.\nThe tidyverse packages used throughout this course have excellent documentation:\n\nreadr\ndplyr\ntidyr\nggplot2\nrvest\n\nThe quarto guide is particularly useful.\nStudents may also benefit from the Unofficial Solutions for R4DS, the Posit R Cheatsheets, Statistical Infernece via Data Science, and the book Elegrant Graphics for Data Analysis.\nThe book Happy Git with R is particularly useful for git usage. General git usage is also covered by the Git Book.\nThe book Veridical Data Science by Yu and Barter has lots of useful advice on applied data analytics that may help with the course project.\nStudents are encouraged to ask the instructor for additional resources as needed.\n\n\nSTA/OPR 9750 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nWritten and oral communication is an important element of this course.\nThe Baruch Writing Center offers free support to all Baruch students; students can meet with a professional writing consultant one-to-one (in person, in NVC 8-185, or online, by video, audio, and text-based chat) and in group workshops. Writing Center consultants will work collaboratively with you to deepen your writing and English language skills. At any step in the process, they’ll help you become a more independent, confident, and versatile writer.1\nBaruch’s Tools for Clear Speech program improves the pronunciation, fluency, and pragmatic abilities of English language learners and non-native English speakers at Baruch College. TfCS participants achieve more effective and intelligible communication, developing skills that empower them to succeed in their classrooms, careers, and beyond. TfCS offers a range of free face-to-face and online services with our professional Speech Consultants, including One-to-One Sessions, small-group Focused Skills Series sessions, large-group Overview Workshops, interview and career preparation, and weekly Conversation Hours.\n\n\n\nAll software used in this course is Free and Open-Source Software that can be installed on your personal machine without cost. Students will need to install, at a minimum,\n\nR\nrstudio Desktop Edition\nquarto\n\nThanks to the Binder project, we are also able to provide free virtual machines equipped with all course software pre-installed:\n\nRStudio\nCommand Line Access\n\nCUNY also provides a Windows-based RStudio virtual machine through Apporto.\nPlease note that these are transient instances and any work saved on these achines may be lost without warning."
  },
  {
    "objectID": "resources.html#course-policies",
    "href": "resources.html#course-policies",
    "title": "STA/OPR 9750 - Course Policies & Additional Resources",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic Integrity Policy\nI fully support CUNY’s Policy on Academic Integrity, which states, in part:\n\nAcademic dishonesty is prohibited in The City University of New York. Penalties for academic dishonesty include academic sanctions, such as failing or otherwise reduced grades, and/or disciplinary sanctions, including suspension or expulsion.\n\n\nAcademic integrity is at the core of a college or university education. Faculty assign essays, exams, quizzes, projects, and so on both to extend the learning done in the classroom and as a means of assessing that learning. When students violate the academic integrity policy (i.e., “cheat”), they are committing an act of theft that can cause real harm to themselves and others including, but not limited to, their classmates, their faculty, and the caregivers who may be funding their education. Academic dishonesty confers an unfair advantage over others, which undermines educational equity and fairness. Students who cheat place their college’s accreditation and their own future prospects in jeopardy.\n\nAcademic sanctions in this class will range from an F on the Assignment to an F in this Course. A report of suspected academic dishonesty will be sent to the Office of the Dean of Students.\nStudents are encouraged to contact the instructor with any questions or concerns related to matters of academic integrity.\n\n\nExternal Resources Use Policy\nFor the coding elements of this course, students are encouraged to use freely available online resources, including question-and-answer fora such as StackOverflow. You may also use AI-driven developer tools such as GitHub Co-Pilot. Paid services are not allowed. On each assignment, you will be asked to list external resources used on each assignment. You are ultimately responsible for the correctness of any submitted materials - ``the AI told me so’’ is not a valid defense.\nNote on ChatGPT and Related Large-Language Models: You may not use large-language models to complete any assignment in this course. Specifically, you may not use tools where you describe the course assignment in natural language and receive (pseudo-)code output. While these tools are powerful, and often surprisingly accurate, for this task, using them in this manner will undermine the learning objectives of this course.\nFor the written elements of this course (e.g. Project Final Report), standard academic expectations of attribution and citation are in place. This will be covered in more detail in the course project documents.\nStudents are highly encouraged to collaborate on homework assignments, but each student is required to individually and complete each assignment. If substantially identical assignments are submitted, the instructor may require each student to individually demonstrate their understanding of the material. Collaborators should be listed at the end of each submitted assignment along with a statement of contributions.\n\n\nUnexcused Abscence Policy\nAttendance is not required, but lecture recordings will not be provided. Students are responsible for the content of all sessions missed.\n\n\nLate Work Policy\nLate work will not be accepted except in extraordinary and unforeseeable circumstances. Students submitting late work should provide supporting documentation to the Office of the Dean of Students; ODS will provide the instructor with a letter authorizing late work submission as appropriate.\nAll assignment submission technology used in this course allows multiple submissions, so students are encouraged to submit early and often to avoid any technology troubles associated with late submission.\nNote that late work is allowed consistent with specific pre-arranged course accomodations as noted below."
  },
  {
    "objectID": "resources.html#course-accomodations",
    "href": "resources.html#course-accomodations",
    "title": "STA/OPR 9750 - Course Policies & Additional Resources",
    "section": "Course Accomodations",
    "text": "Course Accomodations\n\nDisability Services\nIt is CUNY policy to provide Accommodations and Academic Adjustments to students with disabilities.\nAny student who has a disability who may need accommodations in this class should register as early as possible with Student Disability Services. Your registration with Student Disability Services is confidential, and is not recorded on your Baruch Academic Record. SDS can be reached by email at disability.services@baruch.cuny.edu, by phone at 646-312-4590, or in person at NVC 2-272.\nPlease note that the instructor cannot provide accommodations unless requested by SDS.\n\n\nReligious Accomodations\nIt is CUNY policy to provide accommodations for students’ sincerely held religious beliefs. If a religious accommodation is requested, please contact the instructor at least two weeks in advance."
  },
  {
    "objectID": "resources.html#care-resources-for-students1",
    "href": "resources.html#care-resources-for-students1",
    "title": "STA/OPR 9750 - Course Policies & Additional Resources",
    "section": "Care Resources for Students2",
    "text": "Care Resources for Students2\nTake care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress.\nAll of us benefit from support during times of struggle. You are not alone. Asking for support sooner rather than later is often helpful.\nThis course is intended to be demanding, but not difficult. If you feel like you are struggling, please reach out sooner rather than later. Swimming long-distances in choppy waters builds strength: drowning doesn’t.\n\nMental Health Resources\nIf you or anyone you know experiences significant academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support.\nThe Baruch Counselling Center is here to help. You can visit them in person at 137 E 25th St, 9th floor or call them at 646-312-2155 during normal business hours; you can make an appointment online here. For more immediate support, please call NYC WELL (1-888-NYC-WELL or 1-888-692-9355).\nAsking for help is often difficult: consider reaching out to a friend, family, or a member of the faculty you trust for help getting connected to support that can help.\nIf you are worried about a friend or classmate, consider reaching out to the Baruch Campus Intervention Team.\n\n\nPhysical Health\nHealthy CUNY promotes well-being and a culture of health in order to foster the academic and life success of all CUNY students. They can connect you with a variety of campus- and community-based healthcare providers.\nBaruch Health Services provides students with a full range of clinical health services. Call 646-312-2040 or email StudentHealthCareCenter@baruch.cuny.edu to make an appointment.\n\n\nFood Security\nAll CUNY students have access to CUNY Food Pantries located throughout the five boroughs, thanks to the CUNY CARES program. CUNY CARES is also able to help qualifying students with SNAP (“Food Stamps”) enrollment.\nOn campus, you can also access the Bearcat Food Pantry.\n\nFinancial Security\nBaruch students experiencing heightened financial stress have access to Student Emergency Grants administered through the Office of the Dean of Students. Note that funds are also available for students experiencing immigration-related financial stress.\n\n\n\nImmigration Status\nCUNY Citizenship Now! provides confidential, high-quality immigration law services to all CUNY students.\nNote that Citizenship Now!’s primary Manhattan office is located in the Heights, not on the Baruch campus and that an appointment is strongly recommended. Call 646-664-9350 during standard business hours for more information or to make an appointment"
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "STA/OPR 9750 - Course Policies & Additional Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDescriptions of Baruch and CUNY resources adapted from program websites.↩︎\nLanguage adapted from Professor Ryan Tibshirani (UC Berkeley).↩︎"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA/OPR 9750 - Course Syllabus",
    "section": "",
    "text": "Professor Michael Weylandt\nDepartment of Information Systems & Statistics\nZicklin School of Business\nBaruch College, CUNY"
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "STA/OPR 9750 - Course Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheoretically, this may result in scores equivalent to an A in an un-curved course receiving a lower grade in this course. In practice, the instructor will design course assessments to induce a range of scores and does not anticipate “down-curving” happening.↩︎"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Positive and Negative Correlation in Contingency Tables",
    "section": "",
    "text": "Suppose we have a contingency table of the following format:"
  },
  {
    "objectID": "Untitled.html#independence-chi2-testing",
    "href": "Untitled.html#independence-chi2-testing",
    "title": "Positive and Negative Correlation in Contingency Tables",
    "section": "Independence \\((\\chi^2)\\) Testing",
    "text": "Independence \\((\\chi^2)\\) Testing\nIn a first pass, we may wish to test whether \\(X_1\\) (row) is statistically independent of \\(X_2\\); in this test, our null hypothesis is that the two factors \\((X_1, X_2)\\) are uncorrelated. We don’t yet look at signs.\nWe test this by comparing the observed table (above) with an expected (null) table. To build a null table, we note\n\\[P(X_1 = +) = \\frac{n_{+\\cdot}}{n} = \\frac{n_{++} + n_{+-}}{n_{++} + n_{+-} + n_{-+} + n_{--}}\\]\nIn this case, we estimate the probability of \\(X_1\\) without looking at \\(X_2\\) - this is fine since we are assuming independence to build our null table.\nSimilarly, we have\n\\[\\begin{align*}\nP(X_1 = +) &= \\frac{n_{++} + n_{+-}}{n_{++} + n_{+-} + n_{-+} + n_{--}} \\\\\nP(X_1 = -) &= \\frac{n_{-+} + n_{--}}{n_{++} + n_{+-} + n_{-+} + n_{--}} \\\\\nP(X_2 = +) &= \\frac{n_{++} + n_{-+}}{n_{++} + n_{+-} + n_{-+} + n_{--}} \\\\\nP(X_2 = -) &= \\frac{n_{+-} + n_{--}}{n_{++} + n_{+-} + n_{-+} + n_{--}}\n\\end{align*}\\]\nWith these probabilities, and \\(n\\) total observations in the table, we have the “expected” table:\n\nExpected (Null) Table\n\n\n\n\n\n\n\n\nExpected Counts\n\\(X_2 = +\\)\n\\(X_2=-\\)\nSubtotal\n\n\n\n\n\\(X_1=+\\)\n\\(n * P(X_1=+) * P(X_2 =+)\\)\n\\(n * P(X_1=+) * P(X_2 =-)\\)\n\\(n * P(X_1 = +)\\)\n\n\n\\(X_1=-\\)\n\\(n * P(X_1=-) * P(X_2 =+)\\)\n\\(n * P(X_1=-) * P(X_2 =-)\\)\n\\(n * P(X_1 = -)\\)\n\n\nSubtotal\n\\(n * P(X_2 = +)\\)\n\\(n * P(X_2 = -)\\)\n\\(n\\)\n\n\n\nDifference between the Observed Table counts and the Expected Table counts is evidence of statistical dependence between \\(X_1, X_2\\). To put a \\(p\\)-value on it, we use either Fisher’s Exact Test or a suitable \\(\\chi^2\\) Analysis.\nThe \\(\\chi^2\\) test is based on the following test statistic:\n\\[\\chi^2 = \\sum_{ij} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\nThat is, we add up the (squared) difference between observed and expected, weighted by expected.\n\nFully Balanced Tables\nIn the case where \\(n_{\\cdot+} = n_{\\cdot-} = n_{+\\cdot} = n_{-\\cdot} = n/4\\) by construction (e.g. comparing quantile ranges of equal size), many of these calculations are particularly simple and the expected table is uniform with \\(n/4\\) in each cell."
  },
  {
    "objectID": "Untitled.html#quantifying-dependence",
    "href": "Untitled.html#quantifying-dependence",
    "title": "Positive and Negative Correlation in Contingency Tables",
    "section": "Quantifying Dependence",
    "text": "Quantifying Dependence\nTo get a sense of dependence, we can make a table of the individual \\(\\chi^2\\) sum terms: I like something like"
  },
  {
    "objectID": "Untitled.html#application-to-fdiffuse-fspn",
    "href": "Untitled.html#application-to-fdiffuse-fspn",
    "title": "Positive and Negative Correlation in Contingency Tables",
    "section": "Application to FDIFFUSE / FSPN",
    "text": "Application to FDIFFUSE / FSPN\n\nFDIFFUSE/FSPN - Observed\n\n\n\n\n\n\n\n\nObserved at 10TG\n\\(\\text{FSPN} = +\\)\n\\(\\text{FSPN} = -\\)\nSubtotal\n\n\n\n\n\\(\\text{FDIFFUSE} = +\\)\n215\n282\n497\n\n\n\\(\\text{FDIFFUSE} = -\\)\n1\n0\n1\n\n\nSubtotal\n216\n282\n498\n\n\n\nCompare this to the “expected” table\n\n\n\n\n\n\n\n\n\nExpected at 10TG\n\\(\\text{FSPN} = +\\)\n\\(\\text{FSPN} = -\\)\nSubtotal\n\n\n\n\n\\(\\text{FDIFFUSE} = +\\)\n215\n281.5\n\n\n\n\\(\\text{FDIFFUSE} = -\\)\n0.4\n0.5\n\n\n\nSubtotal\n\n\n\n\n\n\nNo real sign of a statistical difference. Counts for \\(\\text{FDIFFUSE} = -\\) are just too low to say anything."
  },
  {
    "objectID": "Untitled.html#application-to-fsds-fspn",
    "href": "Untitled.html#application-to-fsds-fspn",
    "title": "Positive and Negative Correlation in Contingency Tables",
    "section": "Application to FSDS / FSPN",
    "text": "Application to FSDS / FSPN\n\nFSDS/FSPN - Observed\n\n\n\n\n\n\n\n\nObserved at 10TG\n\\(\\text{FSPN} = +\\)\n\\(\\text{FSPN} = -\\)\nSubtotal\n\n\n\n\n\\(\\text{FSDS} = +\\)\n5\n7\n12\n\n\n\\(\\text{FSDS} = -\\)\n165\n243\n408\n\n\nSubtotal\n170\n250\n420\n\n\n\nCompare this to the “expected” table\n\n\n\n\n\n\n\n\n\nExpected at 10TG\n\\(\\text{FSPN} = +\\)\n\\(\\text{FSPN} = -\\)\nSubtotal\n\n\n\n\n\\(\\text{FSDS} = +\\)\n4.85\n7.14\n\n\n\n\\(\\text{FSDS} = -\\)\n165.14\n242.85\n\n\n\nSubtotal\n\n\n\n\n\n\nAgain - nothing really passes the sniff test for a real difference.\nQuestion Why does the grand total \\(n\\) differ between these two?"
  },
  {
    "objectID": "slides/slides07.html#mini-project-01",
    "href": "slides/slides07.html#mini-project-01",
    "title": "STA/OPR 9750 - Week 7",
    "section": "STA/OPR 9750 Mini-Project #01",
    "text": "STA/OPR 9750 Mini-Project #01\nGrades returned this afternoon.\nReview regrade policy and late work policy if you have questions."
  },
  {
    "objectID": "slides/slides07.html#mini-project-02",
    "href": "slides/slides07.html#mini-project-02",
    "title": "STA/OPR 9750 - Week 7",
    "section": "STA/OPR 9750 Mini-Project #02",
    "text": "STA/OPR 9750 Mini-Project #02\nMP#02 - Hollywood Movies – Due October 23rd\n\nGitHub post (used for peer feedback) AND Brightspace\nStart early to avoid Git issues\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness\nUse what you learned from MP#01\nPre-processed data now available as well"
  },
  {
    "objectID": "slides/slides07.html#upcoming-mini-projects",
    "href": "slides/slides07.html#upcoming-mini-projects",
    "title": "STA/OPR 9750 - Week 7",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#03: Political Analysis\nMP#04: Retirement Forecasting"
  },
  {
    "objectID": "slides/slides07.html#course-project",
    "href": "slides/slides07.html#course-project",
    "title": "STA/OPR 9750 - Week 7",
    "section": "Course Project",
    "text": "Course Project\nProposal feedback soon (need to do 2 more before releasing)"
  },
  {
    "objectID": "slides/slides07.html#pre-assignments",
    "href": "slides/slides07.html#pre-assignments",
    "title": "STA/OPR 9750 - Week 7",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is October 23rd\n\nThank you for FAQs and (honest) team feedback. Keep it coming!"
  },
  {
    "objectID": "slides/slides07.html#course-support",
    "href": "slides/slides07.html#course-support",
    "title": "STA/OPR 9750 - Week 7",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Monday + Thursday for rest of semester\nCR Tuesday + Friday\nNo OH during Thanksgiving break\n\n\nAsynchronous\n\nPiazza (\\(&lt;30\\) minute average response time)"
  },
  {
    "objectID": "slides/slides07.html#upcoming-week",
    "href": "slides/slides07.html#upcoming-week",
    "title": "STA/OPR 9750 - Week 7",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nDue Wednesday at 11:45pm:\n\nPre-Assignment #08 (Brightspace)\n\nAdvanced plotting with ggplot2\n\nMP #02 on GitHub AND Brightspace"
  },
  {
    "objectID": "slides/slides07.html#additional-resources",
    "href": "slides/slides07.html#additional-resources",
    "title": "STA/OPR 9750 - Week 7",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nC. Wilke. Fundamentals of Data Visualization\nK. Healy. Data Visualization\nH. Wickham ggplot2: Elegant Visualizations for Data Analysis\n\n\n\nB. Yu and R. Barter Veridical Data Science"
  },
  {
    "objectID": "slides/slides07.html#faq-ggplot2-vs-tableau",
    "href": "slides/slides07.html#faq-ggplot2-vs-tableau",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: ggplot2 vs Tableau",
    "text": "FAQ: ggplot2 vs Tableau\n\nTableau\n\n$$$\nIT department automatically integrates with data sources\nEasy, if it does what you want\n\nggplot2\n\nFree\nCan use arbitrary data sources, with effort\nFlexible / customizable"
  },
  {
    "objectID": "slides/slides07.html#faq-ggplot2-vs-matplotlib",
    "href": "slides/slides07.html#faq-ggplot2-vs-matplotlib",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: ggplot2 vs matplotlib",
    "text": "FAQ: ggplot2 vs matplotlib\n\nggplot2\n\nData visualizations\nEnforces “good practice” via gg\n\nmatplotlib\n\nScientific visualizations\nMore flexible for good or for ill\nInspired by Matlab plotting\n\n\nClosest Python analogue to ggplot2 is seaborn"
  },
  {
    "objectID": "slides/slides07.html#faq-why-use-instead-of",
    "href": "slides/slides07.html#faq-why-use-instead-of",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Why use + instead of |>",
    "text": "FAQ: Why use + instead of |&gt;\n\nggplot2 is older than |&gt;\nPer H. Wickham: if ggplot3 ever gets made, will use |&gt;\nUnlikely to change: too much code depends on it"
  },
  {
    "objectID": "slides/slides07.html#faq-performance",
    "href": "slides/slides07.html#faq-performance",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Performance",
    "text": "FAQ: Performance\n\nI tried an interactive plot with \\(n=132,000\\) points, but it brought my computer to a halt. [Ed. Paraphrased]\n\nThat’s a lot of plots!!\nggplot2 is itself pretty fast, but it depends on (possibly slow) graphics backends\n\nDifferent file types implement graphics differently.\nYou should also think about overplotting / pre-processing"
  },
  {
    "objectID": "slides/slides07.html#faq-overplotting",
    "href": "slides/slides07.html#faq-overplotting",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Overplotting",
    "text": "FAQ: Overplotting\nLarge data sets can lead to overplotting:\n\nPoints “on top of” each other\nCan also occur with “designed” experiments / rounded data\n\nWays to address:\n\ngeom_jitter\ngeom_hex"
  },
  {
    "objectID": "slides/slides07.html#faq-overplotting-1",
    "href": "slides/slides07.html#faq-overplotting-1",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Overplotting",
    "text": "FAQ: Overplotting\nJitter: add a bit of random noise so points don’t step on each other\n\nlibrary(ggplot2); library(patchwork)\np &lt;- ggplot(mpg, aes(cyl, hwy))\np1 &lt;- p + geom_point() + ggtitle(\"geom_point\")\np2 &lt;- p + geom_jitter() + ggtitle(\"geom_jitter\")\np1 + p2"
  },
  {
    "objectID": "slides/slides07.html#faq-hexagonal-binning",
    "href": "slides/slides07.html#faq-hexagonal-binning",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Hexagonal Binning",
    "text": "FAQ: Hexagonal Binning\nLittle “heatmaps” of counts. Hexagons to avoid weird rounding artifacts\n\nlibrary(ggplot2); library(patchwork)\np &lt;- ggplot(diamonds, aes(carat, price))\np1 &lt;- p + geom_point() + ggtitle(\"geom_point\")\np2 &lt;- p + geom_hex() + ggtitle(\"geom_hex\")\np1 + p2"
  },
  {
    "objectID": "slides/slides07.html#faq-inside-vs.-outside-aes",
    "href": "slides/slides07.html#faq-inside-vs.-outside-aes",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Inside vs. Outside aes()",
    "text": "FAQ: Inside vs. Outside aes()\naes maps data to values. Outside of aes, set constant value\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, \n       aes(x=bill_length_mm, y=bill_depth_mm, color=species))+ geom_point()"
  },
  {
    "objectID": "slides/slides07.html#faq-inside-vs.-outside-aes-1",
    "href": "slides/slides07.html#faq-inside-vs.-outside-aes-1",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Inside vs. Outside aes()",
    "text": "FAQ: Inside vs. Outside aes()\naes maps data to values. Outside of aes, set constant value\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, \n       aes(x=bill_length_mm, y=bill_depth_mm))+ geom_point(color=\"blue\")"
  },
  {
    "objectID": "slides/slides07.html#faq-global-vs-geom_-specific-aes",
    "href": "slides/slides07.html#faq-global-vs-geom_-specific-aes",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Global vs geom_ specific aes()",
    "text": "FAQ: Global vs geom_ specific aes()\n\nElements set in ggplot() apply to entire plot\nElements set in specific geom apply there only\n\nOverride globals\n\n\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, \n       aes(x=bill_length_mm, y=bill_depth_mm, color=species))+\n    geom_smooth() + \n    geom_point(color=\"blue\")"
  },
  {
    "objectID": "slides/slides07.html#faq-how-to-choose-plot-types",
    "href": "slides/slides07.html#faq-how-to-choose-plot-types",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: How to choose plot types",
    "text": "FAQ: How to choose plot types\nTwo “modes”\n\nExploratory data analysis. Quick, rapid iteration, for your eyes only\n\nLet the data tell you a story\nLow pre-processing: scatter plots, lines, histograms\n\n“Publication quality”. Polished,\n\nYou tell the reader a story\nMore processing, more modeling: trends, line segments, ribbons"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes",
    "href": "slides/slides07.html#faq-color-palettes",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\nThree types of color palettes:\n\nSequential: ordered from 0 to “high”\n\nExample: rain forecast in different areas\n\nDiverging: ordered from -X to +X with meaningful 0 in the middle\n\nExample: political leaning\n\nQualitative: no ordering\n\n\nWhen mapping quantitative variables to palettes (sequential/diverging), two approaches:\n\nBinned: \\([0, 1)\\) light green, \\([1, 3)\\) medium green; \\([3, 5]\\) dark green\nContinuous"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-1",
    "href": "slides/slides07.html#faq-color-palettes-1",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_distiller(type=\"seq\") # Continuous"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-2",
    "href": "slides/slides07.html#faq-color-palettes-2",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_fermenter(type=\"seq\") # Binned"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-3",
    "href": "slides/slides07.html#faq-color-palettes-3",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_fermenter(type=\"seq\") # Binned + Sequential"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-4",
    "href": "slides/slides07.html#faq-color-palettes-4",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() +\n    scale_color_fermenter(type=\"qual\") # Binned + Qualitative"
  },
  {
    "objectID": "slides/slides07.html#faq-color-palettes-5",
    "href": "slides/slides07.html#faq-color-palettes-5",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Color Palettes",
    "text": "FAQ: Color Palettes\n\nlibrary(ggplot2); library(palmerpenguins)\nggplot(penguins, aes(x=bill_length_mm, y=bill_depth_mm, color=body_mass_g)) + \n    geom_point() + theme_bw() + \n    scale_color_fermenter(type=\"div\") # Binned + Diverging"
  },
  {
    "objectID": "slides/slides07.html#faq-how-to-hard-code-colors",
    "href": "slides/slides07.html#faq-how-to-hard-code-colors",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: How to “hard-code” colors",
    "text": "FAQ: How to “hard-code” colors\n\nlibrary(dplyr)\ndata &lt;- data.frame(x = rnorm(5), \n                   y = rnorm(5), \n                   group = c(\"a\", \"a\", \"b\", \"b\", \"b\"))\n\ndata |&gt; \n    group_by(group) |&gt;\n    mutate(n_count = n()) |&gt;\n    ungroup() |&gt;\n    mutate(color = ifelse(n_count == max(n_count), \"red\", \"black\")) |&gt;\n    ggplot(aes(x=x, y=y, shape=group, color=color)) + \n    geom_point() + \n    scale_color_identity()"
  },
  {
    "objectID": "slides/slides07.html#faq-how-to-customize-themes",
    "href": "slides/slides07.html#faq-how-to-customize-themes",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: How to Customize Themes",
    "text": "FAQ: How to Customize Themes\nBuilt-in themes + ggthemes package:\n\nlibrary(ggplot2); library(ggthemes); \nlibrary(palmerpenguins); library(ggpmisc)\np &lt;- ggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    facet_wrap(~species)"
  },
  {
    "objectID": "slides/slides07.html#faq-themes",
    "href": "slides/slides07.html#faq-themes",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nDefault theme (ggplot2::theme_grey()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-1",
    "href": "slides/slides07.html#faq-themes-1",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nBlack and White theme (ggplot2::theme_bw()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-2",
    "href": "slides/slides07.html#faq-themes-2",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nMinimal theme (ggplot2::theme_minimal()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-3",
    "href": "slides/slides07.html#faq-themes-3",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nLight theme (ggplot2::theme_light()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-4",
    "href": "slides/slides07.html#faq-themes-4",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nDark theme (ggplot2::theme_dark()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-5",
    "href": "slides/slides07.html#faq-themes-5",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nExcel theme (ggthemes::theme_excel()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-6",
    "href": "slides/slides07.html#faq-themes-6",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nGoogle Docs theme (ggthemes::theme_gdocs()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-7",
    "href": "slides/slides07.html#faq-themes-7",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nThe Economist theme (ggthemes::theme_economist()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-8",
    "href": "slides/slides07.html#faq-themes-8",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nThe Economist theme (ggthemes::theme_economist()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-9",
    "href": "slides/slides07.html#faq-themes-9",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nSolarized theme (ggthemes::theme_solarized()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-10",
    "href": "slides/slides07.html#faq-themes-10",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nSolarized2 theme (ggthemes::theme_solarized_2()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-11",
    "href": "slides/slides07.html#faq-themes-11",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nStata theme (ggthemes::theme_stata()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-12",
    "href": "slides/slides07.html#faq-themes-12",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nTufte theme (ggthemes::theme_tufte()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-13",
    "href": "slides/slides07.html#faq-themes-13",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nWall Street Journal theme (ggthemes::theme_wsj()):"
  },
  {
    "objectID": "slides/slides07.html#faq-themes-14",
    "href": "slides/slides07.html#faq-themes-14",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Themes",
    "text": "FAQ: Themes\nMany more online:\n\nThemePark for movie themes"
  },
  {
    "objectID": "slides/slides07.html#faq-order-of-layers",
    "href": "slides/slides07.html#faq-order-of-layers",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Order of Layers",
    "text": "FAQ: Order of Layers\nOrder of layers technically matters, but the effect is small\n\np1 &lt;- ggplot(penguins, aes(x=bill_length_mm, y=flipper_length_mm)) +\n        geom_point(color=\"black\") + \n        geom_smooth(color=\"blue\", method=\"lm\") + ggtitle(\"Line on points\")\np2 &lt;- ggplot(penguins, aes(x=bill_length_mm, y=flipper_length_mm)) +\n        geom_smooth(color=\"blue\", method=\"lm\") + \n        geom_point(color=\"black\") + ggtitle(\"Points on line\")\np1 + p2"
  },
  {
    "objectID": "slides/slides07.html#faq-order-of-layers-1",
    "href": "slides/slides07.html#faq-order-of-layers-1",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Order of layers",
    "text": "FAQ: Order of layers\nOrder matters more with theme. Adding a theme_*() will override any theme() customization you did:\n\np1 &lt;- p + theme_bw() + theme(legend.position=\"bottom\")\np2 &lt;- p + theme(legend.position=\"bottom\") + theme_bw() \np1 + p2"
  },
  {
    "objectID": "slides/slides07.html#faq-stat_poly_lineeq-vs-geom_smooth",
    "href": "slides/slides07.html#faq-stat_poly_lineeq-vs-geom_smooth",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: stat_poly_{line,eq} vs geom_smooth",
    "text": "FAQ: stat_poly_{line,eq} vs geom_smooth\nBy default geom_smooth fits a generalized additive model (GAM)\nggpmisc::stat_poly_{line,eq} fit linear models, so they can expose more machinery.\n\nWhat is a GAM? Take 9890 with me (Spring, Tuesdays at 6) to find out!"
  },
  {
    "objectID": "slides/slides07.html#faq-titles-and-captions",
    "href": "slides/slides07.html#faq-titles-and-captions",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Titles and Captions",
    "text": "FAQ: Titles and Captions\n\nggplot() + \n    labs(title=\"Title\", subtitle=\"Subtitle\", caption=\"Caption\",\n         tag=\"Tag\", alt=\"Alt-Text\", alt_insight=\"Alt-Insight\")\n\n\n+ggtitle(\"text\") is just shorthand for +labs(title=\"text\")"
  },
  {
    "objectID": "slides/slides07.html#faq-relative-importance-of-aesthetics",
    "href": "slides/slides07.html#faq-relative-importance-of-aesthetics",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Relative Importance of Aesthetics",
    "text": "FAQ: Relative Importance of Aesthetics\nPerceptually:\n\nLocation &gt; Color &gt; Size &gt; Shape\n\nHumans are better at:\n\nLength &gt; Area &gt; Volume"
  },
  {
    "objectID": "slides/slides07.html#faq-when-to-use-facets",
    "href": "slides/slides07.html#faq-when-to-use-facets",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: When to Use Facets?",
    "text": "FAQ: When to Use Facets?\nFacets are group_by for plots. Useful for\n\nDistinguishing intra- vs inter-group trends\nAvoiding overplotting"
  },
  {
    "objectID": "slides/slides07.html#faq-simpsons-paradox",
    "href": "slides/slides07.html#faq-simpsons-paradox",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Simpson’s Paradox",
    "text": "FAQ: Simpson’s Paradox"
  },
  {
    "objectID": "slides/slides07.html#faq-simpsons-paradox-1",
    "href": "slides/slides07.html#faq-simpsons-paradox-1",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Simpson’s Paradox",
    "text": "FAQ: Simpson’s Paradox"
  },
  {
    "objectID": "slides/slides07.html#faq-twin-axes-plots",
    "href": "slides/slides07.html#faq-twin-axes-plots",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Twin Axes Plots",
    "text": "FAQ: Twin Axes Plots\n\nHow can I implement a dual (twin) axis plot in ggplot2?\n\nDisfavored. But if you must …\nsec.axis\nDoesn’t allow arbitrary secondary axes; allows transformed axes (e.g., Celsius and Fahrenheit)"
  },
  {
    "objectID": "slides/slides07.html#faq-embedding-images-in-ggplot",
    "href": "slides/slides07.html#faq-embedding-images-in-ggplot",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Embedding images in ggplot",
    "text": "FAQ: Embedding images in ggplot\nSee the ggimage or ggflags package for images as “points”:\n\n#devtools::install_github(\"jimjam-slam/ggflags\"); \nlibrary(ggflags)\nd &lt;- data.frame(x=rnorm(50), y=rnorm(50), \n                country=sample(c(\"ar\",\"fr\", \"nz\", \"gb\", \"es\", \"ca\"), 50, TRUE), \n                stringsAsFactors = FALSE)\nggplot(d, aes(x=x, y=y, country=country, size=x)) + \n  geom_flag() + \n  scale_country()"
  },
  {
    "objectID": "slides/slides07.html#faq-embedding-images",
    "href": "slides/slides07.html#faq-embedding-images",
    "title": "STA/OPR 9750 - Week 7",
    "section": "FAQ: Embedding Images",
    "text": "FAQ: Embedding Images\nSee cowplot::draw_image() for image background:\n\nlibrary(cowplot)\np &lt;- ggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_half_open(12)\n\nlogo_file &lt;- system.file(\"extdata\", \"logo.png\", package = \"cowplot\")\nggdraw() +\n  draw_image(\n    logo_file, scale = .7\n  ) +\n  draw_plot(p)"
  },
  {
    "objectID": "slides/slides07.html#diving-deeper-with-ggplot2",
    "href": "slides/slides07.html#diving-deeper-with-ggplot2",
    "title": "STA/OPR 9750 - Week 7",
    "section": "Diving Deeper with ggplot2",
    "text": "Diving Deeper with ggplot2\nData Sets:\n\ndiamonds from the ggplot2 package\ncdiac from the CVXR package\ngapminder from the gapminder package\n\nYou need to install CVXR and gapminder now.\nExercise: Lab #07"
  },
  {
    "objectID": "slides/slides07.html#diving-deeper-with-ggplot2-learning-goals",
    "href": "slides/slides07.html#diving-deeper-with-ggplot2-learning-goals",
    "title": "STA/OPR 9750 - Week 7",
    "section": "Diving Deeper with ggplot2: Learning Goals",
    "text": "Diving Deeper with ggplot2: Learning Goals\nToday:\n\nFluency with basic geoms\nAnimation\n\nNext Week:\n\nSpatial data\nInteractive graphics\nDashboards (time allowing)"
  },
  {
    "objectID": "slides/slides07.html#breakout-rooms",
    "href": "slides/slides07.html#breakout-rooms",
    "title": "STA/OPR 9750 - Week 7",
    "section": "Breakout Rooms",
    "text": "Breakout Rooms\n\n\n\nRoom\nTeam\n\nRoom\nTeam\n\n\n\n\n1\nRat Pack\n\n6\nCa$h VZ\n\n\n2\nSubway Surfers\n\n7\nListing Legends\n\n\n3\nChart Toppers\n\n8\nTDSSG\n\n\n4\nMetro Mindset\n\n9\nBroker T’s\n\n\n5\nApple Watch\n\n10\nEVengers"
  },
  {
    "objectID": "slides/slides04.html#mini-project-00",
    "href": "slides/slides04.html#mini-project-00",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "STA/OPR 9750 Mini-Project #00",
    "text": "STA/OPR 9750 Mini-Project #00\nThank you to those of you who provided peer feedback!\n(Over 75% of the class reported receiving useful peer feedback.)\n\nInstructor’s Note: For graded MPs #01-04, be a bit more direct in peer feedback. Goal is to help your peers improve: constructive criticism.\n\n\nA few of you still haven’t completed MP#00. Too late for peer feedback, but you need to get this done in order to submit MP#01.\nNo late work accepted on graded MPs."
  },
  {
    "objectID": "slides/slides04.html#mini-project-01",
    "href": "slides/slides04.html#mini-project-01",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "STA/OPR 9750 Mini-Project #01",
    "text": "STA/OPR 9750 Mini-Project #01\nMP#01 released - Transit System Financials\n\nDue September 25th\n\nGitHub post (used for peer feedback) AND Brightspace\n\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness, not necessarily correctness"
  },
  {
    "objectID": "slides/slides04.html#mp01-corrections",
    "href": "slides/slides04.html#mp01-corrections",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "MP#01 Corrections",
    "text": "MP#01 Corrections\nThanks to EL and JA for finding two mistakes in MP statement:\n\nApples/Oranges problem in “longest average trip” (JA)\nData cleaning problem in FARES table (EL)\n\nWill fix after class:\n\nSkip longest average trip question\nBetter instructor-provided code for FARES table"
  },
  {
    "objectID": "slides/slides04.html#upcoming-mini-projects",
    "href": "slides/slides04.html#upcoming-mini-projects",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nMP#02 assigned next week:\n\nHollywood Development Executive Case Study\nMP#03: Political Analysis (tentative)\nMP#04: Retirement Forecasting (tentative)"
  },
  {
    "objectID": "slides/slides04.html#pre-assignments",
    "href": "slides/slides04.html#pre-assignments",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades.\n\nI have to manually change to completion grading."
  },
  {
    "objectID": "slides/slides04.html#course-project",
    "href": "slides/slides04.html#course-project",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "Course Project",
    "text": "Course Project\n3 teams already formed!\n\nBreakout rooms in teams\n\nTeam/Room 1: GZ + VF + EY + AG + TD\nTeam/Room 2: YZ + HM + TN + NG\nTeam/Room 3: SK + HA + DS\n\nAll team commitments due 2024-10-02"
  },
  {
    "objectID": "slides/slides04.html#graduate-teaching-assistant-gta",
    "href": "slides/slides04.html#graduate-teaching-assistant-gta",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "Graduate Teaching Assistant (GTA)",
    "text": "Graduate Teaching Assistant (GTA)\n\nCharles Ramirez\nTwice Weekly Office Hours (Zoom - Links of Brightspace)\n\nTuesdays 4-5pm\nFridays 12-1pm\n\nWill also help coordinate peer feedback (GitHub), Piazza responses, etc.\nExcellent resource for course project advice!"
  },
  {
    "objectID": "slides/slides04.html#upcoming-week",
    "href": "slides/slides04.html#upcoming-week",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nNext Wednesday at 11:45pm:\n\nNext Pre-Assignment\nMP#01 Initial Submission due"
  },
  {
    "objectID": "slides/slides04.html#faq-select-",
    "href": "slides/slides04.html#faq-select-",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "FAQ: select(-)",
    "text": "FAQ: select(-)\ndata |&gt; select(colname) keeps colname, dropping everything else\ndata |&gt; select(-colname) drops colname, keeping everything else\nDropping is mainly useful for\n\nPresentation (removing unwanted columns)\nAdvanced:\n\nOperations across columns"
  },
  {
    "objectID": "slides/slides04.html#faq-filter-vs-group_by",
    "href": "slides/slides04.html#faq-filter-vs-group_by",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "FAQ: filter vs group_by",
    "text": "FAQ: filter vs group_by\ngroup_by is an adverb. On its own, it does nothing; it changes the behavior of later functionality.\n\npenguins |&gt; drop_na() |&gt; print(n=2)\n\n# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n# ℹ 331 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\npenguins |&gt; drop_na() |&gt; group_by(species) |&gt; print(n=2)\n\n# A tibble: 333 × 8\n# Groups:   species [3]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n# ℹ 331 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/slides04.html#faq-order-of-group_by",
    "href": "slides/slides04.html#faq-order-of-group_by",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "FAQ: Order of group_by",
    "text": "FAQ: Order of group_by\n\nNo change to first “grouped” operations\nChange in grouping structure of result\nLast group “removed” by summarize\nNo impact on grouped operations performed by mutate or filter"
  },
  {
    "objectID": "slides/slides04.html#faq-ungroup",
    "href": "slides/slides04.html#faq-ungroup",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "FAQ: ungroup",
    "text": "FAQ: ungroup\n\nRemove all grouping structure\nDefensive to keep group structure from “propogating” unwantedly\n\n\nsum_penguins &lt;- penguins |&gt; \n    group_by(sex, species) |&gt; \n    summarize(mbmg = mean(body_mass_g))\n\n... # Lots of code \n\nsum_penguins |&gt; filter(mbmg == max(mbmg)) # Still grouped!!"
  },
  {
    "objectID": "slides/slides04.html#faq-named-arguments-in-mutate-and-summarize",
    "href": "slides/slides04.html#faq-named-arguments-in-mutate-and-summarize",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "FAQ: Named Arguments in mutate and summarize",
    "text": "FAQ: Named Arguments in mutate and summarize\nmutate and summarize create new columns:\n\nmutate creates “one-to-one”\nsummarize creates “one-per-group”"
  },
  {
    "objectID": "slides/slides04.html#faq-pipe-syntax",
    "href": "slides/slides04.html#faq-pipe-syntax",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "FAQ: Pipe Syntax",
    "text": "FAQ: Pipe Syntax\nPipe syntax (|&gt;) is “syntactic sugar”\nJust makes code easier to read:\n\npenguins |&gt; group_by(species) |&gt; summarize(n_species = n())\n# vs\nsummarize(group_by(penguins, species), n_species=n())\n\nExactly the same execution: improved UX"
  },
  {
    "objectID": "slides/slides04.html#faq-assignment-of-pipeline-results",
    "href": "slides/slides04.html#faq-assignment-of-pipeline-results",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "FAQ: Assignment of Pipeline Results",
    "text": "FAQ: Assignment of Pipeline Results\nWhen to start a pipeline with NAME &lt;-? Creating a new variable:\n\n\nData you intend to reuse\nAssignment operator ‘up front’ indicates important\nMy rules of thumb for names:\n\nNew names for “new complete thoughts” - whole summary in one pipeline\nOverwrite existing names for “like-for-like improvements” (USAGE &lt;- USAGE |&gt; code(...))\n\nRecoding variable names, fixing typos, etc.\nUse name repeatedly so downstream code picks up effects ‘for free’"
  },
  {
    "objectID": "slides/slides04.html#faq-comparison-with-sql-and-pandas-python",
    "href": "slides/slides04.html#faq-comparison-with-sql-and-pandas-python",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "FAQ: Comparison with SQL and Pandas (Python)",
    "text": "FAQ: Comparison with SQL and Pandas (Python)\ndplyr is heavily inspired by SQL (standard query language for data bases)\n\nMW (2014): “Why bother? Can’t folks just use SQL”\n\npandas (in Python) inspired by R data.frame and SQL:\n\nA bit older than dplyr (cousins?)\n“New hotness” (polars) directly inspired by dplyr"
  },
  {
    "objectID": "slides/slides04.html#faq-performance",
    "href": "slides/slides04.html#faq-performance",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "FAQ: Performance",
    "text": "FAQ: Performance\ndplyr is fast, but advanced options:\n\ndbplyr: translates dplyr syntax to SQL and executes in DB\ndtplyr: uses alternate data.table back-end (HFT)\n\nHard to have bad performance in single-table analysis\n\nDanger of accidentally creating ‘extra’ data in multi-table context\nWill discuss more next week"
  },
  {
    "objectID": "slides/slides04.html#diving-deeper-with-group_by-filter-and-summarize",
    "href": "slides/slides04.html#diving-deeper-with-group_by-filter-and-summarize",
    "title": "STA/OPR 9750 - Week 4 Update",
    "section": "Diving Deeper with group_by, filter, and summarize",
    "text": "Diving Deeper with group_by, filter, and summarize\nData Set: nycflights13\nExercise: Lab #04"
  },
  {
    "objectID": "slides/slides02.html#week-2-update",
    "href": "slides/slides02.html#week-2-update",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "STA/OPR 9750 Week 2 Update",
    "text": "STA/OPR 9750 Week 2 Update\n\nWeekly feature\nBrief updates and reminders about course logistics\nSyllabus and Brightspace are binding\n\nIf something is left out of here, it still happens!"
  },
  {
    "objectID": "slides/slides02.html#course-administration-google-calendar-help",
    "href": "slides/slides02.html#course-administration-google-calendar-help",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Course Administration: Google Calendar Help",
    "text": "Course Administration: Google Calendar Help\nThanks to WP (Piazza #15) for delving into Google Calendar formatting\nI’ve updated the course homepage to provide a CSV file with all course deadlines suitable for import to Google Calendar."
  },
  {
    "objectID": "slides/slides02.html#course-administration-course-project-description-released",
    "href": "slides/slides02.html#course-administration-course-project-description-released",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Course Administration: Course Project Description Released",
    "text": "Course Administration: Course Project Description Released\nCourse project description is now online\nDetailed discussion of:\n\nProject structure\nKey deadlines\nGrading rubrics\n\nFirst step: by October 2nd, email me your group members."
  },
  {
    "objectID": "slides/slides02.html#course-enrollment",
    "href": "slides/slides02.html#course-enrollment",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Course Enrollment",
    "text": "Course Enrollment\nTotal enrollment: 46 = 33 (STA) + 13 (OPR)\n\nCourses \\(\\geq\\) 46 are ‘doubles’ in Zicklin\n\nAllows additional instructional resources\nThanks to (unknown) #46\n\nI expect \\(\\approx 12\\) final project teams"
  },
  {
    "objectID": "slides/slides02.html#graduate-teaching-assistant-gta",
    "href": "slides/slides02.html#graduate-teaching-assistant-gta",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Graduate Teaching Assistant (GTA)",
    "text": "Graduate Teaching Assistant (GTA)\n\nGTA hiring in process - name TBA\nGTA to hold weekly virtual office hours (likely 2x)\n\nDoodle Poll to schedule"
  },
  {
    "objectID": "slides/slides02.html#piazza",
    "href": "slides/slides02.html#piazza",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Piazza",
    "text": "Piazza\n\n28 sign-ups: 18 still need to sign up\nGood discussion of GitHub security problems\n4 final project teammate searches underway\n\nGreat tool!\nSay what you’re interested in as a project topic to help coordination"
  },
  {
    "objectID": "slides/slides02.html#pre-assignments",
    "href": "slides/slides02.html#pre-assignments",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nPre-Assignment 02: - 40 / 46 submitted - Ignore “5/10 grading” - limitation of Brightspace - I manually adjust\nPre-Assignment 03: - Before midnight: Available on website + Brightspace"
  },
  {
    "objectID": "slides/slides02.html#mini-project-00",
    "href": "slides/slides02.html#mini-project-00",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Mini-Project 00",
    "text": "Mini-Project 00\n\n7 submissions via Piazza (thank you - I will respond soon)\nDue in (slightly less than) a week\nPossible tech issues, so start early"
  },
  {
    "objectID": "slides/slides02.html#course-bot",
    "href": "slides/slides02.html#course-bot",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Course Bot",
    "text": "Course Bot\nBecause this course is a double, I have resources to create a “bot” to help with course organization on GitHub.\nBot will begin to acknowledge completed MP#00 over the weekend.\nCurrent name: CISSOID: CIS and Statistics bot for Organizing Instructional Delivery\n\nParticular mathematical shape\nName means “Ivy-like”:\n\nUse technology to overcome resource limitations of CUNY\n\n\nBetter name suggestions very welcome!"
  },
  {
    "objectID": "slides/slides02.html#today",
    "href": "slides/slides02.html#today",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Today",
    "text": "Today\n\nReview of Questions from Pre-Assign #02\nCourse Project Overview\nIntroduction to Markdown and Quarto\nIntroduction to GitHub pages\nHow to ask for help"
  },
  {
    "objectID": "slides/slides02.html#q1",
    "href": "slides/slides02.html#q1",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Q1",
    "text": "Q1\n\nWhat is Markdown?\n\nPer Wikipedia: “Markdown is a light-weight, plain-text, markup language specification”\n\n\nLight-weight: relatively simple, focus on content than formatting\nPlain-text: accessible using almost any text editor (RStudio, GitHub, VS Code, etc)\n\nNot locked into specific software (e.g., MS Word)\nEasily incorporated into a variety of technologies"
  },
  {
    "objectID": "slides/slides02.html#q1-1",
    "href": "slides/slides02.html#q1-1",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Q1",
    "text": "Q1\n\nWhat is Markdown?\n\n\n\nMarkup language: a ‘mini-coding language’ for text documents\n\nOther famous examples: HTML, XML\n\nSpecification:\n\nCommonMark defines ‘standard’ Markdown\nSome software allows extensions\nPandoc often powers under the hood"
  },
  {
    "objectID": "slides/slides02.html#q2",
    "href": "slides/slides02.html#q2",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Q2",
    "text": "Q2\n\nOther than text formatting, does Markdown ha[ve] any other us[]es?\n\nOn its own, Markdown is just text formatting (but that’s a lot!)\n\nWe will use Quarto which augments markdown for reproducible research. We can embed code-and its output-inside Markdown documents."
  },
  {
    "objectID": "slides/slides02.html#q3",
    "href": "slides/slides02.html#q3",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Q3",
    "text": "Q3\n\n[W]hat documents use[] Markdown?\n\nSo much! Markdown is used by Bitbucket, GitHub, OpenStreetMap, Reddit, Stack Exchange, Drupal, ChatGPT, Discord, MS Teams and many more!\n\nWith tools like Pandoc/Quarto, Markdown can be rendered to:\n\n\n\nHTML\nPDF\nWeb Slides\nEBooks\n\n\n\nResearch Papers\nWord Documents\nPowerPoint slides\nand so much more!"
  },
  {
    "objectID": "slides/slides02.html#q4",
    "href": "slides/slides02.html#q4",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Q4",
    "text": "Q4\n\n[What is] the difference between [a] Code section and [a] Nested List[? A]re they just different ways of indenting?\n\nNo. Nested lists are ‘just’ text\nCode formatting enables much more if rendering engine supports it:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "slides/slides02.html#q5",
    "href": "slides/slides02.html#q5",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Q5",
    "text": "Q5\n\n[H]ow are we going to use Markdown?\n\nAll written work (mini-projects and final project) in this course will be submitted using Markdown (by way of Quarto).\n\nSpecifically: - Submission pages for 5 mini-projects - Individual reports for course project - Summary (team) report for final project\nYou are also encouraged (but not required) to use Markdown for presentation slides (like these!)"
  },
  {
    "objectID": "slides/slides02.html#q6",
    "href": "slides/slides02.html#q6",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Q6",
    "text": "Q6\n\nHow can I create Tables in Markdown?\n\nMarkdown has two table syntaxes:\n\nan easy one with minimal control\na hard one which allows fine grained control (alignment, column widths, etc.)\n\nIf you are using the advanced (“pipe table”) synatx, I suggest you use RStudio’s Visual editor mode. (DEMO!)"
  },
  {
    "objectID": "slides/slides02.html#q7",
    "href": "slides/slides02.html#q7",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Q7",
    "text": "Q7\n\nHow to create images and links?\n\nBasic hyperlinks look like this:\n[link text](https://the.url/goes/here)\n\nIf you want to embed the contents of a link, prepend it with an exclamation point. This is most useful for images:\n![Image Caption](https://the.url/goes/here.png)\n\n\nYou can even put a link inside an image to be fancy:\n[![Elephant](elephant.png)](https://en.wikipedia.org/wiki/Elephant)"
  },
  {
    "objectID": "slides/slides02.html#q7-1",
    "href": "slides/slides02.html#q7-1",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Q7",
    "text": "Q7\n\nHow to create images and links?\n\nQuarto automatically embeds the results of plotting code:\n\nplot(1:5, main=\"Behold, a Plot!\", col=2:6, cex=5, \n     pch=16, xlab=\"\", cex.main=5)\n\n\nHere, Quarto handles all the file creation and link targeting for us. If I change the code, the figure will change automatically."
  },
  {
    "objectID": "slides/slides02.html#course-project",
    "href": "slides/slides02.html#course-project",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Course Project",
    "text": "Course Project\n\nTeams: 3-5 classmates (either section)\nStages:\n\nProposal (in class presentation)\nMid-semester check-in (in class presentation)\nFinal: in class presentation, individual report, summary report\n\nStructure:\n\nShared “Overarching Question”\nIndividual “Specific Question”\n\n\nFull description online"
  },
  {
    "objectID": "slides/slides02.html#finding-data",
    "href": "slides/slides02.html#finding-data",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Finding Data",
    "text": "Finding Data\n\nStart early!\nNYC Open Data is great\n\nSee also: FRED, Federal Open Data, Nasa EarthData, Kaggle\nAsk on Piazza for pointers\nLots of data hidden in Wikipedia\n\nNothing paid or private without express instructor submission\nEveryone loves spatial data!"
  },
  {
    "objectID": "slides/slides02.html#presentation-hints",
    "href": "slides/slides02.html#presentation-hints",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Presentation Hints",
    "text": "Presentation Hints\n\nLongest time \\(\\neq\\) most important\nStory, story, story! Why are you making these choices?\nHourglass Structure\n\nStart big\nMotivate your overarching question\nSpecific questions\nTie specific to overarching\nFrom overarching back to big motivation\n\nNo less than one figure every other slide"
  },
  {
    "objectID": "slides/slides02.html#markdown-and-quarto-1",
    "href": "slides/slides02.html#markdown-and-quarto-1",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Markdown and Quarto",
    "text": "Markdown and Quarto\n\nQuarto implements Markdown with data-analytic extensions\nSeamless (ideally!) integration of code and text\nNo more copy and paste\n\nQuarto user guide is fantastic!\nSee also source for course materials."
  },
  {
    "objectID": "slides/slides02.html#lab-activity-part-0",
    "href": "slides/slides02.html#lab-activity-part-0",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Lab Activity: Part 0",
    "text": "Lab Activity: Part 0\nIf you haven’t already, install Quarto."
  },
  {
    "objectID": "slides/slides02.html#lab-activity-part-1",
    "href": "slides/slides02.html#lab-activity-part-1",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Lab Activity: Part 1",
    "text": "Lab Activity: Part 1\nCreate a simple PDF quarto document using the RStudio wizard.\n(Note that you may need to install tinytex for this to work properly, but Quarto should install it for you automatically.)"
  },
  {
    "objectID": "slides/slides02.html#lab-activity-part-2",
    "href": "slides/slides02.html#lab-activity-part-2",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Lab Activity: Part 2",
    "text": "Lab Activity: Part 2\nCreate a 5 slide presentation showing the Houston housing market. This should include:\n\nA title slide\nThree body slides with a figure and some text\nA conclusion slide\n\nYou may use the following code snippets:"
  },
  {
    "objectID": "slides/slides02.html#lab-activity-part-3",
    "href": "slides/slides02.html#lab-activity-part-3",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "Lab Activity: Part 3",
    "text": "Lab Activity: Part 3\nView the Quarto Demo Slides and add one new element to your slides from the previous section."
  },
  {
    "objectID": "slides/slides02.html#github-pages-1",
    "href": "slides/slides02.html#github-pages-1",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "GitHub Pages",
    "text": "GitHub Pages\nIn-class discussion of what a static web page is and the role of GitHub Pages as a static web server."
  },
  {
    "objectID": "slides/slides02.html#how-to-ask-for-help-1",
    "href": "slides/slides02.html#how-to-ask-for-help-1",
    "title": "STA/OPR 9750 - Week 2 Update",
    "section": "How to Ask for Help",
    "text": "How to Ask for Help\nProfessional programming is at least half looking things up; at beginning stages, the fraction is even higher.\nSo it’s important to know how to see help the smart way:\n\nOfficial documentation. Free software almost never becomes famous without great documentation: R and its packages are no exception. Everything we will use in this class has solid documentation.\n\n\nTidyverse.org"
  },
  {
    "objectID": "objectives.html",
    "href": "objectives.html",
    "title": "STA/OPR 9750 - Course Learning Objectives",
    "section": "",
    "text": "This course provides an understanding of the principles and concepts of using computer tools for data analysis and visualization. Students will learn to use a scientific programming language (such as R) to import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, and statistical modelling techniques will be introduced. Written reports will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for Masters’ students in statistics and quantitative methods and modeling (QMM), and those interested in carrying out sophisticated statistical analyses of data using statistical software.\n\n\n\nThis course provides an understanding of the principles and concepts of using computer tools for data analysis. Students will learn to use the SAS programming language to handle the collection, editing and storing of large datasets, as well as to simulate data, import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, as well as repeated measure data. Written projects and class presentation will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for statistics and quantitative methods and modeling (QMM) majors, PhD candidates, and those interested in carrying out sophisticated statistical analyses of data using statistical software.\nInstructor’s Note: Contra the official OPR 9750 description, this course will be taught using R not SAS. STA 9750 and OPR 9750 will be jointly taught and graded. Please consult with your degree program director to determine which listing is appropriate for you."
  },
  {
    "objectID": "objectives.html#official-course-description",
    "href": "objectives.html#official-course-description",
    "title": "STA/OPR 9750 - Course Learning Objectives",
    "section": "",
    "text": "This course provides an understanding of the principles and concepts of using computer tools for data analysis and visualization. Students will learn to use a scientific programming language (such as R) to import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, and statistical modelling techniques will be introduced. Written reports will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for Masters’ students in statistics and quantitative methods and modeling (QMM), and those interested in carrying out sophisticated statistical analyses of data using statistical software.\n\n\n\nThis course provides an understanding of the principles and concepts of using computer tools for data analysis. Students will learn to use the SAS programming language to handle the collection, editing and storing of large datasets, as well as to simulate data, import and export data from and into spreadsheets or other statistical software packages. They will gain experience in analyzing both quantitative and qualitative data, as well as repeated measure data. Written projects and class presentation will prepare students for clear communication of their analysis in professional settings. This course is designed primarily for statistics and quantitative methods and modeling (QMM) majors, PhD candidates, and those interested in carrying out sophisticated statistical analyses of data using statistical software.\nInstructor’s Note: Contra the official OPR 9750 description, this course will be taught using R not SAS. STA 9750 and OPR 9750 will be jointly taught and graded. Please consult with your degree program director to determine which listing is appropriate for you."
  },
  {
    "objectID": "objectives.html#course-learning-objectives",
    "href": "objectives.html#course-learning-objectives",
    "title": "STA/OPR 9750 - Course Learning Objectives",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nStudents successfully completing STA/OPR 9750 will be able to:\n\nEffectively communicate the reuslts of data analyses.\nManipulate tabular data in R\nDevelop effective and compelling visualizations using standard statistical software\nManipulate `wild-caught’ data from web-based sources\nUse computational approaches to statistical inference\nDevelop novel analytical products to convey actionable insights.\n\nThe following course elements contribute to these goals:\n\nContribution of Course Elements to Learning Goals\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Goal 1\nLearning Goal 2\nLearning Goal 3\nLearning Goal 4\nLearning Goal 5\nLearning Goal 6\n\n\n\n\nMini Project #00\n✓\n\n\n\n\n\n\n\nMini Project #01\n✓\n✓\n\n\n\n\n\n\nMini Project #02\n✓\n✓\n✓\n\n\n\n\n\nMini Project #03\n✓\n✓\n✓\n✓\n\n\n\n\nMini Project #04\n✓\n✓\n✓\n✓\n✓\n\n\n\nCourse Project\n✓\n✓\n✓\n✓\n✓\n✓"
  },
  {
    "objectID": "objectives.html#program-learning-goals",
    "href": "objectives.html#program-learning-goals",
    "title": "STA/OPR 9750 - Course Learning Objectives",
    "section": "Program Learning Goals",
    "text": "Program Learning Goals\nThis course contributes to the program learning goals of several MS programs offered by the Zicklin School of Business.\n\nMS in Business Analytics\nThis course contributes to the following Program Learning Goals for the MS in Business Analytics:\n\nMSBA Program Learning Goals\n\n\n\n\n\n\n\nSTA/OPR 9750 Learning Goal\nMSBA Learning Goal\nDescription\n\n\n\n\n✓\nData Management\nStudents will be able to apply methods, tools, and software for acquiring, managing/storing, and accessing structured and unstructured data. Students will also demonstrate knowledge of the strategic uses of data.\n\n\n✓\nFoundational Statistical / Quantitative Skills\nStudents will be able to prepare data for statistical analysis, perform basic exploratory and descriptive analysis as well as employ foundational statistical techniques needed to analyze data.\n\n\n✓\nAdvanced Statistical/Quantitative Skills\nStudents will be able to build and interpret advanced predictive models. Students will be able to combine business rules and mathematical models to optimize business decisions from data.\n\n\n\nEthical Awareness\nStudents will be able to articulate an understanding of ethical issues in all phases of business analytics with particular emphasis on the new possibilities afforded by the emergence of big data.\n\n\n✓\nProfessional Communication\nStudents will be able to explain complex analytical models and their results orally and in writing to technical and non technical/lay audiences.\n\n\n✓\nKnowledge Integration\nStudents will be able to apply the three key types of analytics (descriptive, predictive, and prescriptive) in a business domain to add value to business decision-making.\n\n\n\n\n\nMS in Quantitative Methods & Modeling\nThis course contributes to the following Program Learning Goals for the MS in Quantitative Methods & Modeling:\n\nMSQMM Program Learning Goals\n\n\n\n\n\n\n\nSTA/OPR 9750 Learning Goal\nMSQMM Learning Goal\nDescription\n\n\n\n\n✓\nOperations Research & Mathematical Modeling\nStudents will be able to effectively model, evaluate, and solve quantitative (business) problems using quantitative modeling methods (e.g. deterministic and probabilistic operations research techniques).\n\n\n✓\nStatistics\nStudents will be able to correctly apply appropriate statistical methods when defining, solving, and analyzing problems.\n\n\n✓\nTechnology Competency\nStudents will be able to use current technological tools, including spreadsheets and specialized software, when solving problems.\n\n\n✓\nProfessional Communication\nStudents will be able to effectively communicate their problem solving methods and solutions to technical and non-technical audiences.\n\n\n\n\n\nMS in Statistics\nThis course contributes to the following Program Learning Goals for the MS in Statistics:\n\nMS Statistics Program Learning Goals\n\n\n\n\n\n\n\nSTA/OPR 9750 Learning Goal\nMS Stat Learning Goal\nDescription\n\n\n\n\n✓\nGeneral Statistical Competence\nStudents will be able to apply appropriate probability models and statistical techniques when analyzing problems from business and other fields.\n\n\n✓\nStatistical Practice\nStudents will become familiar with the standard tools of statistical practice for multiple regression, along with the tools of a subset of specialized statistical areas such as multivariate analysis, applied sampling, time series analysis, experimental design, data mining, categorical analysis, and/or stochastic processes.\n\n\n✓\nTechnology Competency\nStudents will learn to use one or more of the benchmark statistical software platforms, such as SAS or R."
  },
  {
    "objectID": "preassigns/pa12.html",
    "href": "preassigns/pa12.html",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "",
    "text": "This week, we begin to study the world of text data. While numerical data is reasonably straight-forward to deal with, text data is remarkably complex. A full discussion of text data requires understanding the vast world of human written language, but we will discuss enough of the major points to hopefully solve 95% of the challenges you will face in your career."
  },
  {
    "objectID": "preassigns/pa12.html#goals",
    "href": "preassigns/pa12.html#goals",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "Goals",
    "text": "Goals\nIn our quest to understand text data, we have two major goals:\n\nUnderstanding String Encodings and Unicode\nManipulating Strings with Regular Expressions\n\nBefore we get into these, let’s begin with a basic review of the character data type in R."
  },
  {
    "objectID": "preassigns/pa12.html#string-vectors",
    "href": "preassigns/pa12.html#string-vectors",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "String Vectors",
    "text": "String Vectors\nRecall that R works by default on vectors - ordered collections of the “same sort” of thing. R supports the following vector types:\n\nRaw for pure access to bytes without any additional meaning: rarely useful for pure data-analytic work\nInteger: 32-bit signed integers, ranging from \\(-2^{30}\\) to \\(2^{30}-1\\). (If you have done low-level work before, you might ask where the extra bit went: it’s used for encoding NA values.)\nNumeric: 64-bit (double precision) floating point values, ranging from (approximately) \\(\\pm 10^{308}\\). The detailed behavior of numeric (often called double) data is beyond this course, but it is well documented elsewhwere.\nCharacter: the topic of today’s discussion.\n\nR makes no difference between a character - in the sense of a single letter - and a string: in particular, each element of a character vector is an (arbitrary length) string. Specialized functions are required for work at the true “single letter” scale. If you come from other languages, this behavior might be surprising, but it allows R to handle much of the complexity associated with characters automagically, which greatly simplifies data analysis.\nWhen speaking, we refer to R as using strings, even if R itself calls them character elements for historical reasons."
  },
  {
    "objectID": "preassigns/pa12.html#encoding",
    "href": "preassigns/pa12.html#encoding",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "Encoding",
    "text": "Encoding\nHow are strings represented on a computer? The answer has evolved over time, but the current state of the art - used by almost all non-legacy software - is based on the Unicode system and the UTF-8 encoding.\nThe Unicode system is comprised of two essential parts: - A numbered list of “letter-like” elements - Rules for manipulating those elements\nWhile this seems simple, it is anything but. The history of string representations in computers is a long and painful story of programmers repeatedly underestimating the complexity of the seemingly simple task of listing “all the letters.”\nThe Unicode consortium makes a long list of characters that computers should be able to represent: the most recent version of the Unicode standard includes 149,813 characters divided into 161 scripts. These include everything from the basic (Anglo-American) Latin alphabet to the Greek and Cyrillic alphabets to Chinese and Japanese characters to the undeciphered Linear A alphabet and Tengwar, the fictional script used in the Lord of the Rings novels. The Unicode standard also includes a wide set of Emoji (approximately 4000) and many “modifying” characters.\nTo each of these, the Unicode consortium assigns a code point : a numerical identifier. Even superficially similar characters may be assigned different code points to distinguish them: for example, “H” is code point U+0048 with the official description “Latin Capital Letter H” while “Η” is U+0397, “Greek Capital Letter Eta.”\nThe difference between these characters is essential to know how to manipulate them:\n\n\n\n\n\n\n\n\nUse the tolower function to lower-case each of these:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Unicode standard defines the lower case mapping of U+0048 as the Latin lower case h, while the lower case mapping of U+0397 as the Greek lower case eta, which looks something like a streched n. \nIn general, these mappings are incredibly complicated and depend not only on the specific code point, but also the set of translation rules being used. (Certain languages have different lower/upper mappings for what are otherwise the same letter.)\nWhile you don’t need to know all of this complexity, it is essential to know that it’s out there and to rely on battle-tested libraries to perform these mappings.\nUnicode is supplemented by the UTF-8 encodings, which controls how 0/1-bit strings are actually translated to code points. (Fonts then map code points to what you see on the screen.) UTF-8 is more-or-less back-compatible with other major encodings, so it’s a good default. When dealing with modern websites or public data sources, they almost always present their contents in a UTF-8 compatible encoding (if not UTF-8 proper) so you should be ok.\nA well-formatted website will state its encoding near the top of the page:\n\nlibrary(rvest)\nread_html(\"https://baruch.cuny.edu\") |&gt;\n    html_elements(\"meta[charset]\") |&gt;\n    html_attr(\"charset\")\n\n[1] \"UTF-8\"\n\n\nAdvice: Whenever possible, make sure you are using UTF-8 strings: if your data is not UTF-8, reencode it to UTF-8 as soon as possible. This will save you much pain."
  },
  {
    "objectID": "preassigns/pa12.html#string-manipulation",
    "href": "preassigns/pa12.html#string-manipulation",
    "title": "STA 9750 Week 12 Pre-Assignment: Strings and Things",
    "section": "String Manipulation",
    "text": "String Manipulation\nOnce data is in R and encoded as UTF-8 Unicode points, we have several tools for dealing with strings. Your first port of call should be the stringr package.\nAll the functions of the stringr package start with str_ and take a vector of strings as the first argument, making them well suited for chained analysis.\nLet’s start with str_length which simply computes the length of each element. For the basic Latin alphabet, this more or less matches our intuition:\n\nlibrary(stringr)\nx &lt;- c(\"I\", \"am\", \"a\", \"student\", \"at\", \"Baruch.\")\nstr_length(x)\n\n[1] 1 2 1 7 2 7\n\n\nbut it can be tricky for strings that involve Unicode combining characters.\n\nstr_length(\"X̅\")\n\n[1] 2\n\n\nHere the “overbar” is a combining character which we add on to the X. This is commonly (though not always) used for languages with accents (e.g. French) or for languages where vowels are written above and below the main script (Arabic or Hebrew). This same idea is used for certain Emoji constructs:\n\nstr_length(\"👨🏿\")\n\n[1] 2\n\n\nHere, “Man with Dark Skin Tone” is the combination of “Man” and “Dark Skin Tone.” (Compare how this appears in the rendered document to how RStudio prints it.)\nWhile there is complexity in all of Unicode, str_length will behave as you might expect for “regular” text. I’m going to stop showing the “scary case” of Unicode, but you should be aware of it for the remainder of these exercises.\n\nConcatenation\nYou have already seen the base paste and paste0 functions for combining two string vectors together.\n\nx &lt;- c(\"Michael\", \"Mary\", \"Gus\")\ny &lt;- c(\"Son\", \"Daughter\", \"Dog\")\n\npaste(x, y)\n\n[1] \"Michael Son\"   \"Mary Daughter\" \"Gus Dog\"      \n\n\nBy default, paste combines strings with a space between them, while paste0 omits the space. paste is typically what you want for strings for human reading, while paste0 is a better guess for computer-oriented text (e.g., putting together a URL).\nYou can change the separator by passing a sep argument to paste:\n\npaste(x, y, sep = \" is my \")\n\n[1] \"Michael is my Son\"   \"Mary is my Daughter\" \"Gus is my Dog\"      \n\n\nYou can also combine together multiple elements of a vector using the collapse argument:\n\npaste(x, collapse = \" and \")\n\n[1] \"Michael and Mary and Gus\"\n\n\n\nExercises:\nUsing the paste function, make a vector of strings like “John’s favorite color is blue”:\n\npeople &lt;- c(\"John\", \"Jane\", \"Randy\", \"Tammi\")\ncolors &lt;- c(\"blue\", \"orange\", \"grey\", \"chartreuse\")\n\nModify your answer to write a (run-on) sentence of favorite colors: “John’s favorite color is blue and Jane’s favorite color is orange and …”\n\npeople &lt;- c(\"John\", \"Jane\", \"Randy\", \"Tammi\")\ncolors &lt;- c(\"blue\", \"orange\", \"grey\", \"chartreuse\")\n\n\n\n\nSubstring Selection\nWhen cleaning up data for analysis, it is common to need to take substrings from larger text. The str_sub function will do this:\n\nx &lt;- c(\"How\", \"much\", \"is\", \"that\", \"puppy\", \"in\", \"the\", \"window?\")\nstr_sub(x, 1, 2)\n\n[1] \"Ho\" \"mu\" \"is\" \"th\" \"pu\" \"in\" \"th\" \"wi\"\n\n\nIf you want to go all the way to the end, set the end element to -1:\n\nstr_sub(x, 2, -1)\n\n[1] \"ow\"     \"uch\"    \"s\"      \"hat\"    \"uppy\"   \"n\"      \"he\"     \"indow?\"\n\n\n\nExercises\nUsing str_sub, remove the system name (CUNY or UC) and return only the campus name:\n\nlibrary(stringr)\nuc_schools &lt;- c(\"UC Berkeley\", \"UC San Diego\", \"UC Santa Cruz\", \"UC Davis\")\ncuny_schools &lt;- c(\"Baruch College, CUNY\", \"City College, CUNY\", \"La Guardia Community College, CUNY\")\nstr_sub(uc_schools)\n\n[1] \"UC Berkeley\"   \"UC San Diego\"  \"UC Santa Cruz\" \"UC Davis\"     \n\n\n\n\n\nDetect and Matching\nOften we only need to know whether a particular substring is present in a larger string. We can use str_detect to do this:\n\nlibrary(stringr)\ndogs &lt;- c(\"basset hound\", \"greyhound\", \"labrador retreiver\", \"border collie\", \"Afgahn hound\")\nstr_detect(dogs, \"hound\")\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE\n\n\nThe str_match function will return the text of the match. Here it’s useless, but we’ll see that it becomes more powerful when we allow more flexible pattern specifications.\n\nlibrary(stringr)\ndogs &lt;- c(\"basset hound\", \"greyhound\", \"labrador retreiver\", \"border collie\", \"Afgahn hound\")\nstr_match(dogs, \"hound\")\n\n     [,1]   \n[1,] \"hound\"\n[2,] \"hound\"\n[3,] NA     \n[4,] NA     \n[5,] \"hound\"\n\n\n\nExercises\nUse str_detect to find the CUNY schools:\n\nlibrary(stringr)\nschools &lt;- c(\"UC Davis\", \n             \"UC Santa Cruz\", \n             \"City College, CUNY\", \n             \"UC Berkeley\", \n             \"La Guardia Community College, CUNY\", \n             \"Baruch College, CUNY\", \n             \"UC San Diego\")\nstr_sub(uc_schools)\n\n[1] \"UC Berkeley\"   \"UC San Diego\"  \"UC Santa Cruz\" \"UC Davis\"     \n\n\n\n\n\nSpecifying Patterns\nWhile working by individual characters is sometimes useful (for very regular data), we generally need more powerful tools: regular expressions (RE) provide a compact language for specifying patterns in strings. We’ll introduce the basics here to help with string functions and then explore some more advanced RE features.\nThe most basic pattern is a set of elements in brackets: this means “any of these”.\nFor example, we want to see which names have an “A” in them:\n\nnames &lt;- c(\"Jane\", \"Rhonda\", \"Reggie\", \"Bernie\", \"Walter\", \"Arthur\")\nstr_detect(names, \"a\") ## Wrong!\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE FALSE\n\nstr_detect(names, \"A\") ## Wrong!\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\nstr_detect(names, \"[Aa]\") ## Right!\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n\n\nAlternatively, we can see which strings contain numbers:\n\nx &lt;- c(\"73 cows\", \"47 chickens\", \"a dozen eggs\")\nstr_detect(x, \"[0123456789]\")\n\n[1]  TRUE  TRUE FALSE\n\n\nIf we use str_match we can pull out the matching element:\n\nx &lt;- c(\"2 burgers\", \"3 soups\", \"5 fish\")\nstr_match(x, \"[0123456789]\")\n\n     [,1]\n[1,] \"2\" \n[2,] \"3\" \n[3,] \"5\" \n\n\nBy default, this only finds one appearance of the pattern:\n\nx &lt;- c(\"23 burgers\", \"34 soups\", \"56 fish\")\n\n# Why is this wrong?\nstr_match(x, \"[0123456789]\")\n\n     [,1]\n[1,] \"2\" \n[2,] \"3\" \n[3,] \"5\" \n\n\nWe can modify the pattern specifier to include count information. The basic behavior is to add explicit count bounds:\n\nx &lt;- c(\"2 burgers\", \"34 soups\", \"567 fish\")\nstr_match(x, \"[0123456789]{2}\")\n\n     [,1]\n[1,] NA  \n[2,] \"34\"\n[3,] \"56\"\n\nstr_match(x, \"[0123456789]{3}\")\n\n     [,1] \n[1,] NA   \n[2,] NA   \n[3,] \"567\"\n\nstr_match(x, \"[0123456789]{2,3}\")\n\n     [,1] \n[1,] NA   \n[2,] \"34\" \n[3,] \"567\"\n\nstr_match(x, \"[0123456789]{2,}\")\n\n     [,1] \n[1,] NA   \n[2,] \"34\" \n[3,] \"567\"\n\n\nHere a single number is an exact count ({2}), while pairs ({2,3}) specify a range. If one end of the range is left empty, it is 0 or infinite (depending on the direction).\nCertain count specifications are sufficiently useful to get their own syntax:\n\nOne or more: + is equivalent to {1,}\nZero or more: * is equivalent to {0,}\nOne or zero: ? is equivalent to {0,1}.\n\nUse these specifications for the next set of exercises.\n\nExercises\nWhich strings contain a three digit number?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstr_detect(x, \"\\\\d{3}\")\nstr_detect(x, \"\\\\d{3}\")\n\n\n\n\n\n\n\n\n\nCombining patterns\nYou can combine REs to make more complex patterns:\n\n(a|b) means a or b. This is like [] notation but a, b can be more complex than single characters\n\n\nx &lt;- c(\"Baruch College, CUNY\", \"UC Berkeley\", \"Harvard University\")\nstr_detect(x, \"(CUNY|UC)\")\n\n[1]  TRUE  TRUE FALSE\n\n\n\n[^abc] means anything other than a, b, c. You can often achieve a similar effect using the negate argument to str_detect, but you need this specifically for str_match\n\n\nx &lt;- c(\"10 blue fish\", \"three wet goats\", \"15 otters in hats\")\nstr_detect(x, \"[^0123456789]\")\n\n[1] TRUE TRUE TRUE\n\n\n\n^ outside of a bracket denotes the start of a line:\n\n\nx &lt;- c(\"rum\", \"white rum\", \"flavored rum\")\nstr_detect(x, \"^rum\")\n\n[1]  TRUE FALSE FALSE\n\n\n\n$ denotes the end of a line:\n\n\nx &lt;- c(\"bourbon whiskey\", \"scotch whisky\", \"whiskey liqueurs\")\nstr_detect(x, \"whisk[e]?y$\")\n\n[1]  TRUE  TRUE FALSE\n\n\nSee the stringr RE docs for more examples of regular expressions.\n\nExercises\n\nUse a regular expression to find which of these are fish species:\n\n\n\n\n\n\n\n\n\n\nUse a regular expression to find words with three or more vowels in a row:\n\n\n\n\n\n\n\n\n\n\nFind the words where “q” is not followed by a “u”\n\n\n\n\n\n\n\n\n\n\n\n\nReplacement\nThe str_replace function allows us to replace a string with something else. This is particularly useful when cleaning up text:\n\nx &lt;- c(\"Manhattan, NY\", \"Bronx, New York\", \"Brooklyn, ny\", \"Queens, nY\")\nstr_replace(x, \"([nN][yY]|New York)\", \"NY\")\n\n[1] \"Manhattan, NY\" \"Bronx, NY\"     \"Brooklyn, NY\"  \"Queens, NY\""
  },
  {
    "objectID": "preassigns/pa02.html",
    "href": "preassigns/pa02.html",
    "title": "STA/OPR 9750 Week 2 Pre Assignment: Getting Started with Markdown",
    "section": "",
    "text": "Due Date: 2024-09-04 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week, we are going to learn to use quarto, a data science publishing platform. quarto documents are written using Markdown, a light-weight mark-up language.12\nFor this week’s pre-assignment, complete this interactive Markdown tutorial, which should take you about 10 minutes. Once you’ve familiarized yourself with Markdown, take a look at the source code for this website and see how certain Markdown documents are rendered as web pages.\nDuring this week’s lab session, we will take particular advantage of:\nso make sure to pay attention to those parts of the tutorial.\nAfter you are done with the introduction to Markdown, log in to CUNY Brightspace and complete the Pre-Assignment 02 “Getting to Know You” quiz. As part of this quiz, you will be asked to attest that you successfully completed the Markdown tutorial."
  },
  {
    "objectID": "preassigns/pa02.html#footnotes",
    "href": "preassigns/pa02.html#footnotes",
    "title": "STA/OPR 9750 Week 2 Pre Assignment: Getting Started with Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGet it? If you learn nothing else in this class, you will certainly learn that programmers love terrible puns. R itself is actually a pun as it was originally a free ‘knock-off’ of the S programming language developed by Ross and Rob.↩︎\nA mark-up language is a way of specifying the formatting applied to given text. It exists somewhere between “plain text” and a full document format like a .docx file. Other markup languages include HTML (hyper text markup language), rST (reStructured Text), LaTeX (used for scientific typesetting), and many others. Markdown is the simplest of these and the only one you will be required to write in this course. You will need to learn a bit of how HTML is structured and, if you are including math in your mini-project or final project submissions, a bit of LaTeX will go a long way.↩︎"
  },
  {
    "objectID": "miniprojects/mini00.html",
    "href": "miniprojects/mini00.html",
    "title": "STA/OPR 9750 Mini-Project #00: Course Set-Up",
    "section": "",
    "text": "In lieu of traditional homework, this course has a set of four mini-projects, which will be assessed in two stages. In the first, you will complete a small data analysis project1; after submission of your analysis, it will be assigned to a classmate, who will evaluate it according to an instructor-provided rubric. This peer feedback stage is an opportunity to see how your classmates answered questions and to compare it to your own response. In doing so, you will learn to evaluate data science work product and will develop a critical eye that can be turned to your own work.\nThis mini-project, however, is a meta-mini-project, designed to help you set up the course infrastructure you will use for the four graded mini-projects.\nNB: Mini-Project #00 is not graded, but it is required. For STA/OPR 9750, it serves as the legally mandated Verification of Enrollment activity. If it is not completed on time, you may be involuntarily disenrolled from the course.\nEstimated Time: 2 hours.\nThis course will use the industry-standard code sharing platform GitHub. Mini-projects and course-projects will be submitted by posting to a relevant GitHub project and creating a world-readable HTML page. A secondary goal of this course is to help students build a web-presence and a data science portfolio, giving you a place to showcase your skills to potential employers. The four mini-projects and the final course project should form an excellent basis for a portfolio. The main aim of Mini-Project #00 is to set up the “skeleton” of this portfolio.\nYou may choose to complete these tasks under a pseudonym if you do not want current or potential employers, classmates, or the world at large to see your work. You will be required to disclose your pseudonym to the instructor. If you choose to use a pseudonym, it will be straightforward to add your name to any or all coursework after the semester ends. Within the course, you will have the option to switch to a pseudonym as desired, but it is difficult to fully anonymize anything once it has been posted on the public internet. With all those cautions, please take a moment to reflect as to whether you wish to proceed under your own name or using a pseudonym."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-1-github-account-creation",
    "href": "miniprojects/mini00.html#stage-1-github-account-creation",
    "title": "STA/OPR 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 1: GitHub Account Creation",
    "text": "Stage 1: GitHub Account Creation\nTo complete this course, you will need a free GitHub personal account, which you can create here. Please note that whatever account name you use will be public, so you need to define a pseudonym here if you choose to use one."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-2-course-repo-creation",
    "href": "miniprojects/mini00.html#stage-2-course-repo-creation",
    "title": "STA/OPR 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 2: Course Repo Creation",
    "text": "Stage 2: Course Repo Creation\n\nCreating GitHub Repo\nNow that you have created a GitHub account, log in and proceed to your dashboard at https://github.com. In the top right corner, click the + symbol and select “New Repository.”2\n\nCreate a new repository named STA9750-2024-FALL with a suitable description.\n\nThis repo needs to be public. You do not need to select a README, .gitignore, or a license at this time.\nAfter you create your repo, you should see a page like this:\n\nNote the URL highlighted in the main box:\n**https://github.com/&lt;USERNAME&gt;/STA9750-2024-FALL.git**\nYou will need this in the next step.\n\n\nConnecting GitHub Repo to RStudio\nNow that you have set up an empty repo, you need to connect it to your personal machine and to RStudio. RStudio’s concept of projects roughly map to GitHub repos and that is what we will use here.\nOpen RStudio and click the project menu in the top right corner:\n\nFollow through the menu to click:\n\nNew Project\nVersion Control\nGit\n\nThis will take you to the following screen:\n\nCopy the .git URL from the previous step into Repository URL.3\n\n\nSecuring Connections to GitHub\nAn important aspect of source code management is access control to your code repository. While it’s typically no risk to make your code world-readable, you don’t want just anyone being able to add code to your repository. Traditionally, this type of access would be controlled with a username+password scheme, but GitHub has moved to a Access Token structure.\nThis process is a “one-time” task, documented in the Connect Section of the Happy Git with R book, but we’ll cover the highlights here.\n\nGit Credential Manager\nBefore starting, you will want to make sure you have installed the Git Credential Manager (GCM). If you used the Git for Windows bundle, you already have GCM installed. If you are on a Mac, you can download the GCM installer directly or install GCM via the GitHub Desktop Client.\n\n\nGitHub Personal Access Token\nNext, you need to create a Personal Access Token (PAT) for GitHub. You can think of this as a “special-purpose” password. Unlike your general account password, a PAT can be restricted to only perform certain activities or only for a limited time period.\nAfter logging in to GitHub via a web browser, visit https://github.com/settings/personal-access-tokens/new to begin the token creation process. Give the token a meaningful name and description and set an expiration date after (at least) the end of the semester.\nSet the “Repository Access” to “All repositories” and, under “Permissions &gt; Repository Permissions”, set “Contents” to “Read and Write”. This will now let anyone using your token read and write to all your repositories. After you create your token, you will be given only one opportunity to copy it. (Note that you can change permissions later, but you can only copy the token once.) Copy this and save it for later use. If you loose this token, you may need to generate a new one.\nWhen you make your first push to GitHub (as described below), use this token as your password. If everything is set up correctly, GCM will save this token and use it to authenticate you every time you push to GitHub. You should not need to paste this token every time.\n\n\n\nInitial Push\nNow, to make sure everything is working, let’s save a basic README file and push it to GitHub. This is a plain text file with no particular structure.\nTo create it, click the new file button in RStudio (top left; piece of paper with a green plus) and select Text File. RStudio will open this file in the editor: type some basic content, e.g.,\nSubmission materials for STA/OPR 9750 at Baruch College. \n\nOwner: &lt;YOURNAME&gt;\n(It doesn’t matter what you push: whatever you type will be the default text appearing when someone visits your repo.)\nSave the file and open the Git pane in RStudio.\n\nCheck the box next to the README file to stage it for git.\n\nThen click the Commit button a type a brief message (Initial commit is fine).\n\nFinally, push the Push button. If everything works, you should see a screen like the below:\n\nTo confirm everything worked, return to the GitHub repo in your browser. You should see the text of your README file displayed at the bottom of the page."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-3-personal-website-creation",
    "href": "miniprojects/mini00.html#stage-3-personal-website-creation",
    "title": "STA/OPR 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 3: Personal Website Creation",
    "text": "Stage 3: Personal Website Creation\nNow that you created a place where you can push files to GitHub and have successfully pushed a basic README, it’s time to build a webpage using quarto.\nWe will need three pages to build a website:\n\nA configuration file, _quarto.yml, used to specify the look and layout of your website.\nAn index.qmd file used to create the homepage.\nA build script to create the website.\n\n\nConfiguration File\nOpen a new text file and save it as _quarto.yml. This is a configuration file used by quarto to control the layout of your site. For a barebones site, copy the following into _quarto.yml:\nproject:\n  type: website\n  output-dir: docs\n\nwebsite:\n  title: \"STA/OPR 9750 2024 Submission Material\"\n  description:\n    Course Projects for STA/OPR 9750 at Baruch College\n  site-url: \"https://&lt;GITHUB_NAME&gt;.github.io/STA9750-2024-FALL/\"\n  navbar:\n    background: primary\n    search: false\n    \nformat:\n  html:\n    theme: &lt;THEME&gt;\n    toc: false\nNote that the indentation pattern is important so copy this exactly.\nReplace &lt;GITHUB_NAME&gt; with your GitHub user name.\nFor &lt;THEME&gt;, visit the Bootswatch theme gallery and pick your preferred theme. Replace &lt;THEME&gt; with a lower case version of the theme name; if you want to use the Sandstone theme used for this course website, &lt;THEME&gt; will be sandstone.\nOnce you have created this _quarto.yml, stage it (click the check mark) in RStudio’s git pane.\n\n\nindex.qmd\nNext, we’ll build your home page, conventionally called index.html. We will not write the HTML code by hand - it’s quite cumbersome - and will instead let quarto create it for us. Create another plain text file and save it as index.qmd.\nThis file will be divided into two parts, a header giving the metadata for the site, and a body, giving the content of the site.\nFirst write the header, separated by three horizontal bars (minus signs) above and below. For now, all you need to specify is a title:\n---\ntitle: \"YOUR TITLE GOES HERE\"\n---\nBelow the header, write the basic content of your website: a brief introduction of who you are.4 You can use markdown here for formatting. Basic text will suffice, but this is also a great opportunity to include things like a personal headshot, a link to a full resume, or similar.\nAs you work on this, click the “Render” button at the top of the editor pane to see what your site will look like.\nOnce you are happy with this landing page, stage it and we’ll move on to building the website properly.\n\n\nbuild_site.R\nFinally, open a new file - but now it’s an R script, not a text file, in RStudio. Copy the following into build_site.R:\n#!/usr/bin/env Rscript\nif(!require(\"quarto\")){\n    install.packages(\"quarto\")\n}\nlibrary(quarto)\nif(!quarto::quarto_binary_sitrep()){\n    stop(\"Something is wrong with your quarto installation.\")\n}\nquarto::quarto_render(\".\")\nsystem(\"git add docs/*\")\nif(!any(grepl(\"rstudio\", search()))){q(\"no\")}\nClick the Source button in the top-right corner of the editor pane to run this code. If everything works, it will build your website and automatically stage it. Stage build_site.R as well.\nFinally, Commit all these staged files and Push them to GitHub. You have now created a website and just need to turn on a web server so you can access it."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-4-github-pages-deployment",
    "href": "miniprojects/mini00.html#stage-4-github-pages-deployment",
    "title": "STA/OPR 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 4: GitHub Pages Deployment",
    "text": "Stage 4: GitHub Pages Deployment\nReturn to the GitHub repo you created; recall that the URL is something like:\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2024-FALL/\nOpen the “Settings” menu and proceed to the “Pages” submenu. You should see a page that looks like this:\n\nUnder Build and Deployment, set the main branch to deploy and select the docs directory on that branch. Hit save and your website will go live!\nTo check your website is working, proceed to\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2024-FALL\nIf everything works, you will see your site! (If you used the Render feature in RStudio, it should look familiar.)\nIf you get stuck, use the course discussion board to seek help from your classmates and, if necessary, the instructor."
  },
  {
    "objectID": "miniprojects/mini00.html#stage-5-submission",
    "href": "miniprojects/mini00.html#stage-5-submission",
    "title": "STA/OPR 9750 Mini-Project #00: Course Set-Up",
    "section": "Stage 5: Submission",
    "text": "Stage 5: Submission\nOnce your site is live, you will submit it to the instructor in two ways:\n\nLog into the course discussion board (Piazza) and send me your GitHub name so I can link it to my gradebook.\nTag @michaelweylandt on GitHub to make sure I can access your repo.\n\nThese both must be completed to complete the assignment and verify enrollment.\n\nDiscussion Board (Piazza)\nFirst, send me a private message through the course discussion board with the following details:\n\nReal Name\nCUNY EmplID (8 digit ID code)\nCUNY email\nGitHub user name\nWhich course section you are enrolled in: STA 9750 or OPR 9750\n\nThis is the only place where you are required to connect your GitHub ID with your real name and CUNY credentials. I need this information to connect your public activity with my (private) gradebook and the CUNY system.\nIf all your information looks good, I might not reply through the discussion board. When I reply through GitHub, I’m acknowledging both parts of your submission.\n\n\nInstructor Tagging\nFinally, you’re going to contact me through GitHub: go to\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2024-FALL/issues/new\nto open a new issue. Title the issue STA/OPR 9750 &lt;GITHUB_USERNAME&gt; MiniProject #00 and fill in the following text for the issue:\nHi @michaelweylandt!\n\nI've created my STA/OPR 9750 website - check it out!\n\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2024-FALL/\n(Replace &lt;GITHUB_USERNAME&gt; with your username throughout.)\n\nThis will send me a notification through GitHub and I will confirm that I can access your repository and website. If you don’t do this, I may not be able to access your graded assignments when you submit them! I will confirm that I have your real ID verified as well."
  },
  {
    "objectID": "miniprojects/mini00.html#wrap-up",
    "href": "miniprojects/mini00.html#wrap-up",
    "title": "STA/OPR 9750 Mini-Project #00: Course Set-Up",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nOnce I acknowledge receipt of your ID and website, you’re done with Mini-Project #00! You’ve built a website and are ready for the course to begin in earnest.\nMini-Projects #01-#04 will be submitted as separate pages in your website (different quarto documents) and hosted via GitHub pages for peer feedback. We will discuss that process in more detail after Mini-Project #00 is complete."
  },
  {
    "objectID": "miniprojects/mini00.html#hints",
    "href": "miniprojects/mini00.html#hints",
    "title": "STA/OPR 9750 Mini-Project #00: Course Set-Up",
    "section": "Hints",
    "text": "Hints\nIf you need help, the course discussion board should be your first stop.\nIf you want to personalize your website further, you can see how I have created mine on GitHub. Recall that the Markdown syntax used by quarto is summarized at https://www.markdownguide.org/basic-syntax/.\nYou may want to use the About Page functionality to improve the look of your home page."
  },
  {
    "objectID": "miniprojects/mini00.html#footnotes",
    "href": "miniprojects/mini00.html#footnotes",
    "title": "STA/OPR 9750 Mini-Project #00: Course Set-Up",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEarly in the course, I will ‘scaffold’ most of the analysis, leaving only some small steps for you to fill in. As the course progresses, the mini-projects will be more self-directed.↩︎\nAlternatively, simply go to https://github.com/new after logging in.↩︎\nYou can leave the next two boxes blank or set a custom directory name and location. RStudio’s defaults are reasonable; the default directory name will simply be STA9750-2024-FALL and it will be located in your home directory.↩︎\nIf you choose to complete the course using a pseudonym, make up something fun. If you are using your real name, this is a great place to state that you are a Baruch student, your expected graduation date, your field of employment (current or desired), and one or two personal facts. This, along with a LinkedIn page, will quickly become one of the first things that comes up when a potential employer searches your name, so make a good impression!↩︎"
  },
  {
    "objectID": "miniprojects/mini03.html",
    "href": "miniprojects/mini03.html",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Released to Students: 2024-10-24\nInitial Submission: 2024-11-13 11:45pm ET on GitHub and Brightspace\n\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-11-14 on GitHub\nPeer Feedback Due: 2024-11-20 11:45pm ET on GitHub"
  },
  {
    "objectID": "miniprojects/mini04.html",
    "href": "miniprojects/mini04.html",
    "title": "Mini-Project #04",
    "section": "",
    "text": "Due Dates\n\nReleased to Students: 2024-11-14\nInitial Submission: 2024-12-04 11:45pm ET on GitHub and Brightspace\n\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-12-05 on GitHub\nPeer Feedback Due: 2024-12-11 11:45pm ET on GitHub\n\n\n\n\nThis work ©2024 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "STA/OPR 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "Topics:\n\nInstalling R and RStudio\n\nInstalling git\n\nGetting Started on GitHub\n\nBasic Principles of “Clean Code”\n\n\nThe primary programming language used in this course is R, one of the two most popular languages used in data science. R, like its predecessor the S language, is optimized for interactive, data-analytic work, in contrast with python, which is optimized for general purpose computing.\nR is a programming language and runtime; we will supplement it with RStudio, an Integrated Development Environment or, less formally, an editor. RStudio is the software where you will write the code and then the R runtime will execute it.\n\nStudents should first install R from https://cloud.r-project.org/.\n\n\nDon’t fear the 90’s web design! Click image for detailed installation instructions.\n\nAs of 2024-08-26, the most recent version of R is 4.4.1. Using the most current version of R will reduce the likelihood of issues later in the course.\n\nNext, download and install the RStudio IDE (desktop edition).\n\n\nClick image for detailed installation instructions.\n\nRStudio is highly configurable and I recommend taking advantage of all its built-in features. If you go to the Global Options menu (accessible under Tools), I recommend the following settings:\n\nGeneral: Uncheck “Restore .RData into workspace at startup”.\nGeneral: Set “Save workspace to .RData on exit” to “Never”\nCode / Editing: Set “Tab width” to 2\nCode / Editing: Check\n\n“Insert spaces for Tab”\nAuto-detect code indentation\nInsert matching parens / quotes\nUse native pipe operator\nAuto-indent code after paste\nVertically align arguments in auto-indent\nContinue comment when inserting new line\n\n\nCode / Display: Check\n\nShow line numbers\nShow margin (margin column should be 80)\n\n\nCode / Diagnostics: check all “R” diagnostics.\nAppearance: Pick a color theme you enjoy. (I’m partial to light text on a dark background)\n\nYou may wish to enable GitHub Copilot. I have little experience with GH Copilot, but it seems quite popular and is allowed in this course. It is not guaranteed to be accurate at all times - and “the AI told me to” is not a valid excuse if your code is wrong - but on balance, it should be useful.\n\nWe won’t use it this week, but you will need to install Quarto before starting on Mini-Project #00.\n\n\ngit is a source-code management tool, used by developers to manage the code they write. If you’ve ever been part of a large project and struggled to coordinate all team members using the same version of a document, git exists to solve that problem.\nIf you don’t have git pre-installed, install either Git for Windows or the XCode Command Line Tools for MacOS. If not automatically prompted when you try to use git, the Mac install can be manually triggered by running xcode-select --install at a command line.\nIn this course, we will use three main functions of git:\n\n\nstaging: telling git, I want you to prepare to save a certain file\n\ncommitting: saving a set of related changes\n\npushing: copying your committed changes to a separate server for sharing and backup\n\nWhenever you write code you are happy with, you should use git to save it. Saving changes with git is cheap and easy - so do it regularly. You always want git to have a backup of good code in case you loose power, accidentally delete a file, break something in a way you’re not sure how to undo, etc..\nRStudio comes with powerful git integration. Once you have created a project, you should see a tab labelled “Git” in the top right corner of your IDE window that looks something like this:\n\nTo stage a file - prepare to save it - click the empty check box next to the file name. A new file shows a status of “?” - this is git saying “I’ve never seen this file before. Do you want me to track it for you?”. Later, when you make further changes to file you have already asked git to track, a status of “M” (for Modified) will be shown.\nOn its own staging a file does nothing. You also need to commit it for git to truly track it.1 The Commit button will commit all staged changes. When you make a commit, git requires a brief message summarizing the changes. There’s no particular formatting requirement to this message, but it should be something that future-you is able to easily understand. For instance, the commit message from the initial draft of this document reads as:\nInitial draft of Lab 01 (STA9750)\n\n- Installing R and RStudio\n- Git and GitHub\n- Leaflet Example for Styler\n\nTODO: Fuller shell explainers\nTODO: Link more git help\nWhen I read this, I know the purpose of the change I made (first line), the contents of that change (list), and parts that still need more work.\nFinally, after you save a change, it is only saved on your computer. The true power of git comes from its ability to copy changes and backups across machines. This gives you an easy way to store backups in case your computer dies and makes collaboration efficient and fun. git allows you to push and pull changes between machines in endlessly powerful (but sometimes complex) ways. For this course, we’ll keep things simple and only use GitHub to share code. We discuss GitHub in the next section.\nReference: We will not use all of the functionality of git in this course, but you should familiarize yourself with Chapters 1, 2, and 6 of the Git Book over the next two weeks.\n\nGitHub is an industry-standard code hosting and collaboration platform. In addition to hosting copies of code, GitHub provides web hosting, bug reporting, code review, continuous integration, documentation wikis, and discussion fora. You will explore GitHub in more detail starting in Mini-Project #00.\n\n\nA major theme of this course will be sharing and co-developing code with your classmates, both for peer feedback and for the course project. Code sharing is hard! Everyone writes code a little differently and what is clear to you may not be clear at all to your reader.\nTo make code sharing just a bit easier, we use tools to ensure all code shared in this course is consistently formatted. By using consistent formatting, you reduce the cognitive load on your reader, making it easier for them to focus on the ideas of your code, not how you chose to write it.\nA major strength of R is its huge number of user-contributed packages. These are “add-ins” which provide additional functionality not available in the basic version of R. As of 2024-08-26, there are over 21 thousand packages available on CRAN, the largest official repository of R packages. Beyond all those, there are thousands more packages available on other code hosting websites like GitHub.2\nWe will use the contributed styler package to format code in this course. Run the following command to automatically download and install the styler package:\n\ninstall.packages(\"styler\")\n\n(Use the clipboard icon on the right of code snippets to automatically copy code suitable for pasting into RStudio.)\nYou should see something like this:\n\nThe styler package has been downloaded and installed on your computer, but it is not yet “active” or “open” in R. In general, you will only need to download packages once, but you will need to load them each time you want to use them.3\nOpen a R file in RStudio and copy the following (ugly) code:\n\nif(!require(\"leaflet\")) install.packages(\"leaflet\")\nif(!require(\"tidyverse\")){\n    install.packages(\"tidyverse\")\n   }\n library(tidyverse)\n     library(rvest)\nlibrary(leaflet)\n\npAGE = read_html('https://en.wikipedia.org/wiki/Baruch_College')\n  pAGE |&gt; html_element(\".latitude\") |&gt; html_text2() -&gt; BaruchLatitude\n  baruch_longitude &lt;- pAGE |&gt; html_element(\".longitude\") |&gt; html_text2()\n  \n    BaruchLatitude &lt;- sum(as.numeric(strsplit(BaruchLatitude, \n                                     \"[^0123456789]\")[[1]]) * (1/60)^(0:2), na.rm=TRUE)\n baruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) * \n                             (1/60)^(0:2), na.rm=TRUE)\n  \nleaflet() %&gt;% addTiles() %&gt;% setView(-baruch_longitude, BaruchLatitude, zoom=17) %&gt;%\n    addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nYou don’t need to understand what this does just yet, but it’s hopefully clear that this is ugly code. Nothing is lined up properly, capitalization is erratic, and different coding styles are intermixed rather recklessly.\nNear the top of your RStudio pane, you will see a drop-down menu titled Addins. If you successfully installed styler above, one of the Addins choices will be “style active file.” Click this and the code will be cleaned up (a bit) resulting in something like this:\n\nif (!require(\"leaflet\")) install.packages(\"leaflet\")\nif (!require(\"tidyverse\")) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(leaflet)\n\npAGE &lt;- read_html(\"https://en.wikipedia.org/wiki/Baruch_College\")\npAGE |&gt;\n  html_element(\".latitude\") |&gt;\n  html_text2() -&gt; BaruchLatitude\nbaruch_longitude &lt;- pAGE |&gt;\n  html_element(\".longitude\") |&gt;\n  html_text2()\n\nBaruchLatitude &lt;- sum(as.numeric(strsplit(\n  BaruchLatitude,\n  \"[^0123456789]\"\n)[[1]]) * (1 / 60)^(0:2), na.rm = TRUE)\nbaruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) *\n  (1 / 60)^(0:2), na.rm = TRUE)\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(-baruch_longitude, BaruchLatitude, zoom = 17) %&gt;%\n  addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nIt’s far from perfect - and we will discuss the many issues in this example throughout the course - but it’s better! At a minimum, you should make sure to run styler like this on all code you submit during this course.\nAnd now that your code is cleaned up, you should run it! The Source button in the top right corner will run all code in the open file. Running the code produces something like this:\n\n\n\n\n\n\nNot too shabby! That’s an interactive, dynamic map showing the location of Baruch College obtained by parsing the Baruch Wikipedia page, getting the GPS coordinates of Baruch, downloading a map file, and locating Baruch on that map.\nChallenge: Adjust this code to show Hunter college instead of Baruch.\n\nIf you want even more feedback on writing good code, install the lintr package and use the associated RStudio add-in. Unlike styler, lintr won’t make changes automatically for you, but it will highlight much more subtle possible problems.4\n\nTo become a true “power user” of tools like R and python, you will need to become more familiar with the command line interface (CLI) and associated tools.5\nThe Software Carpentry Unix Shell Tutorial is a great introduction to shell usage. Check it out!\nNB: MacOS and Linux systems work quite similarly under the hood, as both descend from the Unix tradition. By contrast, Windows works somewhat differently. Learners whose personal machine runs Windows are encouraged to take advantage of the provided Linux-running virtual machines6 as they work through this section.\n\n\nNext week, we will use these tools to begin coding in earnest. If you’re feeling ambitious, go ahead and get started on Mini-Project #00."
  },
  {
    "objectID": "labs/lab01.html#r-and-rstudio",
    "href": "labs/lab01.html#r-and-rstudio",
    "title": "STA/OPR 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "The primary programming language used in this course is R, one of the two most popular languages used in data science. R, like its predecessor the S language, is optimized for interactive, data-analytic work, in contrast with python, which is optimized for general purpose computing.\nR is a programming language and runtime; we will supplement it with RStudio, an Integrated Development Environment or, less formally, an editor. RStudio is the software where you will write the code and then the R runtime will execute it.\n\nStudents should first install R from https://cloud.r-project.org/.\n\n\nDon’t fear the 90’s web design! Click image for detailed installation instructions.\n\nAs of 2024-08-26, the most recent version of R is 4.4.1. Using the most current version of R will reduce the likelihood of issues later in the course.\n\nNext, download and install the RStudio IDE (desktop edition).\n\n\nClick image for detailed installation instructions.\n\nRStudio is highly configurable and I recommend taking advantage of all its built-in features. If you go to the Global Options menu (accessible under Tools), I recommend the following settings:\n\nGeneral: Uncheck “Restore .RData into workspace at startup”.\nGeneral: Set “Save workspace to .RData on exit” to “Never”\nCode / Editing: Set “Tab width” to 2\nCode / Editing: Check\n\n“Insert spaces for Tab”\nAuto-detect code indentation\nInsert matching parens / quotes\nUse native pipe operator\nAuto-indent code after paste\nVertically align arguments in auto-indent\nContinue comment when inserting new line\n\n\nCode / Display: Check\n\nShow line numbers\nShow margin (margin column should be 80)\n\n\nCode / Diagnostics: check all “R” diagnostics.\nAppearance: Pick a color theme you enjoy. (I’m partial to light text on a dark background)\n\nYou may wish to enable GitHub Copilot. I have little experience with GH Copilot, but it seems quite popular and is allowed in this course. It is not guaranteed to be accurate at all times - and “the AI told me to” is not a valid excuse if your code is wrong - but on balance, it should be useful.\n\nWe won’t use it this week, but you will need to install Quarto before starting on Mini-Project #00."
  },
  {
    "objectID": "labs/lab01.html#source-code-management",
    "href": "labs/lab01.html#source-code-management",
    "title": "STA/OPR 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "git is a source-code management tool, used by developers to manage the code they write. If you’ve ever been part of a large project and struggled to coordinate all team members using the same version of a document, git exists to solve that problem.\nIf you don’t have git pre-installed, install either Git for Windows or the XCode Command Line Tools for MacOS. If not automatically prompted when you try to use git, the Mac install can be manually triggered by running xcode-select --install at a command line.\nIn this course, we will use three main functions of git:\n\n\nstaging: telling git, I want you to prepare to save a certain file\n\ncommitting: saving a set of related changes\n\npushing: copying your committed changes to a separate server for sharing and backup\n\nWhenever you write code you are happy with, you should use git to save it. Saving changes with git is cheap and easy - so do it regularly. You always want git to have a backup of good code in case you loose power, accidentally delete a file, break something in a way you’re not sure how to undo, etc..\nRStudio comes with powerful git integration. Once you have created a project, you should see a tab labelled “Git” in the top right corner of your IDE window that looks something like this:\n\nTo stage a file - prepare to save it - click the empty check box next to the file name. A new file shows a status of “?” - this is git saying “I’ve never seen this file before. Do you want me to track it for you?”. Later, when you make further changes to file you have already asked git to track, a status of “M” (for Modified) will be shown.\nOn its own staging a file does nothing. You also need to commit it for git to truly track it.1 The Commit button will commit all staged changes. When you make a commit, git requires a brief message summarizing the changes. There’s no particular formatting requirement to this message, but it should be something that future-you is able to easily understand. For instance, the commit message from the initial draft of this document reads as:\nInitial draft of Lab 01 (STA9750)\n\n- Installing R and RStudio\n- Git and GitHub\n- Leaflet Example for Styler\n\nTODO: Fuller shell explainers\nTODO: Link more git help\nWhen I read this, I know the purpose of the change I made (first line), the contents of that change (list), and parts that still need more work.\nFinally, after you save a change, it is only saved on your computer. The true power of git comes from its ability to copy changes and backups across machines. This gives you an easy way to store backups in case your computer dies and makes collaboration efficient and fun. git allows you to push and pull changes between machines in endlessly powerful (but sometimes complex) ways. For this course, we’ll keep things simple and only use GitHub to share code. We discuss GitHub in the next section.\nReference: We will not use all of the functionality of git in this course, but you should familiarize yourself with Chapters 1, 2, and 6 of the Git Book over the next two weeks.\n\nGitHub is an industry-standard code hosting and collaboration platform. In addition to hosting copies of code, GitHub provides web hosting, bug reporting, code review, continuous integration, documentation wikis, and discussion fora. You will explore GitHub in more detail starting in Mini-Project #00."
  },
  {
    "objectID": "labs/lab01.html#code-styling",
    "href": "labs/lab01.html#code-styling",
    "title": "STA/OPR 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "A major theme of this course will be sharing and co-developing code with your classmates, both for peer feedback and for the course project. Code sharing is hard! Everyone writes code a little differently and what is clear to you may not be clear at all to your reader.\nTo make code sharing just a bit easier, we use tools to ensure all code shared in this course is consistently formatted. By using consistent formatting, you reduce the cognitive load on your reader, making it easier for them to focus on the ideas of your code, not how you chose to write it.\nA major strength of R is its huge number of user-contributed packages. These are “add-ins” which provide additional functionality not available in the basic version of R. As of 2024-08-26, there are over 21 thousand packages available on CRAN, the largest official repository of R packages. Beyond all those, there are thousands more packages available on other code hosting websites like GitHub.2\nWe will use the contributed styler package to format code in this course. Run the following command to automatically download and install the styler package:\n\ninstall.packages(\"styler\")\n\n(Use the clipboard icon on the right of code snippets to automatically copy code suitable for pasting into RStudio.)\nYou should see something like this:\n\nThe styler package has been downloaded and installed on your computer, but it is not yet “active” or “open” in R. In general, you will only need to download packages once, but you will need to load them each time you want to use them.3\nOpen a R file in RStudio and copy the following (ugly) code:\n\nif(!require(\"leaflet\")) install.packages(\"leaflet\")\nif(!require(\"tidyverse\")){\n    install.packages(\"tidyverse\")\n   }\n library(tidyverse)\n     library(rvest)\nlibrary(leaflet)\n\npAGE = read_html('https://en.wikipedia.org/wiki/Baruch_College')\n  pAGE |&gt; html_element(\".latitude\") |&gt; html_text2() -&gt; BaruchLatitude\n  baruch_longitude &lt;- pAGE |&gt; html_element(\".longitude\") |&gt; html_text2()\n  \n    BaruchLatitude &lt;- sum(as.numeric(strsplit(BaruchLatitude, \n                                     \"[^0123456789]\")[[1]]) * (1/60)^(0:2), na.rm=TRUE)\n baruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) * \n                             (1/60)^(0:2), na.rm=TRUE)\n  \nleaflet() %&gt;% addTiles() %&gt;% setView(-baruch_longitude, BaruchLatitude, zoom=17) %&gt;%\n    addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nYou don’t need to understand what this does just yet, but it’s hopefully clear that this is ugly code. Nothing is lined up properly, capitalization is erratic, and different coding styles are intermixed rather recklessly.\nNear the top of your RStudio pane, you will see a drop-down menu titled Addins. If you successfully installed styler above, one of the Addins choices will be “style active file.” Click this and the code will be cleaned up (a bit) resulting in something like this:\n\nif (!require(\"leaflet\")) install.packages(\"leaflet\")\nif (!require(\"tidyverse\")) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(leaflet)\n\npAGE &lt;- read_html(\"https://en.wikipedia.org/wiki/Baruch_College\")\npAGE |&gt;\n  html_element(\".latitude\") |&gt;\n  html_text2() -&gt; BaruchLatitude\nbaruch_longitude &lt;- pAGE |&gt;\n  html_element(\".longitude\") |&gt;\n  html_text2()\n\nBaruchLatitude &lt;- sum(as.numeric(strsplit(\n  BaruchLatitude,\n  \"[^0123456789]\"\n)[[1]]) * (1 / 60)^(0:2), na.rm = TRUE)\nbaruch_longitude &lt;- sum(as.numeric(strsplit(baruch_longitude, \"[^0123456789]\")[[1]]) *\n  (1 / 60)^(0:2), na.rm = TRUE)\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(-baruch_longitude, BaruchLatitude, zoom = 17) %&gt;%\n  addPopups(-baruch_longitude, BaruchLatitude, \"Look! It's &lt;b&gt;Baruch College&lt;/b&gt;!\")\n\nIt’s far from perfect - and we will discuss the many issues in this example throughout the course - but it’s better! At a minimum, you should make sure to run styler like this on all code you submit during this course.\nAnd now that your code is cleaned up, you should run it! The Source button in the top right corner will run all code in the open file. Running the code produces something like this:\n\n\n\n\n\n\nNot too shabby! That’s an interactive, dynamic map showing the location of Baruch College obtained by parsing the Baruch Wikipedia page, getting the GPS coordinates of Baruch, downloading a map file, and locating Baruch on that map.\nChallenge: Adjust this code to show Hunter college instead of Baruch.\n\nIf you want even more feedback on writing good code, install the lintr package and use the associated RStudio add-in. Unlike styler, lintr won’t make changes automatically for you, but it will highlight much more subtle possible problems.4"
  },
  {
    "objectID": "labs/lab01.html#extra-welcome-to-shell",
    "href": "labs/lab01.html#extra-welcome-to-shell",
    "title": "STA/OPR 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "To become a true “power user” of tools like R and python, you will need to become more familiar with the command line interface (CLI) and associated tools.5\nThe Software Carpentry Unix Shell Tutorial is a great introduction to shell usage. Check it out!\nNB: MacOS and Linux systems work quite similarly under the hood, as both descend from the Unix tradition. By contrast, Windows works somewhat differently. Learners whose personal machine runs Windows are encouraged to take advantage of the provided Linux-running virtual machines6 as they work through this section."
  },
  {
    "objectID": "labs/lab01.html#looking-ahead",
    "href": "labs/lab01.html#looking-ahead",
    "title": "STA/OPR 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "",
    "text": "Next week, we will use these tools to begin coding in earnest. If you’re feeling ambitious, go ahead and get started on Mini-Project #00."
  },
  {
    "objectID": "labs/lab01.html#footnotes",
    "href": "labs/lab01.html#footnotes",
    "title": "STA/OPR 9750 Week 1 In-Class Activity: R and RStudio\n",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis two stage process is a bit cumbersome for the first stage of a small project, but it quickly becomes incredibly valuable. Instead of saving everything every time, there is great power in only saving “good” or “finished” changes to a large project, while leaving work-in-progress elsewhere unsaved. You probably won’t need this level of control until you get to the course project, but it’s better to have it than not.↩︎\nIf you are interested in bioinformatics, the Bioconductor project develops incredible open-source R packages.↩︎\nWhile this may feel cumbersome, it’s really not dissimilar to any other software you use (or R itself). You need to download it once, but you need to open it each time you intend to use it. There’s no harm in re-downloading–free software!–but it wastes time and bandwidth. Since we benefit so much from the free-software community, the very least we can do is not run up their internet bills unnecessarily.↩︎\nSome of the issues identified by lintr may be false positives, but the false positive rate is quite low, especially for the sort of procedural code that is the focus of this course. You should default to trying to appease lintr, but feel free to use the course discussion board for any questions.↩︎\nAs an added benefit, use of the CLI also makes you look like a 90s movie hacker to all your friends.↩︎\nSee the Course Resources page.↩︎"
  },
  {
    "objectID": "labs/lab02.html",
    "href": "labs/lab02.html",
    "title": "STA/OPR 9750 Week 2 In-Class Activity: Getting Down with Markdown",
    "section": "",
    "text": "Welcome!\nSlides"
  },
  {
    "objectID": "labs/lab03.html",
    "href": "labs/lab03.html",
    "title": "STA/OPR 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "labs/lab03.html#review-of-r",
    "href": "labs/lab03.html#review-of-r",
    "title": "STA/OPR 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Review of R",
    "text": "Review of R\nIn this section, we will review some of the basic ‘built-in’ features of R. In the next section (Packages) we will discuss how to add to the “base-bones” functionality. When working with R, there are two interacting ‘subsystems’ in play:\n\nThe R language and interpreter: this is the part of R that is similar to python or C/C++. You will write R code in the R language and the R interpreter will run that code. The fact that all of these elements are called R is a bit confusing, but once you get the hang of things, the distinctions will melt away.\nR packages: When working in R, you do not have to start from scratch every time. Other programmers make sets of code available to you in the form of packages. For our purposes, a package can contain two things:\n\nPre-written functions to help you achieve some goal\nData sets Most of the time, the primary purpose of a package is sharing functions and code: there are easier ways to share data with the world.\n\n\nWhen you first downloaded R, you downloaded the interpreter and a set of base packages written by the “R Core Development Team”.\nRun the following code to see what your R environment looks like:\n\n\n\n\n\n\n\n\nCompare the output of running this here-in the browser-with what you get by running sessionInfo() on your machine.\nThere is lots of useful information here, including\n\nthe version of R being used\nthe operating system\nthe numerical linear algebra libraries (BLAS, LAPACK) used\nsystem language and time zone information\nloaded packages\n\nWhen asking for help, always include the output of the sessionInfo() command so that your helper can quickly know how your system is configured.\n\nPackages\nR code is distributed as packages, many of which come included with R by default. These are the base packages, and they are noted in your sessionInfo(). But we can do many more things with R using contributed (non-base) packages!\nThe most common platform for distributing R packages is CRAN, the Comprehensive R Archive Network, available at https://cran.r-project.org/. You have likely already visited this site to download R. The available.packages() function in R lists all packages currently on CRAN. We can see that there are many:\n\nNROW(available.packages())\n\n[1] 21510\n\n\nIf you want to use a contributed package, you need to do two things:\n\nDownload it from CRAN and install it onto your computer (one time)\nLoad it from your hard drive into R (every time you restart R)\n\nThe first step - download and install - can be completed using the install.packages() function. For example, to install the palmerpenguins package, I would run:\n\ninstall.packages(\"palmerpenguins\")\n\nThis will automatically download and install this package for me. R is helpful and also tries to automatically install all packages that a given package relies upon. Because of this, it is often sufficient to install the “last step” and trust R to handle the dependencies automatically. In this course, most of the packages we use can be automatically installed by installing the tidyverse package.\n\ninstall.packages(\"tidyverse\")\n\nNote that there really isn’t much in the tidyverse package we want, but it’s a useful proxy for a much larger set of packages.\nOnce a package is installed, we need to load it into R with the library function:\n\nlibrary(palmerpenguins)\n\nAfter doing this, we have access to the contents of the palmerpenguins package until we restart R.\n\nNote that the install.packages function wants you to quote its argument, but library does not. This is a weird historical quirk of R that you will trip up on many times before this course ends.\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\nFor example, palmerpenguins package provides a data set of penguin measurements. If we try to get the data set without loading palmerpenguins, we get an error message:\n\n\n\n\n\n\n\n\nAfter we install and load palermerpenguins, we are good to go:\n\n\n\n\n\n\n\n\nSo much tuxedo goodness!\nIn general, if you get a error message of the form Error: object 'X' not found, you should:\n\nMake sure you spelled X properly\nIf X comes from a package, make sure you library() that package.\n\nThere’s no harm to library()-ing a package multiple times; if you install.packages() a package that you have already loaded, you may need to restart R.\n\nAs mentioned last week, I strongly recommend never saving your workspace in R or RStudio. One of the things “saved” in a workspace is the list of loaded packages, so it becomes essentially impossible to reinstall a package properly.\n\n\n\nVariables and Assignment\nWhenever you type a “word” of R code, it must be one of three things:\n\nA reserved word: this is a small set of keywords that R keeps for its own use. These have special rules for their use that we’ll learn as we go along. The main ones are: if, else, for, in, while, function, repeat, break, and next.\nIf you use one of these words and get a weird error message, it’s likely because you aren’t respecting the special rules for these words.\nFor the nitty gritty, see the Reserved help page but feel free to skip this for now. The Control help page gives additional details.\n(When you run a help page in this tutorial, it looks a bit funny. Try running ?Reserved directly in RStudio for better formatting.)\nA “literal”. This is a word that represents “just the thing” without any additional indirection. The most common types of literals are:\n\nNumeric: e.g., 3, 42.0, or 1e-3\nString: e.g., 'a', \"beach\", or 'cream soda' There are a few rules for literals, but the most important is that strings begin and end with the same character, either a single quote or a double quote. When R sees a single quote, it will read everything until the next single quote as one string, even if there’s a double quote inside.\n\nTry some literals:\n\n\n\n\n\n\n\n\n\nWhat does the literal 0xF represent? (You don’t need to worry about why. This is a fancier literal than we will use in this class.) - A “variable name”. This is the most common sort of “word” in code. It is used to something without actually having to know what it is.\nWe can create variables using the “assignment” operator: &lt;-\n\n\n\n\n\n\n\n\nWhen you read this outloud, read &lt;- as “gets” so x &lt;- 3 becomes “x gets 3.”\nWhen we use the assignment operator on a variable, it overwrites the value of a variable silently and without warning\n\n\n\n\n\n\n\n\nWe also put expressions on the right hand side of an assignment:\n\n\n\n\n\n\n\n\nAlso note the trick we’ve used here a few times: a “plain” line of code without an assignment generally prints its value.\n\n\nComments\nWhen you include a # symbol, R will ignore everything after it. This is called a comment and you can use it to leave notes to yourself about what you are doing and why.\n\n\nVector Data Types\nA vector is a ordered array of the same sort of thing (e.g., all numbers or all strings). We can create vectors using the c command (short for concatenate).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the above example to c(1, \"a\", 3) and examine the output. What happened? Why?\n\n\n\nTo see the type (sort) of a vector, you can use the str command.\n\n\n\n\n\n\n\n\nstr(x) tells us about the structure of x. Here, we see that x is a numeric vector of length 3.\nR will try to do the right thing when doing arithmetic on vectors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen you give R vectors of different lenghts, it will “recycle” the shorter one to the length of the longer one.\n\n\n\n\n\n\n\n\nThis can be a double-edge sword when the two vectors don’t fit together so nicely:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow was the last element of x+y computed?\n\n\n\nHere we see also that R printed a warning message. A warning message is R’s way of saying “something is funny, but I can still do this” while it (successfully) implements your command. It’s here to help you, but sometimes can be safely ignored if you’re sure about what you’re doing.\nAn error is a “I can’t do this” message. When R encounters an error it stops and does not fully execute the command\n\n\n\n\n\n\n\n\nHere we get an error because there is no meaningful way to multiply a string by a number, unlike earlier where the recycling rule told R what to do, even if it was probably a bad idea."
  },
  {
    "objectID": "labs/lab03.html#functions",
    "href": "labs/lab03.html#functions",
    "title": "STA/OPR 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Functions",
    "text": "Functions\n\nFunctions\nIn many of these exercises, we have used commands that have the form NAME() with zero or more comma-separated elements in the parentheses.\nThis represents a function call. Specifically, the command func(x, y) calls the function named func with two arguments x and y.\nFunctions are the verbs of the programming world. They are how anything gets done. So far, we’ve only used some basic functions:\n\nc: the concatenate function\nprint: the print function\nstr: the structure function\nlist: the list making function\n\nBut there are tons of other useful ones!\nTry these out: - length on a vector - colnames on a data frame (like PlantGrowth) - toupper on a string (vector) - as.character on a numeric value\n\n\n\n\n\n\n\n\n\nArguments: Positional and Keyword\nThe inputs to a function are called the arguments. They come in two forms: - Positional - Keyword\nSo far we have only seen positional arguments. The function interprets them in an order that depends on they were given:\n\n\n\n\n\n\n\n\nHere paste combines two values into a string. We get different output strings depending on the order of the input.\nOther arguments can be passed as keyword arguments. Keyword arguments come with names that tell functions how to interpret them. For example, the paste function has an optional keyword argument sep that controls how the strings are combined.\n\n\n\n\n\n\n\n\nKeyword arguments typically have defaults so you don’t need to always provide them. For the paste function, the sep defaults to \" \".\n\n\n\nCreating Your Own Functions\nWhen you want to create your own function, you use a variant of the assignment structure\n\nmy_addition &lt;- function(x, y) {\n    x + y\n}\n\nLet’s break this into pieces:\n\nOn the left hand side of the assignment operator &lt;-, we see the function name. This works exactly the same as vector assignment.\nImmediately to the right of the assignment operator, we see the keyword function. This tells R that we are defining a function.\nAfter the word function, we see the “argument list”, i.e., the list of inputs to the function (comma separated). Here, we are not providing default values for any function.\nFinally, between the curly braces, we get the body of the function. This is actually the code defining a function’s behavior. You can do basically anything here! Define variables, do arithmetic, load packages, call other functions - it’s all valid. (In fact, you can even define a function within a function, but that’s sort of advanced.)\nThe last line of the body (here the only line) defines the return value of the function, i.e., its output.\n\nThis function will add two numbers together. Now that we’ve defined it, we can use it just like a built-in function:\n\nmy_addition(2, 4)\n\n[1] 6\n\n\nTip: You can see the code used to define any function by simply printing it: think of the code as being the “value” and the function name as the variable name. (This isn’t actually just a metaphor - it’s literally true!)\n\nDefault Arguments\nSometimes, we want functions to have default but changeable behvaior. This is default arguments come in. If the user provides a value, the function uses it, but otherwise the default is used.\nFor example,\n\nmake_bigger &lt;- function(x, by=2){\n    x + by\n}\n\nmake_bigger(3)\n\n[1] 5\n\nmake_bigger(3, by = 3)\n\n[1] 6\n\n\nHere by defaults to 2, but the user is required to supply x because it has no default.\n\nmake_bigger(by=3)\n\nError in make_bigger(by = 3): argument \"x\" is missing, with no default\n\n\nThere are lots of details in the mechanics - they even can be ‘dynamic’ using some tricks - but in general, they should “just work.”"
  },
  {
    "objectID": "labs/lab03.html#control-flow",
    "href": "labs/lab03.html#control-flow",
    "title": "STA/OPR 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Control Flow",
    "text": "Control Flow\nSo far, all of the code we have written executes linearly, one line at a time. To write complex programs, however, we sometimes need code to execute in other ways: e.g., going line by line through a complex data set running the same code (a “loop”) or doing different things depending on the value of a variable (a “conditional”). This brings us to the topic of control flow, or how a program gets executed.\n\nConditionals\nPerhaps the most common control flow operator is the “conditional” - the if operator. In R, the if operator looks like this:\n\nif(TEST){\n    # Some code goes here\n    # This gets run if `TEST` is true\n} else {\n    # Some code goes here\n    # This gets run if `TEST` if false\n}\n\nFor example\n\nx &lt;- 3\nif(x &gt; 0){\n    print(\"x is positive!\")\n} else {\n    print(\"x is negative\")\n}\n\n[1] \"x is positive!\"\n\n\nThe element inside the if (the test statement) should ideally be Boolean (TRUE/FALSE-ish) but R will make a reasonable guess if it isn’t.\nNote that you can omit the else part and the second set of braces that go with it, but the first set of braces, immediately after if(), should always be there.\n\n\n\n\n\n\n\n\nChange the value of x and see what happens. Next, modify this by adding an else statement to handle the case of odd numbers.\nNote that we’re using the %% operator here. If you haven’t seen it before, recall you can get help by typing\n\n?%%\n\nin R. In this case, %% is a modulo operator; that is, it is the “remainder” from division. (Do you see how it works here?)\nWe’ll practice using conditional operators below.\n\n\nLooping\nIn other ## Programming Exercises\nWrite functions to perform each of the following tasks.\n\nWrite a function that takes in a vector of numbers, calculates the length and maximum value of the vector, and prints that information to the screen in a formatted way.\n\n&gt; func_1(c(1, 2, 3, 5, 7))\nThe largest value in that list of 5 numbers is 7.\n\n&gt; func_1(c(1, 2, 5, 5))\nThe largest value in that list of 4 numbers is 5.\nTo make your output as attractive as possible, you might want to use the cat command instead of the print command.\n\nWrite a program that tests whether its (integer) outputs are leap years. Recall the leap year rules:\n\nA year is a leap year if it is divisible by 4\nBut it is not a leap year if it is divisible by 100\nUnless it is also divisible by 400\n\n\n&gt; leap_year(2023)\nFALSE\n&gt; leap_year(2024)\nTRUE\n&gt; leap_year(2100)\nFALSE\n&gt; leap_year(2000)\nTRUE\nRemember our discussion of the %\\% operator from class.\n\nWrite a function to greet your classmates with varying levels of enthusiasm. It should have three optional arguments:\n\nname. The name of the person to greet. Default \"friend\"\ntimes. The number of times to repeat the greeting.\nemphasis. A Boolean (TRUE/FALSE) value indicating whether the greeting should end with an exclamaition point. (Default FALSE)\n\n\n&gt; greetings()\nHello, friend\n&gt; greetings(name=\"Michael\")\nHello, Michael\n&gt; greetings(times=2)\nHello, friend\nHello, friend\n&gt; greetings(emphasis=TRUE)\nHello, friend!\n&gt; greetings(\"Michael\", 5, TRUE)\nHello, Michael!\nHello, Michael!\nHello, Michael!\nHello, Michael!\nHello, Michael!\n\nThe Riemann Zeta Function is a famous function in analytic number theory1 defined as \\[\\zeta(k) = 1 + \\left(\\frac{1}{2}\\right)^k + \\left(\\frac{1}{3}\\right)^k + \\dots = \\sum_{i=1}^{\\infty} i^{-k} \\] We cannot implement an infinite series in R, but we can get very close by taking a large number of terms in the series (e.g, the first 500,000). Implement the zeta function and show that \\(\\zeta(2) = \\frac{\\pi^2}{6}\\)\n\n&gt; zeta(2)\n[1] 1.644932\n&gt; zeta(3)\n[1] 1.202057\n&gt; zeta(4)\n[1] 1.082323\n&gt; all.equal(zeta(2), pi^2/6, tol=1e-4)\n[1] TRUE\n\nHero of Alexandria developed a method for computing square roots numerically. He showed that by performing the following update repeatedly, \\(x\\) will converge to \\(\\sqrt{n}\\): \\[x \\leftarrow \\frac{1}{2}\\left(x + \\frac{n}{x}\\right)\\] You can start with any positive \\(x\\), but \\(n/2\\) is a good choice.\nImplement this method to compute square roots. Use an optional keyword argument (default value 100) to control how many iterations are performed:\n\n&gt; hero_sqrt(100)\n[1] 1.644932\n&gt; hero_sqrt(3)^2\n[1] 3\n&gt; hero_sqrt(3, iter=2)\n[1] 1.732143\n&gt; hero_sqrt(3000)\n[1] 54.77226"
  },
  {
    "objectID": "labs/lab03.html#footnotes",
    "href": "labs/lab03.html#footnotes",
    "title": "STA/OPR 9750 Week 3 In-Class Activity: R, These are your First Steps",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nANL is basically the application of calculus techniques to prove properties of prime numbers: it’s a surprisingly powerful approach.↩︎"
  },
  {
    "objectID": "labs/lab04.html#rs-missing-data-model---na",
    "href": "labs/lab04.html#rs-missing-data-model---na",
    "title": "STA/OPR 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "R’s Missing Data Model - NA",
    "text": "R’s Missing Data Model - NA\nLet’s begin with a simple question: what is the single most delayed flight in our data set:\n\n\n\n\n\n\n\n\nHmmmm…That’s odd. Certainly something has to be the maximum arrival delay - why did we get no rows back?\nLet’s look at this expression more closely: firstly, what happens if we simply fix a delay amount?\n\n\n\n\n\n\n\n\nThat’s fine. So perhaps the problem was in computing max(arr_delay).\n\n\n\n\n\n\n\n\nThat’s weird - what is this NA object?\nNA is R’s representation of missing data: this is not a NaN object you have seen from other languages. NaN represents invalid arithmetic output (Not-a-Number), e.g,\n\n\n\n\n\n\n\n\nNA is statistical missingness. The data exists - and is well defined - but we simply don’t know it. Like we said above, there is some most delayed flight, but we don’t know what it is.\nThe NA construct is a bit odd when you start with it - but it’s actually one of R‘s great strengths. Missingness matters in data analysis and R forces you to deal with it explicitly. The behavioral rules of NA are reasonably straightforward - NA is ’contagious’. Any calculation that takes at least one NA input usually has NA output. (This is not dissimilar to the “random in, random out” rule of functions of random variables) For example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat last result may be a bit surprising - isn’t anything times zero just zero?\nThat’s true in ‘real’ math, but not actually true for computer (“floating-point”) math:\n\n\n\n\n\n\n\n\nHere, because 0 * NA could be 0 or NaN, the answer is still unknown and hence NA.\nThere are some rare operations where NA can be “over-ruled” but they are not super common:\n\n\n\n\n\n\n\n\nThis follows because both:\n\n\n\n\n\n\n\n\nso the value of NA doesn’t actually matter here.\nAlso note that not all NA values are ‘the same’:\n\n\n\n\n\n\n\n\nWhy is this the case? Well, suppose we rewrite this as:\n\n\n\n\n\n\n\n\nIs today the same temperature as tomorrow? If we don’t know either temperature, we can’t say!\nSimilarly,\n\n\n\n\n\n\n\n\n\nis.na and na.rm\nWhile it’s certainly helpful that R handles NA values so intelligently for us, it can also be a bit annoying. Eventually we want (non-NA) answers!\nWe generally deal with this in one of two ways:\n\nfiltering out NA values from our data set\nignoring NA values in our calculations.\n\nWe’ve already done a bit of the latter option - ignoring NA values in our calculations - so let’s review it first.\nMost base R functions have an na.rm optional argument to remove NA values. Returning to our motivating example:\n\n\n\n\n\n\n\n\nor indeed\n\n\n\n\n\n\n\n\nThat’s a horrendous (21+ hour) delay! But is it actually the “maximum” delay? It depends… we’ll come back to this example in a bit.\nNot all functions, however, provide a na.rm argument: in those cases, it’s our responsibility to remove the NA values ourselves.\nWe can do this using the is.na function: this takes in a vector of values and finds the NAs:\n\n\n\n\n\n\n\n\nIf we combine this with the filter operator, we now have an efficient way of removing NA values:\n\n\n\n\n\n\n\n\nFrom here, we can get back to our analysis of the most delayed flight:\n\n\n\n\n\n\n\n\nPoor folks!\nNote that I’m using glimpse here to ensure all columns are printed.\n\n\ndrop_na\ndplyr provides a drop_na function which removes any row that has an NA value in any column. It’s a bit of a blunt approach - do you really need remove a row in computing X just because it has an NA value in column Y? - but it can be useful for “quick and dirty” work. I recommend against using it without a thorough manual examination of your data first however.\n\n\nNA values in filter\nEarlier we saw that filter plays funny with NA values. It’s worth being explicit here\n\n\n\n\n\n\n\n\nfilter checks for TRUE conditions - not for “not FALSE”. Because of this, checks which result in NA lead to dropped rows. This means that most NA rows are automatically discarded when you start filtering.\nThis isn’t a bad default - but it’s one you should be aware of. For instance, in our motivating example:\nWe might use the following to compute the average arrival delay:\n\n\n\n\n\n\n\n\nbut this drops\n\n\n\n\n\n\n\n\nflights for which we have no arrival delay information. Of these,\n\n\n\n\n\n\n\n\nwe even have an arrival time but the delay itself is missing for some reason. Is it fair to exclude these flights or should we compute the delay ourselves? For flights that are missing arrival and departure times (i.e., cancelled flights), should we exclude them? Are they infinitely delayed? 24 hour delayed (assuming passengers were rebooked to the same flight on the next day)?\nThere’s no clear right-or-wrong answer to questions like this. It’s all context dependent: if you are the DOT trying to ensure good customer experience, a cancelled flight is very delayed; if you are instead a Boeing engineer looking to improve flight speeds, the cancelled flights simply aren’t useful to you.\nWhen faced with these challenges, data scientists often give the answer “defer to subject matter experts (SMEs)”. Unfortunately, we rarely have the resources to have a qualified SME at hand to answer ever little data analytic question we may have.\nI instead advocate for a strategy of reproducible transparency. Using tools like quarto, we can show our code and document the choices made. Then, when we share our results with an SME,"
  },
  {
    "objectID": "labs/lab04.html#boolean-operators-and-filter",
    "href": "labs/lab04.html#boolean-operators-and-filter",
    "title": "STA/OPR 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "Boolean Operators and filter",
    "text": "Boolean Operators and filter\nfilter() lets you use a logical test to extract specific rows from a data frame. To use filter(), pass it the data frame followed by one or more logical tests. filter() will return every row that passes each logical test.\nSo for example, we can use filter() to select every flight in flights that departed on January 1st:\n\n\n\n\n\n\n\n\nThe filter function is similar to the WHERE clause in SQL. As we will later see, it can also be used to implement the HAVING clause, when applied in conjunction with group_by.\nLike all dplyr functions, filter() returns a new data frame for you to save or use. It doesn’t overwrite the old data frame. If you want to save the output of filter(), you’ll need to use the assignment operator, &lt;-.\nRerun the command in the code chunk below, but first arrange to save the output to an object named jan1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood job! You can now see the results by running the name jan1 by itself. Or you can pass jan1 to a function that takes data frames as input.\nDid you notice that this code used the double equal operator, ==? == is one of R’s logical comparison operators. Comparison operators are key to making full use of filter(), so let’s take a closer look at them.\n\nLogical Comparisons\nR provides a suite of comparison operators that you can use to compare values: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). Each creates a logical test. For example, is pi greater than three?\n\npi &gt; 3\n\n[1] TRUE\n\n\nWhen you place a logical test inside of filter(), filter applies the test to each row in the data frame and then returns the rows that pass, as a new data frame.\nOur code above returned every row whose month value was equal to one and whose day value was equal to one.\n\nWatch out!\nWhen you start out with R, the easiest mistake to make is to test for equality with = instead of ==. When this happens you’ll get an informative error:\n\n\n\n\n\n\n\n\nIf you give filter() more than one logical test, filter() will combine the tests with an implied “and.” In other words, filter() will return only the rows that return TRUE for every test. You can combine tests in other ways with Boolean operators…\n\n\n&, |, and !\nR uses Boolean operators to combine multiple logical comparisons into a single logical test. These include & (and), | (or), ! (not or negation), and xor() (exclusive or).\nBoth | and xor() will return TRUE if one or the other logical comparison returns TRUE. xor() differs from | in that it will return FALSE if both logical comparisons return TRUE. The name xor stands for exclusive or.\nStudy the diagram below to get a feel for how these operators work.\n\n\n\nIn the figure above, x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each command selects.\n\n\n\n\nCommon mistakes\nIn R, the order of operations doesn’t work like English. You can’t write filter(flights, month == 11 | 12), even though you might say “finds all flights that departed in November or December”. Be sure to write out a complete test on each side of a Boolean operator.\nHere are four more tips to help you use logical tests and Boolean operators in R:\n\nA useful short-hand for this problem is x %in% y. This will select every row where x is one of the values in y. We could use it to rewrite the code in the question above:\n\n\n\n\n\n\n\n\n\n\nSometimes you can simplify complicated subsetting by remembering De Morgan’s laws: !(x & y) is the same as !x | !y, and !(x | y) is the same as !x & !y. For example, if you wanted to find flights that weren’t delayed (on arrival or departure) by more than two hours, you could use either of the following two filters:\n\n\n\n\n\n\n\n\n\n\nAs well as & and |, R also has && and ||. Don’t use them with filter()! You’ll learn when you should use them later.\nWhenever you start using complicated, multipart expressions in filter(), consider making them explicit variables instead. That makes it much easier to check your work.\n\n\n\n\nExercises\n\nFilter Statements\nUsing filter and various Boolean operators, find all flights satisfying the following conditions.\n\nHad an arrival delay of two or more hours\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(arr_delay &gt; 120)\nflights |&gt; filter(arr_delay &gt; 120)\n\n\n\n\n\n\n\nFlew to Houston (IAH or HOU)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(dest %in% c(\"IAH\", \"HOU\"))\nflights |&gt; filter(dest %in% c(\"IAH\", \"HOU\"))\n\n\n\n\n\n\n\nWere operated by United (UA), American (AA), or Delta (DL)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\nflights |&gt; filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n\n\n\n\n\n\nDeparted in summer (June, July, or August)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(month &gt;= 6, month &lt;= 8)\nflights |&gt; filter(month &gt;= 6, month &lt;= 8)\n\n\n\n\n\n\n\nArrived more than two hours late, but didn’t leave late\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(arr_delay &gt; 120, dep_delay &lt;= 0)\nflights |&gt; filter(arr_delay &gt; 120, dep_delay &lt;= 0)\n\n\n\n\n\n\n\nWere delayed more than an hour, but made up more than 30 minutes in flight\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter(dep_delay &gt; 60, (dep_delay - arr_delay) &gt; 30)\nflights |&gt; filter(dep_delay &gt; 60, (dep_delay - arr_delay) &gt; 30)\n\n\n\n\n\n\n\nDeparted between midnight and 6am (inclusive)\n\n\n\n\n\n\n\n\n\n\n\n\nflights |&gt; filter((dep_time &lt;= 600) | (dep_time == 2400))\nflights |&gt; filter((dep_time &lt;= 600) | (dep_time == 2400))"
  },
  {
    "objectID": "labs/lab04.html#grouped-operations",
    "href": "labs/lab04.html#grouped-operations",
    "title": "STA/OPR 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "Grouped Operations",
    "text": "Grouped Operations\nIn this week’s preassignment, you also already saw the basics of the group_by operator for performing analyses on subgroups. The most common use of group_by is to modify summarize to perform group-wise summarization. We’ll next explore how it can be used to do group level filtering, similar to an SQL HAVING clause.\nAs before, let’s start by asking what is the average arrival delay (after removing NA values)?\n\n\n\n\n\n\n\n\nOk. But now suppose we want to know which carrier had flights that were later than average? We_could_ simply copy the value over into a new line of code:\n\n\n\n\n\n\n\n\nTo get carrier-wise statistics, we might try:\n\n\n\n\n\n\n\n\nThis works, but it requires us to keep the number 6.9 at hand, which is a bit inconvenient.\nWe next might be tempted to use a variable here to avoid hard-coding a specific value:\n\n\n\n\n\n\n\n\nThis is definitely better! If our data changes, we don’t have to worry about the number 6.9 being ‘out of date’. But it’s still maybe a bit clunky: we filter our data twice for NA values and have to repeat ourselves.\nLet’s try something else:\n\n\n\n\n\n\n\n\nThis creates a new column called mean delay. On its own, it’s not very interesting:\n\n\n\n\n\n\n\n\nNote the trick of using everything() inside a select statement to reorder columns.\nThe mean_delay column simply repeats the number 6.9 over and over. (Recall R’s recycling rules- we needed a long vector here, so the output of mean was repeated enough to fill the whole table.) But now we can work with this:\n\n\n\n\n\n\n\n\nand, if we want, we can get the carrier specific statistics:\n\n\n\n\n\n\n\n\nPretty nice! And when it matters - for very large data stored on a database - a little faster to boot!\nBefore going deeper down this path, what happens if we move the group_by earlier in our pipeline?\n\n\n\n\n\n\n\n\nDefinitely different! But why?\nTo see the difference, let’s compare the mean_delay column:\n\n\n\n\n\n\n\n\nWe now see here that the mean_delay is computed “group-wise”, so we’re not getting flights that are delayed compared to an average flight; we are instead counting flights that are delayed compared to an average flight on that airline. Put another way, we’re holding American Airlines (AA) and Delta (DL) flights to a higher standard than Jet Blue (B6) or ExpressJet (EV).\nAs always - the question you should ask yourself is not “is this the right thing” but “when is this the right thing?”. It’s simply a different question!\nRecall that group_by followed by a summarize removes one “layer” of grouping. If we use this group_by + mutate + filter construction, the result is still grouped, which can lead to weird bugs. To address this, it is sometimes easier to use the .by argument to mutate and filter which will modify the grouping for that command only.\n\n\n\n\n\n\n\n\nIt’s a matter of taste.\n\nHAVING clause\nRecall that a SQL HAVING clause applies group-level filtering based on some summary statistics: this is easy enough in dplyr.\nFor example, suppose we want the average flight delays of large airlines, which we can define as those with more than 10,000 departures in our data set.\nWe can compute this in two ways: directly, computing the number of flights and average delay for each airline.\n\n\n\n\n\n\n\n\nThis totally works, but now we’ve lost all the other flight-level information. An alternate approach is to compute counts group-wise and filter before averaging:\n\n\n\n\n\n\n\n\nThis has the advantage of being readily adaptable to other non-summarizing questions: for instance, of the delayed flights of the major carrier, how many were going to Houston?\n\n\n\n\n\n\n\n\nHere, we re-used the n column and so the old value of n was quietly replaced. This is probably ok with a simple variable name like n (which wasn’t all that interesting) but for “raw” data columns, you probably should avoid this.\nIn class, we’ll do more exercises based on group-specific filtering, both filtering on groups and filtering within groups. See if you can answer:\n\nWhat carrier has the lowest rate of delayed flights?\nWhat carrier has the highest chance of early arrivals?\nWhat carrier is most likely to “make up time in flight” after a delayed departure?\nWhich origin airport has the highest rate of delays?\nWhich month has the most flights?\nWhat is the furthest flight in this data?\nWhat is the shortest flight in this data?\nAre longer flights more likely to be delayed than short ones?\n\n\nThe readings in this tutorial follow R for Data Science, section 5.2. The exercises for filter were adapted from the official documentation of the learnr package."
  },
  {
    "objectID": "labs/lab04.html#footnotes",
    "href": "labs/lab04.html#footnotes",
    "title": "STA/OPR 9750 Week 4 In-Class Activity: Single Table Verbs, Group-Aware Filtering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 2013 NYC version of this data has become a semi-standard teaching example, but the US Bureau of Transportation Statistics releases new versions of this data constantly. If you are interested in performing this type of analysis for a different set of airports or a different time period, check out the anyflights package. It’s very easy - but a bit slow - to get flight data from almost any US airport this way. If you want to develop your data cleaning skills, it’s a great exercise to parse the BTS website directly and compare your output with the anyflights package.↩︎"
  },
  {
    "objectID": "labs/lab05.html",
    "href": "labs/lab05.html",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "labs/lab05.html#join-specifications",
    "href": "labs/lab05.html#join-specifications",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Join Specifications",
    "text": "Join Specifications\ndplyr specifies joins using the join_by function. The output of the join_by function, also known as a “join specification” is a series of logical tests applied to pairs of rows. The results of these logical tests are used to identify “matches” between rows. Joins differ primarily on how they use the outputs of these logical tests to construct their output.\nThe simplest and most useful logical test to use in a join is an equality test. In dplyr, these are simply written as\n\njoin_by(left_name == right_name)\n\nThis type of test checks whether the value in the left_name column of the first (left) argument matches the value in the right_name column of the second (right) argument.\nFor example, if I wanted to join the origin column of flights table to the faa column of the airports table, I might use something like the following:\n\ninner_join(flights, airports, join_by(origin == faa))\n\n# A tibble: 336,776 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 18 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt;, lat &lt;dbl&gt;,\n#   lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n\nHere origin is taken to be a column from the first (left) table and faa is taken to be a column from the second (right) table. As with other dplyr functions, there is a bit of programming magic used to allow column names to be used as variables and interpreted correctly.\nFor the airport identifiers, we only need to match on the single unique ID. (We can assume the FAA assigns unique IDs to each airport.) In other circumstances, we need to combine several logical tests to get a true match.\nFor example, suppose we want to align our flights with the weather at their origin airport at scheduled take off time. Here, we’d need to combine the flights and weather table on many columns:\n\norigin to origin\nyear to year\nmonth to month\nday to day\nhour to hour\n\nIn this case, we’d pass 5 equality conditions to join_by:\n\ninner_join(flights, \n           weather, \n           join_by(origin == origin,\n                   year == year,\n                   month == month,\n                   day == day,\n                   hour == hour))\n\n# A tibble: 335,220 × 29\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 335,210 more rows\n# ℹ 21 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour.x &lt;dttm&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;,\n#   humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;,\n#   precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour.y &lt;dttm&gt;\n\n\nHere we look only at those rows which match on all 5 tests. In this way, join_by behaves like filter: it “passes” the intersection of positive results.\nNote that it is relatively common for matched columns to have the same name in both tables: to support this case, dplyr reads a single column name as “self-equality”. So the above code can be more concisely written as:\n\ninner_join(flights, \n           weather, \n           join_by(origin, \n                   year, \n                   month, \n                   day, \n                   hour))\n\n# A tibble: 335,220 × 29\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 335,210 more rows\n# ℹ 21 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour.x &lt;dttm&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;,\n#   humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;,\n#   precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour.y &lt;dttm&gt;\n\n\nI recommend against using this short-cut. It takes hardly more time to write your intent explicitly and it’s far more robust. Measure twice, cut once.\nUnfortunately, it is not easy to perform an “OR” in join_by. We may cover this below, time allowing.\nWe now turn to specific joins. All of these joins use the join_by operator but they construct results differently based on its output."
  },
  {
    "objectID": "labs/lab05.html#inner-joins",
    "href": "labs/lab05.html#inner-joins",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Inner Joins",
    "text": "Inner Joins\nThe most common and most important join in data analysis is the inner_join. The inner join returns matches between two tables. Conceptually, the inner join constructs all possible pairs of rows between the two tables (so {r eval=FALSE} NROW(x) * NROW(y) total rows) and then filters down to those which pass the join_by test. In practice, more efficient algorithms are used to prevent wasteful computation.\nInner joins are used when seeking matches between two tables. They are particularly useful when both tables are “comprehensive” and we are sure that there are matches. For instance, we can use an inner_join to combine most of the tables in nycflights13 because they come from a comprehensive government data source. (E.g., No flights going to secret “unauthorized” airports.)\nLet’s start by asking what the average arrival delay of flights going to west coast airports is. We do not have enough information to answer this using the flights table alone. To identify west coast airports, let’s filter airports on tzone:\n\nwest_coast_airports &lt;- airports |&gt; filter(tzone == \"America/Los_Angeles\")\n\nWe can now join this to the original flights table to find only those flights with destination matches in west_coast_airports:\n\ninner_join(flights, west_coast_airports, join_by(dest == faa))\n\n# A tibble: 46,324 × 26\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      558            600        -2      924            917\n 2  2013     1     1      558            600        -2      923            937\n 3  2013     1     1      559            600        -1      854            902\n 4  2013     1     1      611            600        11      945            931\n 5  2013     1     1      628            630        -2     1016            947\n 6  2013     1     1      646            645         1     1023           1030\n 7  2013     1     1      651            655        -4      936            942\n 8  2013     1     1      655            700        -5     1037           1045\n 9  2013     1     1      658            700        -2     1027           1025\n10  2013     1     1      702            700         2     1058           1014\n# ℹ 46,314 more rows\n# ℹ 18 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt;, lat &lt;dbl&gt;,\n#   lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n\nHere, we have only a subset of our original flights table. From this, we can compute our relevant summary statistic:\n\ninner_join(flights, west_coast_airports, join_by(dest == faa)) |&gt;\n    summarize(mean(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(arr_delay, na.rm = TRUE)`\n                            &lt;dbl&gt;\n1                            1.28\n\n\nIs this any better than the following alternative approach:\n\ninner_join(flights, airports, join_by(dest == faa)) |&gt;\n    filter(tzone == \"America/Los_Angeles\") |&gt;\n    drop_na() |&gt;\n    summarize(mean(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(arr_delay, na.rm = TRUE)`\n                            &lt;dbl&gt;\n1                            1.28\n\n\nFormally, these are basically equivalent. (filter and inner_join commute). As usual, it’s a matter of communicating intent. Here the single line filter(tzone == \"America/Los_Angeles\") is simple enough it probably doesn’t need a separate variable. But if, instead of a one line operation, we performed a very complex set of filtering options, we may benefit from giving it a separate name as opposed to trying to shoe-horn the complex filtering into a pipeline.\nPerformance-wise, it is a bit better to perform filter before inner_join (Why? Think about the size of the result of each step.) but the difference is rarely material. Clarity of intent, not optimizing performance, should dictate the order in which you perform steps.\nBoth approaches are also equivalent to:\n\ninner_join(flights, \n           airports |&gt; filter(tzone == \"America/Los_Angeles\"), \n           join_by(dest == faa)) |&gt;\n    drop_na() |&gt;\n    summarize(mean(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(arr_delay, na.rm = TRUE)`\n                            &lt;dbl&gt;\n1                            1.28\n\n\nBut I find this sort of “filter inside join argument” to be terribly difficult to read: it mixes standard (inside-out) and piped (left to right) evaluation orders in a confusing manner. Avoid this!\nWork with your group to answer the following questions using inner_join.\n\nWhat is the name of the airline with the longest average departure delay?\nWhat is the name of the origin airport with the longest average departure delay?\nWhat is the name of the destination airport with the longest average departure delay?\nAre average delays longer for East-coast destinations or West-coast destinations?\nWhich plane (tailnum) flew the most times leaving NYC? Who manufactured it?\nWhich manufacturer has the most planes flying out of NYC airports?\nWhich manufacturer has the longest average flight?\nWhat model of plane has the smallest average delay leaving NYC?"
  },
  {
    "objectID": "labs/lab05.html#left-join",
    "href": "labs/lab05.html#left-join",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Left Join",
    "text": "Left Join\nLeft joins are useful when you don’t want to dropped unmatched columns in one table. For instance, suppose we misplace some rows from our airlines table:\n\nairlines_major &lt;- airlines |&gt;\n    filter(carrier %in% c(\"AA\", \"DL\", \"UA\", \"WN\", \"B6\", \"AS\"))\n\nIf we inner join on airlines_major, we loose many of the rows in flights.\n\nNROW(flights)\n\n[1] 336776\n\ninner_join(flights, \n           airlines_major, \n           join_by(carrier == carrier)) |&gt;\n    NROW()\n\n[1] 207128\n\n\nSometimes this is what we want, but not always. If we instead use a left join, we keep all of the rows in flights:\n\nNROW(flights)\n\n[1] 336776\n\nleft_join(flights, \n          airlines_major, \n          join_by(carrier == carrier)) |&gt;\n    NROW()\n\n[1] 336776\n\n\nRows lacking a pair in airlines_major fill the missing columns with NA. This fits our mental model of missing values in R: in theory, these flights should have some carrier name, but given the data at hand, we don’t know what it is.\n\nNROW(flights)\n\n[1] 336776\n\nleft_join(flights, \n          airlines_major, \n          join_by(carrier == carrier)) |&gt;\n    filter(carrier %in% c(\"MQ\", \"OO\", \"VX\")) |&gt;\n    glimpse() # Look at 'name' column\n\nRows: 31,591\nColumns: 20\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 600, 602, 608, 624, 656, 658, 729, 749, 800, 805, 811, …\n$ sched_dep_time &lt;int&gt; 600, 605, 600, 630, 705, 700, 730, 710, 810, 815, 630, …\n$ dep_delay      &lt;dbl&gt; 0, -3, 8, -6, -9, -2, -1, 39, -10, -10, 101, -4, -5, -8…\n$ arr_time       &lt;int&gt; 837, 821, 807, 840, 1007, 1027, 1049, 939, 949, 1006, 1…\n$ sched_arr_time &lt;int&gt; 825, 805, 735, 830, 940, 1025, 1115, 850, 955, 1010, 83…\n$ arr_delay      &lt;dbl&gt; 12, 16, 32, 10, 27, 2, -26, 49, -6, -4, 137, -13, -13, …\n$ carrier        &lt;chr&gt; \"MQ\", \"MQ\", \"MQ\", \"MQ\", \"MQ\", \"VX\", \"VX\", \"MQ\", \"MQ\", \"…\n$ flight         &lt;int&gt; 4650, 4401, 3768, 4599, 4534, 399, 11, 3737, 4406, 4490…\n$ tailnum        &lt;chr&gt; \"N542MQ\", \"N730MQ\", \"N9EAMQ\", \"N518MQ\", \"N722MQ\", \"N627…\n$ origin         &lt;chr&gt; \"LGA\", \"LGA\", \"EWR\", \"LGA\", \"LGA\", \"JFK\", \"JFK\", \"EWR\",…\n$ dest           &lt;chr&gt; \"ATL\", \"DTW\", \"ORD\", \"MSP\", \"XNA\", \"LAX\", \"SFO\", \"ORD\",…\n$ air_time       &lt;dbl&gt; 134, 105, 139, 166, 233, 361, 356, 148, 80, 101, 118, 5…\n$ distance       &lt;dbl&gt; 762, 502, 719, 1020, 1147, 2475, 2586, 719, 427, 479, 5…\n$ hour           &lt;dbl&gt; 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 6, 8, 8, 8, 8, 18, 9, 9, …\n$ minute         &lt;dbl&gt; 0, 5, 0, 30, 5, 0, 30, 10, 10, 15, 30, 25, 35, 40, 50, …\n$ time_hour      &lt;dttm&gt; 2013-01-01 06:00:00, 2013-01-01 06:00:00, 2013-01-01 0…\n$ name           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nleft_joins are useful if we want to join two tables, but want to avoid dropping any rows from a ‘gold standard’ table."
  },
  {
    "objectID": "labs/lab05.html#outer-join",
    "href": "labs/lab05.html#outer-join",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Outer Join",
    "text": "Outer Join"
  },
  {
    "objectID": "labs/lab05.html#advanced-join-specifications",
    "href": "labs/lab05.html#advanced-join-specifications",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Advanced Join Specifications",
    "text": "Advanced Join Specifications"
  },
  {
    "objectID": "labs/lab05.html#cumulative-operators",
    "href": "labs/lab05.html#cumulative-operators",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Cumulative Operators",
    "text": "Cumulative Operators"
  },
  {
    "objectID": "labs/lab05.html#rank",
    "href": "labs/lab05.html#rank",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "*_rank",
    "text": "*_rank"
  },
  {
    "objectID": "labs/lab05.html#advanced-joins",
    "href": "labs/lab05.html#advanced-joins",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Advanced Joins",
    "text": "Advanced Joins\n\ncross_join\n\n\nsemi_join\n\n\nanti_join\n\n\nnest_join"
  },
  {
    "objectID": "labs/lab05.html#bind_rows-and-bind_columns",
    "href": "labs/lab05.html#bind_rows-and-bind_columns",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "bind_rows and bind_columns",
    "text": "bind_rows and bind_columns"
  },
  {
    "objectID": "labs/lab05.html#footnotes",
    "href": "labs/lab05.html#footnotes",
    "title": "STA/OPR 9750 Week 5 In-Class Activity: Let us JOIN Our Tables Together",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that some SQL engines use LEFT OUTER JOIN than LEFT JOIN. Because OUTER is a bit ambiguous, dplyr emphasizes full_ vs left_ in its function naming. Also note the convention of dplyr names - lower case, underscore separated - and that it differs from SQL syntax.↩︎"
  },
  {
    "objectID": "labs/lab07.html",
    "href": "labs/lab07.html",
    "title": "STA/OPR 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "",
    "text": "Update Slides: Slides 07\nThis week, we’re going to break into project groups and do three ggplot2 exercises of increasing difficulty. As you work through these with your teammates, be sure to reflect on what plots and what tools you will need to best present your mini-project and course project findings."
  },
  {
    "objectID": "labs/lab07.html#exercise-1-basic-ggplot2-15-minutes",
    "href": "labs/lab07.html#exercise-1-basic-ggplot2-15-minutes",
    "title": "STA/OPR 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Exercise 1: Basic ggplot2 (15 minutes)",
    "text": "Exercise 1: Basic ggplot2 (15 minutes)\nIn this exercise, you will create ggplot2 graphics to analyze the diamonds data from the ggplot2 package. This data contains pricing and measurements for 50,000 diamonds sold in the US. (Note that these prices are rather out of date.) Before beginning this exercise, you might want to read about the “4 C’s of Diamonds” commonly used to measure quality.\n\nMake a scatter plot of price vs carat and facet it by cut.\nUse geom_smooth to see how the price-carat relationship changes by color.\nCreate a frequency polygon plot of price, broken out by different diamond cuts.\nCreate a scatter plot of color by clarity. Why is this plot not useful?\n\nStretch Goal: Make a better plot to visualize this relationship using the ggmosaic package."
  },
  {
    "objectID": "labs/lab07.html#exercise-2-trend-analysis-with-ggplot2-30-minutes",
    "href": "labs/lab07.html#exercise-2-trend-analysis-with-ggplot2-30-minutes",
    "title": "STA/OPR 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Exercise 2: Trend Analysis with ggplot2 (30 minutes)",
    "text": "Exercise 2: Trend Analysis with ggplot2 (30 minutes)\nThe Carbon Dioxide Information and Analysis Center studies the effect of carbon dioxide on global and local temperature trends. A key tool in their analysis is the temperature “anomaly”. An anomaly is the difference between observed temperature (in a world with anthropogenic atmospheric CO2) and ‘natural’ temperature (from a world without anthropogenic gases). Note that these anomalies require significant analysis to compute and are not “simple observational” data.\nPoliticians have adopted the tools of temperature anomaly to set national and international emissions targets, e.g., the 2 Degree Target. Note that 2 degrees is calculated as a global average: in practice, some regions will experience a much larger change in temperature, while others may experience a smaller change or even a negative change.\nThe CVXR package includes the cdiac dataset, capturing CDIAC’s estimated global temperature anomalies from 1850 to 2015. In this question, you will explore these estimated anomalies. Note that you may need to install the CVXR package before beginning this question.1\n\ninstall.packages(\"CVXR\")\n\n\nlibrary(CVXR)\nlibrary(tidyverse)\ndata(cdiac)\nglimpse(cdiac)\n\nRows: 166\nColumns: 14\n$ year   &lt;int&gt; 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 186…\n$ jan    &lt;dbl&gt; -0.702, -0.303, -0.308, -0.177, -0.360, -0.176, -0.119, -0.512,…\n$ feb    &lt;dbl&gt; -0.284, -0.362, -0.477, -0.330, -0.280, -0.400, -0.373, -0.344,…\n$ mar    &lt;dbl&gt; -0.732, -0.485, -0.505, -0.318, -0.284, -0.303, -0.513, -0.434,…\n$ apr    &lt;dbl&gt; -0.570, -0.445, -0.559, -0.352, -0.349, -0.217, -0.371, -0.646,…\n$ may    &lt;dbl&gt; -0.325, -0.302, -0.209, -0.268, -0.230, -0.336, -0.119, -0.567,…\n$ jun    &lt;dbl&gt; -0.213, -0.189, -0.038, -0.179, -0.215, -0.160, -0.288, -0.310,…\n$ jul    &lt;dbl&gt; -0.128, -0.215, -0.016, -0.059, -0.228, -0.268, -0.297, -0.544,…\n$ aug    &lt;dbl&gt; -0.233, -0.153, -0.195, -0.148, -0.163, -0.159, -0.305, -0.327,…\n$ sep    &lt;dbl&gt; -0.444, -0.108, -0.125, -0.409, -0.115, -0.339, -0.459, -0.393,…\n$ oct    &lt;dbl&gt; -0.452, -0.063, -0.216, -0.359, -0.188, -0.211, -0.384, -0.467,…\n$ nov    &lt;dbl&gt; -0.190, -0.030, -0.187, -0.256, -0.369, -0.212, -0.608, -0.665,…\n$ dec    &lt;dbl&gt; -0.268, -0.067, 0.083, -0.444, -0.232, -0.510, -0.440, -0.356, …\n$ annual &lt;dbl&gt; -0.375, -0.223, -0.224, -0.271, -0.246, -0.271, -0.352, -0.460,…\n\n\n\nPlot the estimated annual global mean temperature (GMT) anomaly from 1850 to 2015.\n\n\nUse scale_x_date to improve the \\(x\\)-axis\n\n\nPlot the GMT anomaly for each month on the same plot (as different lines).\n\n\nBefore starting this, you may need to use the pivot_ functionality to get this data in the right shape. Recall that ggplot2 expects “data point” per row.\n\n\nPlot the monthly GMT anomaly series as one long line (with a point for each month).\nNow focus only on July: plot the July GMT anomaly series. Use the runmed()\nfunction to add a second series to the plot giving the median July GMT anomaly of the previous 5 years. Is there evidence of an increasing warming trend?\nFor each year, identify the warmest month (as measured by GMT anomaly); create a histogram showing the probability a given month was the hottest (largest anomaly) in its year.\n\n\n\nMake sure your \\(x\\)-axis is in reasonable (chronological) order - not alphabetical.\nYou will need to use dplyr tools to find the warmest month in a given year."
  },
  {
    "objectID": "labs/lab07.html#exercise-3-animated-graphics-1-hour",
    "href": "labs/lab07.html#exercise-3-animated-graphics-1-hour",
    "title": "STA/OPR 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Exercise 3: Animated Graphics (1 hour)",
    "text": "Exercise 3: Animated Graphics (1 hour)\nIn this question, you will use the gganimate extension to ggplot2 to create animated graphics. We will use the famous gapminder data set from the gapminder package. Install the gganimate, gapminder, gifski, and av packages before attempting attempting this problem.\n\nFor background, watch Hans Rosling’s talk on human prosperity.\nCreate a scatter plot of the relationship between GDP and Life Expectancy in the year 1952.\n\n\nColor points by continent and use the size aesthetic to represent population.\nYou might want to put quantities on a log-scale.\n\n\nThere is an outlier country in this data with very high GDP.\n\n\nWhat is it?\nIdentify and remove it.\n\n\nUsing the transition_time function, make this an animated plot showing how this data changes over time.\nUsing the theme machinery, labels, etc. make this a “publication ready” plot.\n\n\nNote that you can use {frame_time} in the title to get a dynamically changing year.\n\n\nUse the country_colors data from the gapminder plot to color the points using Dr. Rosling’s perferred color scheme.\n\n\nThis is a different color scale than ggplot2 uses by default, so you will need to override the scale_color_* functionality.\nThe help page for ?country_colors will be helpful here."
  },
  {
    "objectID": "labs/lab07.html#footnotes",
    "href": "labs/lab07.html#footnotes",
    "title": "STA/OPR 9750 Week 7 In-Class Activity: More Thoughts on Plots",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCVXR is actually an incredible piece of software and super-useful for developing and implementing statistical and machine learning techniques. We, sadly, will not explore it in this course.↩︎"
  },
  {
    "objectID": "miniprojects.html",
    "href": "miniprojects.html",
    "title": "STA/OPR 9750 - Mini Projects",
    "section": "",
    "text": "In lieu of traditional homework, STA/OPR 9750 has a series of mini-projects designed to achieve several interlocking goals:\n\nImprove your skills at data analysis\nImprove your improve your ability to give feedback on data analysis work\nSeed a ‘portfolio’ of data science work you can demonstrate to potential employers\n\nEach Mini-Project will be submitted via GitHub, an industry-standard code management platform, as both raw analysis code and as a HTML document hosted on GitHub pages.\nAfter each Mini-Project is submitted, 2-3 peer reviewers will be assigned to give feedback and to assign an initial grade following an instructor provided rubric. This feedback will be given via GitHub Issues.\nIn order to ensure good peer feedback, the peer feedback will be evaluated by the instructor in a “meta-review” worth a small fraction of the overall grade.\nIf you believe your mini-project has received inaccurate peer feedback, please contact the instructor directly within 48 hours of the peer feedback deadline. No student-initiated requests for re-grading will be accepted after that time, though the instructor may re-grade the work during the meta-review stage.\n\nMini-Projects\n\nMini-Project #00: Course Set-Up\nDue Dates:\n\nReleased to Students: 2024-08-29\nInitial Submission: 2024-09-11 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-09-12\nPeer Feedback Due: 2024-09-18 11:45pm ET\n\n\nIn the ungraded Mini-Project #00, there is no data analysis required, but you will set up the basic web tooling used to submit projects #01 to #04.\nNote that, even though ungraded, Mini-Project #00 must be completed to remain enrolled in this course and before any other Mini-Projects can be submitted.\n\n\nMini-Project #01: Fiscal Characteristics of Major US Public Transit Systems\nDue Dates:\n\nReleased to Students: 2024-09-12\nInitial Submission: 2024-09-25 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-09-26\nPeer Feedback Due: 2024-10-02 11:45pm ET\n\n\nIn Mini-Project #01, students will investigate the fiscal characteristics of US public transit authorities. In this project, I handle the data import and tidying; students are mainly responsible for “single table” dplyr operations (mutate, group_by, summarize, select, arrange, rename) to produce summary statistics.\n\n\nMini-Project #02: Business of Show Business\nDue Dates:\n\nReleased to Students: 2024-09-26\nInitial Submission: 2024-10-23 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-10-24\nPeer Feedback Due: 2024-10-30 11:45pm ET\n\n\nIn Mini-Project #02, students will act as Hollywood development executives, diving deep into Hollywood history to develop a pitch for a new movie. Students develop their skills in working with large (\\(\\approx 20\\) GB) data and in working with relational data structures (joins and their kin). This project uses the IMDb Non-Commerical Data Release.\n\n\nMini-Project #03: TBA\nDue Dates:\n\nReleased to Students: 2024-10-24\nInitial Submission: 2024-11-13 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-11-14\nPeer Feedback Due: 2024-11-20 11:45pm ET\n\n\n\n\nMini-Project #04: TBA\nDue Dates:\n\nReleased to Students: 2024-11-14\nInitial Submission: 2024-12-04 11:45pm ET\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-12-05\nPeer Feedback Due: 2024-12-11 11:45pm ET"
  },
  {
    "objectID": "miniprojects/mini02.html",
    "href": "miniprojects/mini02.html",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Warning: package 'glue' was built under R version 4.4.1"
  },
  {
    "objectID": "miniprojects/mini02.html#introduction",
    "href": "miniprojects/mini02.html#introduction",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Introduction",
    "text": "Introduction\nWelcome to Mini-Project #02. In this project, you will play the role of a Hollywood development executive; that is, you are the executive in charge of coming up with new movie ideas. Historically, development executives would source the “life rights” necessary to make “based on a true story” movies, would secure production options on promising new novels, and would partner with owners of established intellectual property (IP) to develop movie adaptations. Recently, however, the development process has been criticized by Hollywood insiders and audiences alike for over-reliance on rote sequels. Our goal is to develop a set of data-driven ideas for new movies. Before doing so, however, we will dive into Hollywood history to identify key characteristics of successful movies, to identify successful filmmakers and actors, and to examine some of Hollywood’s most famous flops.\nStudent Responsbilities\nRecall our basic analytic workflow and table of student responsibilities:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn this mini-project, you are mainly responsible for data alignment and basic statistical analyses. While not the main focus of this mini-project, you are also expected to provide basic data visualizations to support your findings; the grading rubric, below, emphasizes the dplyr tools used in this project, but reports without any visualization will be penalized severely. Note that data visualization will play a larger role in Mini-Project #03.\nAs before, I will provide code to automatically download and read in the data used for this project. Also note that, as compared with Mini-Project #01, I am providing significantly less ‘scaffolding’: students are more responsible for directing their own analyses.\nRubric\nSTA/OPR 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\nCode completes all instructor-provided tasks satisfactorially.\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three or ore instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses.\n\n\nFormatting & Display\n\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation.\nFigures are ‘publication-quality’, with suitable axis labels, well-chosen structure, attractive color schemes, titles, subtitles, and captions, etc.\n\n\nTables are well-formatted, but still have room for improvement.\nFigures are above ‘exploratory-quality’, but do not reach full ‘publication-quality’.\n\n\nTables lack significant ‘polish’ and need improvement in substance (filtering and down-selecting of presented data) or style.\nFigures are suitable to support claims made, but are ‘exploratory-quality’, reflecting minimal effort to customize and ‘polish’ beyond ggplot2 defaults.\n\n\nUnfiltered ‘data dump’ instead of curated table.\nBaseline figures that do not fully support claims made.\n\nReport lacks basic tables; OR report lacks basic figures.\nReport includes one or more high-quality graphics (created using R) using tools beyond static basic ggplot2. These can be created using extensions toggplot2 or speciality packages for interactive graphics.\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nAutomatic (10/10). Out of scope for this mini-project\n\n\n\n\nReport modifies instructor-provided import code to use additional columns or data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA/OPR 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp2.qmd so the rendered document can be found at docs/mp2.html in the student’s repository and served at the URL:\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2024-FALL/mp02.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2024-FALL/issues/new .\n\nTitle the issue STA/OPR 9750 &lt;GITHUB_USERNAME&gt; MiniProject #02 and fill in the following text for the issue:\nHi @michaelweylandt!\n\nI've uploaded my work for MiniProject #02 - check it out!\n\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2024-FALL/mp02.html\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "miniprojects/mini02.html#mini-project-02",
    "href": "miniprojects/mini02.html#mini-project-02",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Mini-Project #02",
    "text": "Mini-Project #02\nData\nFor this project, we will use data from the Internet Movie Database (IMDb). Specifically, we will use the tables from the IMDb non-commercial release. These files are made freely available by IMDb for non-commercial use.\nThe following code will automatically download and load these files into R:\n\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\n\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\n\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\n\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\nNote that these are large files and it will take some time for them to download the first time. Because these files are so large, it will also take a little while to read them. If you want to speed up this stage, you can cache the code chunk that reads the files. This will ‘save’ the result of the chunk and only require it to be re-executed when it is changed.\nData Sub-Sampling\nThis data is large enough that we’re going to need to immediately start down-selecting to get to a data set that we can analyze fluidly. For our NAME_BASICS table, we’ll restrict our attention to people with at least two “known for” credits.1\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nIMDb has a long tail of obscure movies:\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\nTo keep our computers from working too hard, let’s throw out any title with less than 100 ratings. It’s not too hard to see that this drops about 75% of the entire data set:\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     101 2942823 \n\n\nApplying this drop, we significantly reduce the size of our data set:\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nWe want to perform the same filtering on our other TITLE_* tables. This is a rare use for the semi_join. Recall that a semi_join returns only values which have a match,but doesn’t actually add columns.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\nAt this point, we’ve filtered down our data significantly and are ready to begin analysis in earnest. Note that our sub-sampling may induce some ‘dangling’ references: some of the people dropped from the NAME_BASICS table may only appear in one famous movie, and we’ve likely lost their info.\n\n\n\n\n\n\nProcessing Large Data\n\n\n\nEven with this processing, this a non-trivial amount of data, requiring approximately 2 GB of memory. If your computer is significantly struggling to perform this pre-processing, the instructor may be able to provide smaller data files upon request. (Even on my quite modern laptop, the initial processing phase takes a few minutes: by ‘significant struggling’, I’m referring to processing taking upwards of half an hour or exhausting all available memory.) Please contact the instructor and TA through the course discussion board to discuss this possibility.\nProcessing large data sets is a skill, however, so we’re starting with the large data set to help you practice it.\n\n\n\n\n\n\n\n\nPre-Processed Data\n\n\n\n\n\nExports of the pre-processed data can be found on the course GitHub repo. If your computer is struggling to handle the full data set, you may choose to use these instead. The readr::read_csv files handles zip compression transparently, but you will need to modify get_imdb_file above to:\n\nPoint to my GitHub instead of the IMDB archive\nUse .csv.zip files instead of .tsv.gz\n\n\nNote also that, to get the compressed files small enough to store on GitHub, I had to apply more filtering than the code above uses. Make sure to note if you are using this extra-filtered extract so that a reader knows why you might be getting different answers.\n\n\n\nInitial Exploration\nAt this point, let’s start examining our data more closely. Use the glimpse function to examine each table, taking care to note the type or mode of each column. For this data set, most columns appear to be read in as character (string) vectors, even when they should be numeric. This can occur when “null” values are represented in some non-standard way. For instance, in these files, we see that missing values are represented as \\\\N. R does not know that these are NA values and so retains them as strings.2\nTo fix this, we need to use:\n\nthe mutate command, since we’re changing the type of a column\nthe as.numeric command to change the type of the column.\n\nWe can clean the NAMES_BASIC command as follows:\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n    mutate(birthYear = as.numeric(birthYear),\n           deathYear = as.numeric(deathYear))\n\n\n\n\n\n\n\nTask 1: Column Type Correction\n\n\n\nCorrect the column types of the TITLE tables using a combination of mutate and the coercion functions as.numeric and as.logical.\n\n\nAnother non-tidy aspect of this data is that it combines multiple pieces of information in a single cell separated by commas. We already saw a bit of this in the NAME_BASICS table, where both the primaryProfession and knownForTitles columns combine multiple values.\n\nglimpse(NAME_BASICS)\n\nRows: 3,175,526\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;dbl&gt; 1899, 1924, 1934, 1949, 1918, 1915, 1899, 1924, 1925…\n$ deathYear         &lt;dbl&gt; 1987, 2014, NA, 1982, 2007, 1982, 1957, 2004, 1984, …\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nWe can use the separate_longer_delim function to break these into multiple rows: for example\n\nNAME_BASICS |&gt; separate_longer_delim(knownForTitles, \",\") |&gt; slice_head(n=10)\n\n      nconst     primaryName birthYear deathYear\n1  nm0000001    Fred Astaire      1899      1987\n2  nm0000001    Fred Astaire      1899      1987\n3  nm0000001    Fred Astaire      1899      1987\n4  nm0000001    Fred Astaire      1899      1987\n5  nm0000002   Lauren Bacall      1924      2014\n6  nm0000002   Lauren Bacall      1924      2014\n7  nm0000002   Lauren Bacall      1924      2014\n8  nm0000002   Lauren Bacall      1924      2014\n9  nm0000003 Brigitte Bardot      1934        NA\n10 nm0000003 Brigitte Bardot      1934        NA\n                    primaryProfession knownForTitles\n1        actor,miscellaneous,producer      tt0072308\n2        actor,miscellaneous,producer      tt0050419\n3        actor,miscellaneous,producer      tt0053137\n4        actor,miscellaneous,producer      tt0027125\n5  actress,soundtrack,archive_footage      tt0037382\n6  actress,soundtrack,archive_footage      tt0075213\n7  actress,soundtrack,archive_footage      tt0117057\n8  actress,soundtrack,archive_footage      tt0038355\n9   actress,music_department,producer      tt0057345\n10  actress,music_department,producer      tt0049189\n\n\nTo preserve flexibility, let’s not fully separate NAME_BASICS just yet, but you will need to use separate_longer_delim to answer various questions.\nUsing your knowledge of dplyr functionality, answer the following questions\n\n\n\n\n\n\nTask 2: Instructor-Provided Questions\n\n\n\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\nWho is the oldest living person in our data set?\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\nWhat four projects is the actor Mark Hamill most known for?\nWhat TV series, with more than 12 episodes, has the highest average rating?\n\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\nIs it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\n\nHint: It may be useful to create a “map” of which columns map to which tables before attempting these questions. While these can be quite formal, even some basic sketches on a scratch piece of paper are often quite clarifying.\n\n\nQuantifying Success\nOur goal is to proposal successful new movies. To do so, we need a way of measuring the success of a movie given only IMDb ratings.3 While there’s no “magic number” for success, it is logical to assume that a successful project will have both a high average IMDb rating, indicating quality, and a large number of ratings, indicating broad awareness in the public.\n\n\n\n\n\n\nTask 3: Custom Success Metric\n\n\n\nDesign a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness. Implement your success metric using a mutate operator to add a new column to the TITLE_RATINGS table.\nValidate your success metric as follows:\n\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nChoose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nPerform at least one other form of ‘spot check’ validation.\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value \\(v\\) such that movies above \\(v\\) are all “solid” or better.\n\n\n\nYou will use your success metric and threshold to complete the rest of this Mini-Project. You may, if you wish, restrict your attention to movies for the remainder of your analysis, though a good development executive should also consider making TV series.\nExamining Success by Genre and Decade\nNow that you have a working proxy for success, it’s time to look at trends in success over time. Answer the following questions. Your responses should include at least 2 graphics.\n\n\n\n\n\n\nTask 4: Trends in Success Over Time\n\n\n\nUsing questions like the following, identify a good “genre” for your next film. You do not need to answer these questions precisely, but these are may help guide your thinking.\n\nWhat was the genre with the most “successes” in each decade?\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nWhat genre has become more popular in recent years?\n\n\n\nBased on your findings, select a genre for your next project. Note that you may wish to avoid an “oversatured” genre; you just need to make the argument that your proposal is a good investment, not necessarily the most studio-produced focus-grouped committee-designed generic satisfying choice, so feel free to lean in to your own artistic preferences, as long as you can make an argument for them.\nSuccessful Personnel in the Genre\nNow that you have selected a target genre, identify two actors and one director who will anchor your project. You want to identify key personnel who have worked in the genre before, with at least modest success, and who have at least one major success to their credit.\nAs you develop your team, you may want to consider the following possibilities:\n\nAn older established actor and an up-and-coming actor\nAn actor/director pair who have been successful together\nAn actor/director pair who are both highly successful but have never worked together\nA pair of established actors who have had success in many genres\n\nAs you select your key personnel, consider what IMDb says they are known for; this will be useful in developing your marketing materials.\n\n\n\n\n\n\nTask 5: Key Personnel\n\n\n\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\n\n\nNostalgia and Remakes\nNow that you have found a target genre and key talent for your project, you need a story. Like any good development executive, your first instinct should be to produce a remake of a classic film in the genre.\n\n\n\n\n\n\nTask 6: Finding a Classic Movie to Remake\n\n\n\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.4\nOnce you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”"
  },
  {
    "objectID": "miniprojects/mini02.html#putting-it-together",
    "href": "miniprojects/mini02.html#putting-it-together",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Putting It Together",
    "text": "Putting It Together\n\n\n\n\n\n\nTask 7: Write and Deliver Your Pitch\n\n\n\nNow that you have completed your analysis, write an “elevator pitch” of approximately 200-250 words for your proposed Hollywood project. This is the pitch you will bring to the studio head (your boss); if the studio head likes your pitch, you will be given a small sum of money to start securing the story rights and locking down tentative deals with key talent.\nYour pitch needs to synthesize the analysis above into two to three quick and compelling points. (E.g., “The market for animated young adult horror musicals has grown 200% in the past decade” or “Over 90% of Director D’s movies are successes.”) You need to present the strongest argument for each element of your pitch, including genre, director, actors, and story.\nIf your boss approves the pitch, you will need to have a brief trailer ready for the next quarterly earnings call. The marketing department has asked that you prepare a classic 90’s style teaser for them. Adapt the following cliched formula for your pitch.\n\nFrom director D, the visionary mind between N1; and From actor A, beloved star of N2; and From actor A2, Hollywood icon of genre G, Comes the timeless tail N3 A story of TOPIC, TOPIC, and TOPIC Coming soon to a theater near you.\n\nIf you’re creatively-minded, you could have some fun here using Generative tools to draft a script or mock up a movie poster for your pitch."
  },
  {
    "objectID": "miniprojects/mini02.html#general-remarks",
    "href": "miniprojects/mini02.html#general-remarks",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "General Remarks",
    "text": "General Remarks\nAs you approach this project, recall there are no right or wrong answers. You are exploring data looking for exciting and actionable findings. You have several key decisions to make and you can support them with data, but the decisions are ultimately yours. This project is an exercise both in the “nuts-and-bolts” of analyzing a large data set and in using data to inform and refine what is ultimately still a “gut feeling” qualitative business decision.\nAs you iterate on this project, you will see that seemingly small different choices can produce very different results. That’s ok! As data analysts, we are constantly faced with small and essentially arbitrary decisions. An important “meta-skill” is knowing which of these decisions radically change our findings and which are meaningless. (An arbitrary decision with no impact on the bottom line is harmless; an arbitrary decision that could entirely change the plan for the next ten years is a problem.) Our responsibility is to clearly communicate these choices to our partners and clients: then we can receive their feedback on which way they would like to proceed.\nWorking in tools like Quarto and R helps here: if we provide clean and reproducible code, it should be easy to modify to see how our final conclusions are changed. Graphics also play an essential role in this form of clear communication: a ‘point estimate’ like “Action A is the best” is far less interpretable than a chart showing the predicted outcomes of several different actions.\nAs you approach this project, focus on justifying and communicating the choices you make. Structure your argument to communicate both key findings and uncertainties around them. Think about how you can use both document structure (headings vs subsections) and graphics to communicate with both clarity and nuance.\nGood luck!\n\nThis work ©2024 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "miniprojects/mini02.html#footnotes",
    "href": "miniprojects/mini02.html#footnotes",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Footnotes",
    "text": "Footnotes\n\nIt’s not entirely transparent who IMDb decides what projects an actor or director is “known for”. Still, it’s a reasonable filter that leaves us with more than enough to work with for this project.↩︎\nRecall that strings can contain essentially any data type and so are a safe fall-back. For instance, a column containing 1 and a can be losslessly represented by the string vector c(\"1\", \"a\") but coercion to the numeric vector c(1, NA) is lossy. R tries very hard not to destroy any information and so it doesn’t perform this conversion for us unless we explicitly request it.↩︎\nSadly, I couldn’t find permissively licensed movie box office data. If you are aware of some, please let me know!↩︎\nIn order to see that a movie has not been recently remade, it is sufficient to confirm that no movie has been made with the same name in the past 25 years.↩︎"
  },
  {
    "objectID": "miniprojects/mini01.html",
    "href": "miniprojects/mini01.html",
    "title": "STA/OPR 9750 Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "Released to Students: 2024-09-12\nInitial Submission: 2024-09-25 11:45pm ET on GitHub and Brightspace\n\nPeer Feedback:\n\nPeer Feedback Assigned: 2024-09-26 on GitHub\nPeer Feedback Due: 2024-10-02 11:45pm ET on GitHub"
  },
  {
    "objectID": "miniprojects/mini01.html#welcome-to-mini-projects",
    "href": "miniprojects/mini01.html#welcome-to-mini-projects",
    "title": "STA/OPR 9750 Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Welcome to STA/OPR 9750 Mini Projects!",
    "text": "Welcome to STA/OPR 9750 Mini Projects!\nIn the STA/OPR 9750 Mini-Projects, you will perform basic data analyses intended to model best practices for your course final project. (Note, however, that these are mini-projects; your final course project is expected to be far more extensive than any single MP.)\nFor purposes of MPs, we are dividing the basic data analytic workflow into several major stages:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\nIn this course, our primary focus is on the first four stages: you will take other courses that develop analytical and modeling techniques for a variety of data types. As we progress through the course, you will eventually be responsible for the first four steps. Specifically, you are responsible for the following stages of each mini-project:\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn early stages of the course, such as this MP, I will ‘scaffold’ much of the analysis for you, leaving only those stages we have discussed in class for you to fill in. As the course progresses, the mini-projects will be more self-directed and results less standardized.\nRubric\nSTA/OPR 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nLess than half of the instructor-provided tasks were successfully completed.\nReport exhibits particularly creative insights beyond instructor specifications.\n\n\nFormatting & Display\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation. Table has a suitable caption.\nColumn names and digits are well-chosen, but formatting could be improved.\nBad column names (opaque variable names or other undefined acronyms)\nUnfiltered ‘data dump’ instead of curated table.\nNo tables.\nReport includes one or more high-quality graphics (created using R).\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nAutomatic (10/10). Out of scope for this mini-project\n\n\n\n\nReport modifies instructor-provided import code to use additional columns or data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA/OPR 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp01.qmd so the rendered document can be found at docs/mp01.html in the student’s repository and served at the URL:\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2024-FALL/mp01.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2024-FALL/issues/new .\n\nTitle the issue STA/OPR 9750 &lt;GITHUB_USERNAME&gt; MiniProject #01 and fill in the following text for the issue:\nHi @michaelweylandt!\n\nI've uploaded my work for MiniProject #01 - check it out!\n\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2024-FALL/mp01.html\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "miniprojects/mini01.html#mini-project-01-fiscal-characteristics-of-major-us-public-transit-systems",
    "href": "miniprojects/mini01.html#mini-project-01-fiscal-characteristics-of-major-us-public-transit-systems",
    "title": "STA/OPR 9750 Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "text": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems\nFor MP#01, we are taking inspiration from the popular CityNerd YouTube channel. In particular, we’re taking this presentation on farebox recovery, i.e. the fraction of revenues raised from fares instead of taxes, as our starting point:\n\n\nWe will use data from the National Transit Database as our primary source. In particular, since we want to analyze farebox revenues, total number of trips, total number of vehicle miles traveled, and total revenues and expenses by source, we will need to analyze several different tables:\n\nThe 2022 Fare Revenue table\nThe latest Monthly Ridership tables\nThe 2022 Operating Expenses reports\n\nBecause this data is reported on a lag, we will use the 2022 version of all reports. Our data may have some post-pandemic irregularities, but that’s ok. We aren’t looking to make any long-term forecasts in this project.\nThe following code will download, clean, and join the tables. You don’t need to edit it in this project, but you may want to bookmark it as a useful example for later projects where you are responsible for downloading and cleaning the data.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n    # directory.\n    download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                  destfile=\"2022_fare_revenue.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_expenses.csv\" in your project\n    # directory.\n    download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                  destfile=\"2022_expenses.csv\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\nFinally, let’s extract monthly transit numbers:\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"ridership.xlsx\" in your project\n    # directory.\n    download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                  destfile=\"ridership.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nThis creates a table as follows:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\nInstructor’s Note: You might want to explore the functions of the DT package to create more attractive tables. Even more advanced is the gt package.\nThis is useful, but not exactly what we want. Here, the UPT column refers to Unlinked Passenger Trips, which is a measure of rides (controlling for connections and transfers), and VRM refers to Vehicle Revenue Miles, roughly how far the transit provider travelled in total. Some of the other column names are less helpful, so let’s rename them using the rename function.\n\n\n\n\n\n\nTask 1 - Creating Syntatic Names\n\n\n\nRename a column: UZA Name to metro_area.\n\n\nBecause they have no spaces in them, these names will be easier to manipulate in code. Recall that non-syntactic names (names with spaces, punctuation, or strange characters) have to be quoted in backticks or quotes (depending on the context), while most tidyverse functions take syntactic names without quotes.\nThe Mode column is also helpful, but it uses a set of codes that aren’t interpretable. To make life easier for ourselves, let’s use a case_when statement to transform this into something we can make sense of.\n\n\n\n\n\n\nTask 2: Recoding the Mode column\n\n\n\nFirst, find the unique Mode codes in our data using the distinct function. Next, examine the NTD website and find the interpretations of these codes. Complete the following snippet to recode the Mode column.\n\n## This code needs to be modified\nUSAGE &lt;- USAGE |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\", \n        ...\n        ...\n        TRUE ~ \"Unknown\"))\n\n\n\nNow that your data is clean, you may want to create an attractive summary table of your cleaned up USAGE table using the following snippet:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\nTo make your table cleaner, you might want to modify this code to unselect the NTD ID and 3 Mode columns and/or rename the UPT and VRM columns.\nNote: The use of sample_n here is just to make a sufficiently small sample to view it in a table. For your actual analysis, you should use the entire data set.\n\n\n\n\n\n\nTask 3: Answering Instructor Specified Questions with dplyr\n\n\n\nUsing functions we have studied in class, including filter, group_by, summarize, arrange, answer the following questions in your analysis:\n\nWhat transit agency had the most total VRM in our data set?\nWhat transit mode had the most total VRM in our data set?\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n\nWhat mode of transport had the longest average trip in May 2024?\nNote: This question can’t be answered with vehicle miles. To get average passenger trip length, we need passenger miles.\n\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n\n\n\n\n\n\n\nTask 4: Explore and Analyze\n\n\n\nFind three more interesting transit facts in this data other than those above.\n\n\nWe are now ready to combine these usage statistics with the revenue and expenses data. Because our fare data is for 2022 total, we need to convert our usage table to 2022 summary info.\n\n\n\n\n\n\nTask 5: Table Summarization\n\n\n\nCreate a new table from USAGE that has annual total (sum) UPT and VRM for 2022. This will require use of the group_by, summarize, and filter functions. You will also want to use the year function, to extract a year from the month column.\nThe resulting table should have the following columns:\n\nNTD ID\nAgency\nmetro_area\nMode\nUPT\nVRM\n\nMake sure to ungroup your table after creating it.\nName this table USAGE_2022_ANNUAL.\n\n\nOnce you have created this new table, you can merge it to the FINANCIALS data as follows:\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n           FINANCIALS, \n           join_by(`NTD ID`, Mode)) |&gt;\n    drop_na()\n\nNote that the name fields differ between the ridership and financials tables, so it’s a good thing we had the unique identifier NTD ID to rely on.\nWe are now finally ready to our original question about farebox recovery.\n\n\n\n\n\n\nTask 6: Farebox Recovery Among Major Systems\n\n\n\nUsing the USAGE_AND_FINANCIALS table, answer the following questions:\n\nWhich transit system (agency and mode) had the most UPT in 2022?\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nWhich transit system (agency and mode) has the lowest expenses per UPT?\nWhich transit system (agency and mode) has the highest total fares per UPT?\nWhich transit system (agency and mode) has the lowest expenses per VRM?\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\nYou may wish to restrict your answer to major transit systems, which you can define as those with 400,000 UPT per annum.\n\n\nBased on all of this, what do you believe to be the most efficient transit system in the country? (Your answer may differ depending on which form of ‘efficiency’ you care most about)\n\nThis work ©2024 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "preassigns/pa03.html",
    "href": "preassigns/pa03.html",
    "title": "STA/OPR 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "",
    "text": "Due Date: 2024-09-11 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nIt is now time for us to start programming in R properly. In this week’s pre-assignment, we’re going to focus on three basic elements of programming in R:\nBefore we get into these however, let’s introduce the feedback mechanism used throughout this pre-assignment. Throughout this page, you will encounter blocks like the below:\nWait for the exercise to fully load (the blue dot next to Run Code will disappear) and then try giving correct and incorrect solutions.\nThese little code-blocks throughout this pre-assignment will be used to give similar feedback. You can always hit Show Solution to get the correct answer. The feedback engine is a bit overly picky at times, so if your answer is substantially similar to the official solution, I wouldn’t worry too much.\nYou will see the R output sometimes has a [1] before it. Don’t worry about that until you get to the section on vectorized semantics below.\nBlocks that aren’t listed as “Exercise” are interactive snippets. Feel free to adjust the code to check your understanding."
  },
  {
    "objectID": "preassigns/pa03.html#calculator-math-with-r",
    "href": "preassigns/pa03.html#calculator-math-with-r",
    "title": "STA/OPR 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "Calculator Math with R",
    "text": "Calculator Math with R\nLet’s start by using R as a calculator. R implements all the basic operations of arithmetic:\n\na + b: (binary) addition \\(a + b\\)\na - b: (binary) subtraction \\(a - b\\)\n*: (binary) multiplication \\(ab\\)\n/: (binary) division \\(a/b\\)\n-b: (unary) negation \\(-b\\)\n\nYou can type integers and decimals in the usual manner:\n\n\n\n\n\n\n\n\nCompute \\(5! = 5 * 4 * 3 * 2 * 1\\) using R:\n\n\n\n\n\n\n\n\n\nThe blanks should be filled with 3 and 1\n\n\n5 * 4 * 3 * 2 * 1\n5 * 4 * 3 * 2 * 1\n\n\n\n\n\n\nExponentials (powers) can be implemented with either a double star ** or a carrot ^:\n\n\n\n\n\n\n\n\nI tend to prefer the carrot ^ as its one fewer character.\nIn general, R respects the standard “PEMDAS” order of operations:\n\nParentheses\nExponentiation\nMultiplication and Division\nAddition and Subtraction\n\nSo we can compute \\(3 * (2 + 1)\\) as:\n\n\n\n\n\n\n\n\n\nExercises\nCompute the following algebraic expressions using R:\n\n\\[3 * 2^2\\]\n\n\n\n\n\n\n\n\n\n\n\n\n3 * 2^2\n3 * 2^2\n\n\n\n\n\n\n\n\\[(3 * 2)^2\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(3 * 2)^2\n(3 * 2)^2\n\n\n\n\n\n\n\n\\[3 + 2 - 1 + 4\\]\n\n\n\n\n\n\n\n\n\n\n\n\n3 + 2 - 1 + 4\n3 + 2 - 1 + 4\n\n\n\n\n\n\n\n\\[3 + 2 - (1 + 4)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n3 + 2 - (1 + 4)\n3 + 2 - (1 + 4)\n\n\n\n\n\n\n\nExecution in RStudio\nNow, redo these exercises in the RStudio Console. At each step, type the relevant code next to the &gt; prompt and hit enter to execute the command.\nR has greedy execution. When you hit enter, R tries its best to execute the whole line of code. If you enter an incomplete line of code, e.g., 3 +, R will change the &gt; prompt to a + prompt, indicating there is more to be done.\nCompare\n\n\n\n\n\n\n\n\nwith\n\n\n\n\n\n\n\n\nIn the first example, 3- is not a complete mathematical statement, so R knew there had to be more code and continued to await input. In the second, 3 is a perfectly valid (if very simple) mathematical command on its own, so R simply executes it as is.\nContinuation prompts from dangling math are quite rare, but you will often find yourself in this scenario if you let parentheses become mismatched. If you are ever stuck and can’t figure out how to appease R, simply type Cntrl-C to “interrupt” the command and get back to the standard prompt."
  },
  {
    "objectID": "preassigns/pa03.html#function-calls",
    "href": "preassigns/pa03.html#function-calls",
    "title": "STA/OPR 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "Function Calls",
    "text": "Function Calls\nR comes built in with a quite robust mathematical library. You can in general call a function like this:\n\n\n\n\n\n\n\n\n(R also comes with the mathematical constant \\(\\pi\\) pre-loaded.)\nIn general a function call is a “name” immediately followed by parentheses. If a function takes input or arguments, the input is located between the parentheses, separated by commas.\nSo above, cos(pi) implements the math \\(\\cos(\\pi)\\).\nUseful built-in functions are:\n\nsin - in radians\ncos - in radians\nexp - base \\(e\\) exponential\nlog - by default this is the natural logarithm (\\(\\ln\\))\nsqrt\nabs - absolute value\nfactorial - \\(n! = n * (n - 1) * (n - 2) * \\dots * 3 * 2 * 1\\)\n\nUse the built-in functions to compute \\(5!\\):\n\n\n\n\n\n\n\n\n\n\n\nfactorial(5)\nfactorial(5)\n\n\n\n\n\n\n\nExercises\nUsing these built-in functions, compute the following arithmetic expressions:\n\n\\[\\cos^2(\\pi / 4)\\]\n\n\n\n\n\n\n\n\n\n\n\n\ncos(pi/4)^2\ncos(pi/4)^2\n\n\n\n\n\n\n\n\\[e^{\\log(\\pi) + 3}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nexp(log(pi) + 3)\nexp(log(pi) + 3)"
  },
  {
    "objectID": "preassigns/pa03.html#vectorized-semantics",
    "href": "preassigns/pa03.html#vectorized-semantics",
    "title": "STA/OPR 9750 Week 3 Pre Assignment: Calculator Work with R",
    "section": "Vectorized Semantics",
    "text": "Vectorized Semantics\nA distinguishing feature of R is its vectorized semantics. By default, R wants to operate on collections of data - not individual values (scalars). You’ve seen some evidence of this already. Whenever you run a bit of math, R puts [1] at the front of the output. This is R helping you count the number of elements in the solution; it’s been pretty trivial so far, as all our calculations have returned a single number. But this is about to change!\nThe easiest vectors to create in R are sequences: e.g., the list of numbers from 1 to 10:\n\n\n\n\n\n\n\n\nHere, our output is a vector of 10 elements. R still tells us where the vector starts (at the first element) but nothing else. Change this code to create the first 100 elements and read R’s output. Do you understand the output?\nWhen R starts a new line of output, it tells you where you are in the vector. In RStudio, type letters to see the built-in vector of letters; this is a nice example of how the position information can be helpful in sorting through printed output.\nBy default, R operates on vectors elementwise:\n\n\n\n\n\n\n\n\nHere the sqrt function is applied to each element separately.\nWhen two vectors are combined, the operation also works elementwise:\n\n\n\n\n\n\n\n\n(Note that the sequence operator (:) has higher precedence than most arithmetic so this does “what you’d expect.”)\nThings get weird if the two vectors are of different lenghts:\n\n\n\n\n\n\n\n\nUnder the hood, R “recycles” 3 to be a vector of length 5 and then operators elementwise. That is, R computes\n\n3 + 1\n3 + 2\n3 + 3\n3 + 4\n3 + 5\n\nand combines the results.\nThis in general gives useful results, but the results can be quite alarming if combining vectors of unaligned size:\n\n\n\n\n\n\n\n\nThankfully, R gives a warning that something weird happened. (This might seem annoying, but warnings are great! They help you find likely errors before anything too bad happens. Most experienced programmers wish R had more built-in warnings.)\nIt’s worth distinguishing warnings from errors. Errors occur when R absolutely cannot and will not execute your command:\n\n\n\n\n\n\n\n\nIn this case, it is impossible to add the number 3 to a letter, so R throws an error.\nWarnings are hints of possible problems, but do not prevent execution. When dealing with external software and packages, you will often get warnings about old versions of software. These are encouraging you to update, but unless you see an error, things probably worked out ok.\nSome of R’s built-in functions can be used to “summarize” a vector down to a single scalar (or, more precisely, a length 1 vector). These include sum, max, min, and mean. For example, we can compute the sum of the first 100 numbers as follows:\n\n\n\n\n\n\n\n\nApocryphally, a young C.F. Gauss did this calculation in his head to the great surprise of his school teacher. We might not have Gauss’s skills at arithmetic, but we can do quite a lot with R.\nFor example, the famous “Bessel problem” of mathematics is to compute\n\\[ \\sum_{n=1}^{\\infty} \\frac{1}{n^2} = 1 + \\frac{1}{2^2} + \\frac{1}{3^2} + \\dots\\]\nEuler showed, somewhat remarkably, that the answer is \\(\\pi^2 / 6\\). We won’t repeat Euler’s analysis here, but let’s confirm it using R.\n\n\n\n\n\n\n\n\nPretty good alignment!\nDo you understand everything that happened here? If so, you’re ready for next week’s class. Go ahead and fill out this week’s Brightspace quiz."
  },
  {
    "objectID": "preassigns/pa05.html",
    "href": "preassigns/pa05.html",
    "title": "STA/OPR 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "",
    "text": "Due Date: 2024-09-25 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nLast week, we considered single-table verbs: these are appropriate for asking complex questions of a nicely formatted data frame. Sadly, we are rarely provided data frames suitable for every question we might seek to answer. Instead, we typically need to combine information from multiple sources. For instance, if we want to examine the relationship between demographics and electoral results, we will need to combine information from the US Census Bureau and local elections administrators. Or, if we want to investigate the relationship between a company’s financial status and its stock performance, we might need to combine information from multiple databases."
  },
  {
    "objectID": "preassigns/pa05.html#basic-joins",
    "href": "preassigns/pa05.html#basic-joins",
    "title": "STA/OPR 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Basic Joins",
    "text": "Basic Joins\nThe basic operator of combining different tables is the join, patterned after SQL. Each join operates using some notion of “match” between tables. In the best case, this is done using a unique identifier - one universal and consistent name for each entity. Sadly, such perfect identifiers rarely exist. For instance, companies change their names and their ticker symbols somewhat regularly (e.g., Facebook becoming Meta)\nThe simplest join is the inner_join, which returns rows which match between the two tables:\n\nlibrary(dplyr)\nband_members\n\n# A tibble: 3 × 2\n  name  band   \n  &lt;chr&gt; &lt;chr&gt;  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\n\n\nband_instruments\n\n# A tibble: 3 × 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\n\nIn this case, the name column forms a unique ID, so we can use it for our join.\n\ninner_join(band_members, band_instruments)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nWe see here that R automatically performed the join using the common column (name): if we want to be clearer, let’s specify the join element ourselves:\n\ninner_join(band_members, band_instruments, join_by(name))\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nHere join_by is a helper function that can be used to specify the form of the join used. In some contexts, the “common” column has different names in the two tables, so we can use a more explicit call to join_by:\n\nband_instruments2\n\n# A tibble: 3 × 2\n  artist plays \n  &lt;chr&gt;  &lt;chr&gt; \n1 John   guitar\n2 Paul   bass  \n3 Keith  guitar\n\n\nNote that this is the same as band_instruments, but with the name column changed to artist.\n\ninner_join(band_members, band_instruments2, join_by(name == artist))\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nI like to always use this final - most explicit - form, even when the column names are the same between the two tables (join_by(name == name)).\nLet’s look more closely at the result here: we return a table with 3 columns and two rows:\n\ninner_join(band_members, band_instruments, join_by(name == name))\n\n# A tibble: 2 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\n\nThe three columns are pretty easy to understand:\n\nname is the (shared) name column from each table\nband comes from the band_members table\nplays comes from the band_instruments table.\n\nThe two rows are a bit trickier: each of our input tables had three rows, but there were only two “overlaps” so that’s what we get back from an inner_join. Specifically, we drop Mick [Jagger] from band_members because he doesn’t appear in band_instrumentsand we drop Keith [Richards] from band_instruments because he doesn’t appear in band_members.\nIn brief, an inner join is an intersection join. We only get rows back which have a match in both tables.\nOther join operators have complimentary behaviors: the full join (also sometimes called an outer join) is basically a union join. We get back all rows from both tables, regardless of whether a match has been found. But what happens with those unmatched rows?\n\nfull_join(band_members, band_instruments, join_by(name == name))\n\n# A tibble: 4 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\nR fills in the “blanks” with NA values. Here, we can assume Mick [Jagger] plays an instrument, but it is unknown to R here.\nFinally, we have the intermediate left join, which keeps all rows from one table whether or not they have a match:\n\nleft_join(band_members, band_instruments, join_by(name == name))\n\n# A tibble: 3 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\n\nHere we keep Mick because he is in band_members, even though he is missing from band_instruments. Conversely, we drop Keith because he isn’t in band_members (even though he is in band_instruments).\nR also provides a right_join, but it’s not really different: it’s just a “flipped” left_join: left_join(x, y) == right_join(y, x).\nThe following image1 summarizes the different types of joins:\n\nThe anti_join returns elements that appear in one data set, but not the other. It’s rarer, but occasionally useful."
  },
  {
    "objectID": "preassigns/pa05.html#joins-with-repeats",
    "href": "preassigns/pa05.html#joins-with-repeats",
    "title": "STA/OPR 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Joins with Repeats",
    "text": "Joins with Repeats\nIn the previous examples, we have seen joins that have a “one-to-one” (inner) or possibly “one-to-none” (full, left) structure. In many circumstances, we find ourselves with a “one-to-many” type structure, even when both data sets are “tidy”. This typically occurs because different data sets have different models of what a “unit” is. For example, consider a hypothetical instructor who has i) a table with student names and contact information; and ii) a table with grades on different assignments.\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 3\n)\n\ngrades &lt;- tribble(\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    3,           \"B\",            50,\n    1,           \"C\",            95,\n    2,           \"C\",            50,\n    3,           \"C\",            80\n)\n\nWhat happens if we join these?\n\ninner_join(students, grades, join_by(id == student_id))\n\n# A tibble: 9 × 5\n  name     email                id assignment_id grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1 A               100\n2 Bernard  bernard@cuny.edu      1 B                95\n3 Bernard  bernard@cuny.edu      1 C                95\n4 Hunter   hunter@cuny.edu       2 A                95\n5 Hunter   hunter@cuny.edu       2 B                80\n6 Hunter   hunter@cuny.edu       2 C                50\n7 John Jay john.jay@cuny.edu     3 A                80\n8 John Jay john.jay@cuny.edu     3 B                50\n9 John Jay john.jay@cuny.edu     3 C                80\n\n\n(Note here that we need the explicit join_by since the column names don’t match between the two tables: id in students gets joined to student_id. This pattern of id in table tbl getting joined to tbl_id elsewhere is quite common in database design.)\nWe get repeats of the student rows: for each valid student-grade pair, we have a row. From here, we can compute final grades:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    group_by(name, email, id) |&gt;\n    summarize(final_avg = mean(grade)) |&gt;\n    mutate(final_grade = \n               case_when(final_avg &gt; 90 ~ \"A\", \n                         final_avg &gt; 80 ~ \"B\", \n                         final_avg &gt; 70 ~ \"C\", \n                         final_avg &gt; 60 ~ \"D\", \n                         TRUE ~ \"F\")) # In a case_when, TRUE == \"else\"\n\n`summarise()` has grouped output by 'name', 'email'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 5\n# Groups:   name, email [3]\n  name     email                id final_avg final_grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1 Bernard  bernard@cuny.edu      1      96.7 A          \n2 Hunter   hunter@cuny.edu       2      75   C          \n3 John Jay john.jay@cuny.edu     3      70   D          \n\n\nIn this case, everything works well. But let’s try a slightly trickier case, with some students who never fail to submit certain assignments.\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 3\n)\n\ngrades &lt;- tribble(\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    1,           \"C\",            95,\n    3,           \"C\",            80\n)\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    group_by(name, email, id) |&gt;\n    summarize(final_avg = mean(grade)) |&gt;\n    mutate(final_grade = \n               case_when(final_avg &gt; 90 ~ \"A\", \n                         final_avg &gt; 80 ~ \"B\", \n                         final_avg &gt; 70 ~ \"C\", \n                         final_avg &gt; 60 ~ \"D\", \n                         TRUE ~ \"F\"))\n\n`summarise()` has grouped output by 'name', 'email'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 5\n# Groups:   name, email [3]\n  name     email                id final_avg final_grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1 Bernard  bernard@cuny.edu      1      96.7 A          \n2 Hunter   hunter@cuny.edu       2      87.5 B          \n3 John Jay john.jay@cuny.edu     3      80   C          \n\n\nWhy did the final grades go up after we deleted rows?\n\ninner_join(students, \n           grades, \n           join_by(id == student_id))\n\n# A tibble: 7 × 5\n  name     email                id assignment_id grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1 A               100\n2 Bernard  bernard@cuny.edu      1 B                95\n3 Bernard  bernard@cuny.edu      1 C                95\n4 Hunter   hunter@cuny.edu       2 A                95\n5 Hunter   hunter@cuny.edu       2 B                80\n6 John Jay john.jay@cuny.edu     3 A                80\n7 John Jay john.jay@cuny.edu     3 C                80\n\n\nThe “missing” assignments for Hunter and John Jay aren’t reported as zeros - they are just ignored! And hence R takes an average over the two assignments where these students did well, not all three assignments. We’ll talk about one way to fix this below, but for now I’m just flagging it as a possible issue that can come up with missing data and joins. (Here the rows were missing, so it’s harder to catch than a plain NA; better data management would have included a “0” row instead of deleting them, but we don’t always get to assume super well-organized data.)\nSo far, our join results have been relatively straightforward because we have had ‘good’ unique identifiers. If we find ourselves in a situation where we lack unique IDs, things can go wrong quickly:\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 2  # Accidentally repeat an ID\n)\ngrades &lt;- tribble( # Back to the complete data\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    3,           \"B\",            50,\n    1,           \"C\",            95,\n    2,           \"C\",            50,\n    3,           \"C\",            80\n)\n\nfull_join(students, \n          grades, \n          join_by(id == student_id))\n\nWarning in full_join(students, grades, join_by(id == student_id)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 2 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 12 × 5\n   name     email                id assignment_id grade\n   &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 Bernard  bernard@cuny.edu      1 A               100\n 2 Bernard  bernard@cuny.edu      1 B                95\n 3 Bernard  bernard@cuny.edu      1 C                95\n 4 Hunter   hunter@cuny.edu       2 A                95\n 5 Hunter   hunter@cuny.edu       2 B                80\n 6 Hunter   hunter@cuny.edu       2 C                50\n 7 John Jay john.jay@cuny.edu     2 A                95\n 8 John Jay john.jay@cuny.edu     2 B                80\n 9 John Jay john.jay@cuny.edu     2 C                50\n10 &lt;NA&gt;     &lt;NA&gt;                  3 A                80\n11 &lt;NA&gt;     &lt;NA&gt;                  3 B                50\n12 &lt;NA&gt;     &lt;NA&gt;                  3 C                80\n\n\nIn this case, R is kind enough to warn us that a “many-to-many” join has happened (joining multiple students to one grade and multiple grades to one student). This is a very good warning and it highlights a true error here. If faced with data like this, you may not be able to address it with fixing the underlying data, but at least you know something has gone awry."
  },
  {
    "objectID": "preassigns/pa05.html#compound-joins",
    "href": "preassigns/pa05.html#compound-joins",
    "title": "STA/OPR 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Compound Joins",
    "text": "Compound Joins\nOften, data lack a unique identifier, but you can piece one together with several columns: that is, taken on its own, no column is unique, but the tuples formed by comining several columns are unique, e.g., data with year, month, and day columns.\n\nrevenues &lt;- tribble(\n    ~year, ~month, ~day, ~revenue,\n    2024,  09,     22,   100,\n    2024,  09,     23,   200,\n    2024,  10,     22,   200,\n    2024,  10,     22,   200,\n    2025,  09,     22,   500\n    )\n\nexpenses &lt;- tribble(\n    ~year, ~month, ~day, ~expenses,\n    2024,  09,     22,   -200,\n    2024,  09,     23,   -200,\n    2024,  10,     22,   -200,\n    2024,  10,     23,   -200,\n    2025,  09,     22,   -300\n    )\n\nIn this case, a simple join on any one column goes astray:\n\ninner_join(revenues, expenses, join_by(day == day))\n\nWarning in inner_join(revenues, expenses, join_by(day == day)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 14 × 7\n   year.x month.x   day revenue year.y month.y expenses\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1   2024       9    22     100   2024       9     -200\n 2   2024       9    22     100   2024      10     -200\n 3   2024       9    22     100   2025       9     -300\n 4   2024       9    23     200   2024       9     -200\n 5   2024       9    23     200   2024      10     -200\n 6   2024      10    22     200   2024       9     -200\n 7   2024      10    22     200   2024      10     -200\n 8   2024      10    22     200   2025       9     -300\n 9   2024      10    22     200   2024       9     -200\n10   2024      10    22     200   2024      10     -200\n11   2024      10    22     200   2025       9     -300\n12   2025       9    22     500   2024       9     -200\n13   2025       9    22     500   2024      10     -200\n14   2025       9    22     500   2025       9     -300\n\n\nNote the warning!\nIn this scenario, we should really “tidy” up the data by combining the date information, which is spread across three columns, into a single column, but we have the alternative option of a compound join:\n\ninner_join(revenues, expenses, \n           join_by(day == day, \n                   month == month, \n                   year == year))\n\n# A tibble: 5 × 5\n   year month   day revenue expenses\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1  2024     9    22     100     -200\n2  2024     9    23     200     -200\n3  2024    10    22     200     -200\n4  2024    10    22     200     -200\n5  2025     9    22     500     -300\n\n\nHere, as with filter, the list of conditions looks for an intersection: we want all three parts of the date to match."
  },
  {
    "objectID": "preassigns/pa05.html#pivots",
    "href": "preassigns/pa05.html#pivots",
    "title": "STA/OPR 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Pivots",
    "text": "Pivots\nFinally, we may want to re-arrange the output of a join. Returning to our grades example from above:\n\nstudents &lt;- tribble(\n    ~name, ~email, ~id,\n    \"Bernard\",  \"bernard@cuny.edu\",  1,\n    \"Hunter\",   \"hunter@cuny.edu\",   2,\n    \"John Jay\", \"john.jay@cuny.edu\", 3\n)\n\ngrades &lt;- tribble(\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    3,           \"B\",            50,\n    1,           \"C\",            95,\n    2,           \"C\",            50,\n    3,           \"C\",            80\n)\n\ngrade_book &lt;- inner_join(students, \n                         grades, \n                         join_by(id == student_id))\n\ngrade_book\n\n# A tibble: 9 × 5\n  name     email                id assignment_id grade\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1 A               100\n2 Bernard  bernard@cuny.edu      1 B                95\n3 Bernard  bernard@cuny.edu      1 C                95\n4 Hunter   hunter@cuny.edu       2 A                95\n5 Hunter   hunter@cuny.edu       2 B                80\n6 Hunter   hunter@cuny.edu       2 C                50\n7 John Jay john.jay@cuny.edu     3 A                80\n8 John Jay john.jay@cuny.edu     3 B                50\n9 John Jay john.jay@cuny.edu     3 C                80\n\n\nThis isn’t really how we like to see gradebooks: a “wider” format, with a column for each assignment, may be more preferable. In this case, we want to use the pivot_wider column from the tidyr package.\npivot_wider takes a few key arguments:\n\nid_cols which columns that (taken together) uniquely identify a row in the final table\nnames_from: where should we get the column names of the new table\nvalues_from: where should we get the values of the new table\n\nThis is maybe easier by example:\n\ngrade_book |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from=assignment_id,\n                values_from=grade)\n\n# A tibble: 3 × 6\n  name     email                id     A     B     C\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95\n2 Hunter   hunter@cuny.edu       2    95    80    50\n3 John Jay john.jay@cuny.edu     3    80    50    80\n\n\nThis pivot trick is particularly useful for finding missing rows, like those that tripped us up earlier:\n\ngrades &lt;- tribble( # Implicit missing values\n    ~student_id, ~assignment_id, ~grade,\n    1,           \"A\",            100,\n    2,           \"A\",            95,\n    3,           \"A\",            80,\n    1,           \"B\",            95,\n    2,           \"B\",            80,\n    1,           \"C\",            95,\n    3,           \"C\",            80\n)\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade)\n\n# A tibble: 3 × 6\n  name     email                id     A     B     C\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95\n2 Hunter   hunter@cuny.edu       2    95    80    NA\n3 John Jay john.jay@cuny.edu     3    80    NA    80\n\n\nHere, our missing values are now explicit!\nWe can also explicitly fill the NA with a value of our choice, here 0:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0)\n\n# A tibble: 3 × 6\n  name     email                id     A     B     C\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95\n2 Hunter   hunter@cuny.edu       2    95    80     0\n3 John Jay john.jay@cuny.edu     3    80     0    80\n\n\nThere is also an inverse operator pivot_longer which takes a wide table (like this) and makes it longer.\nTo complete our grade book example, we might want to take the average across the three grade columns:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0) |&gt;\n    group_by(name) |&gt;\n    mutate(final_avg = mean(c_across(A:C)))\n\n# A tibble: 3 × 7\n# Groups:   name [3]\n  name     email                id     A     B     C final_avg\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95      96.7\n2 Hunter   hunter@cuny.edu       2    95    80     0      58.3\n3 John Jay john.jay@cuny.edu     3    80     0    80      53.3\n\n\nNote here that we need to use a mutate since our final grade book has the same number of rows before and after we add the final average column. The c_across column here is a variant of the standard c function used to combine scalars: here we’re creating a new length-3 vector of the student’s three grades and passing it to the mean function.\nWhat is group_by(name) doing here? See what happens without it:\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0) |&gt;\n    mutate(final_avg = mean(c_across(A:C)))\n\n# A tibble: 3 × 7\n  name     email                id     A     B     C final_avg\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95      69.4\n2 Hunter   hunter@cuny.edu       2    95    80     0      69.4\n3 John Jay john.jay@cuny.edu     3    80     0    80      69.4\n\n\nRecall that mean is a summarization function - it will combine data from across rows if no grouping structure is present. Since we want seperate averages for each student, we need a group_by. In this case, name is a unique identifier for each student, so we can group on it. We also have the rowwise() helper, which automatically creates group structure with each group a separate row. If you don’t have a clean unique identifier, or just can’t think of one easily, this is sometimes a useful helper.\n\ninner_join(students, \n           grades, \n           join_by(id == student_id)) |&gt;\n    pivot_wider(id_cols = c(name, email, id), \n                names_from = assignment_id,\n                values_from = grade, \n                values_fill = 0) |&gt;\n    rowwise() |&gt;\n    mutate(final_avg = mean(c_across(A:C)))\n\n# A tibble: 3 × 7\n# Rowwise: \n  name     email                id     A     B     C final_avg\n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Bernard  bernard@cuny.edu      1   100    95    95      96.7\n2 Hunter   hunter@cuny.edu       2    95    80     0      58.3\n3 John Jay john.jay@cuny.edu     3    80     0    80      53.3\n\n\nIn class, we will explore joins in more detail by combining the flights data with plane, airport, and weather factors.\nPlease now go fill out the weekly quiz on Brightspace."
  },
  {
    "objectID": "preassigns/pa05.html#footnotes",
    "href": "preassigns/pa05.html#footnotes",
    "title": "STA/OPR 9750 Week 5 Pre Assignment: Multi-Table dplyr Verbs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdapted from Data Carpentry↩︎"
  },
  {
    "objectID": "preassigns/pa04.html",
    "href": "preassigns/pa04.html",
    "title": "STA/OPR 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "",
    "text": "Due Date: 2024-09-18 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week, we begin manipulating R’s most important structure the data.frame. While base R provides tools for working with data.frame objects, our primary focus will be on the tidyverse family of tools. These provide a unified and consistent set of tools for working with data objects."
  },
  {
    "objectID": "preassigns/pa04.html#what-is-a-data.frame",
    "href": "preassigns/pa04.html#what-is-a-data.frame",
    "title": "STA/OPR 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "What is a data.frame?",
    "text": "What is a data.frame?\nLast week, we discussed vectors, one-dimensional collections of the same type of object. While vectors are an important computational tool, real data is rarely quite so simple: we collect multiple features (covariates) from each of our observations and these features may be of different type. For example, when performing a political survey, we may record the:\n\nName (character)\nAge (integer)\nGender (factor)1\nDate of Contact (Date)\nLevel of candidate support (double or numeric)\n\nof each respondant. It is natural to organize this in tabular (“spreadsheet”) form:\n\n\n\nName\nAge\nGender\nDate of Contact\nLevel of Support\n\n\n\n\nTimmy\n25\nM\n2024-09-13\n0.25\n\n\nTammy\n50\nF\n2024-06-20\n-1.35\n\n\nTaylor\n70\nX\n2024-08-15\n200\n\n\nTony\n40\nM\n2024-12-25\n0\n\n\nToni\n65\nF\n2024-11-28\n-4\n\n\n\nNote several important features of this data:\n\nEach row corresponds to one, and only one, sample\nEach column corresponds to one, and only one, feature\nThe values in each column are all of the same type\nThe order doesn’t matter: all important data is reflected in the values, not the presentation\n\nGenerally, data in this pattern will be called “tidy”. R represents this type of data as a data.frame object. For more on what it means for data to be “tidy”, read this paper.\n\nTibbles and the Tidyverse\nMany of the tools we will discuss in this class are from a set of related R packages, collectively known as the tidyverse. They are designed to i) read data from outside of R into tidy formats; ii) manipulate data from one tidy format to another; iii) communicate the results of tidy data analyses.\nWhile you can load these packages separately, they are used together so frequently that the tidyverse package exists to load them all simultaneously. I recommend you start most of your analyses with the command:\n\nlibrary(tidyverse)\n\nThis will automatically load the following packages:\n\nlubridate for date and time manipulation\nforcats for factor manipulation\nstringr for string manipulation\ndplyr for data frame manipulation\npurrr for functional programming\nreadr for tidy data import\ntidyr for tidy data manipulation\ntibble for data frame enhancement\nggplot2 for data visualization\n\nThis week, we are focusing on functionality from the dplyr package.\nYou may, from time to time, see reference to tibbles in R documentation. A tibble is a “souped-up” data frame with somewhat better default printing. For almost all purposes-and everywhere in this course- you can substitute tibble with data.frame without issue.\nBefore we get into dplyr, let’s make sure we have a data frame to play with. Let’s bring back our friends, the Palmer penguins:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "preassigns/pa04.html#subsetting-data-frames",
    "href": "preassigns/pa04.html#subsetting-data-frames",
    "title": "STA/OPR 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Subsetting Data Frames",
    "text": "Subsetting Data Frames\nOur first task will be to subset data frames: that is, to select only some rows and columns and to return a smaller data frame than the one we started with.\n\nSubsetting Columns\nThe main dplyr function for column subsetting is select(). In its simplest form, we provide a set of column names we want to keep and everything else gets dropped.\n\nselect(penguins, species, island)\n\n# A tibble: 344 × 2\n   species island   \n   &lt;fct&gt;   &lt;fct&gt;    \n 1 Adelie  Torgersen\n 2 Adelie  Torgersen\n 3 Adelie  Torgersen\n 4 Adelie  Torgersen\n 5 Adelie  Torgersen\n 6 Adelie  Torgersen\n 7 Adelie  Torgersen\n 8 Adelie  Torgersen\n 9 Adelie  Torgersen\n10 Adelie  Torgersen\n# ℹ 334 more rows\n\n\nHere, we keep the species and island columns and remove the others.\nPause for a moment to note how the select function is structured: the first argument is the data frame on which we are operating; all following arguments control the resulting behavior. This is a common pattern in dplyr functions, designed to take advantage of another key R functionality, the pipe operator.\nR provides an operator |&gt; which “rewrites” code, so\n\nselect(penguins, species, island)\n\nand\n\npenguins |&gt; select(species, island)\n\nare exactly the same thing as far as R is concerned. You may well ask yourself why bother: the second “piped” operator is a bit longer and a bit harder to type. But just hold on - we’ll see this makes for far cleaner code in the long run.\nJust like base R, if we include - signs in our select statement, we get everything except a certain column:\n\npenguins |&gt; select(-bill_length_mm, -bill_depth_mm, -flipper_length_mm)\n\n# A tibble: 344 × 5\n   species island    body_mass_g sex     year\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n 1 Adelie  Torgersen        3750 male    2007\n 2 Adelie  Torgersen        3800 female  2007\n 3 Adelie  Torgersen        3250 female  2007\n 4 Adelie  Torgersen          NA &lt;NA&gt;    2007\n 5 Adelie  Torgersen        3450 female  2007\n 6 Adelie  Torgersen        3650 male    2007\n 7 Adelie  Torgersen        3625 female  2007\n 8 Adelie  Torgersen        4675 male    2007\n 9 Adelie  Torgersen        3475 &lt;NA&gt;    2007\n10 Adelie  Torgersen        4250 &lt;NA&gt;    2007\n# ℹ 334 more rows\n\n\nHere, we dropped three columns.\nThe select operator provides more advanced functionality which is useful for very complex data structures, but we don’t need to dive into that just yet.\n\n\nSubsetting Rows\nOften in data analysis, we want to focus on a subset of the entire population: e.g., we might want to know the fraction of women supporting a certain political candidate or the rate of a rare cancer among patients 65 or older. In this case, we need to select only those rows of our data that match some criterion. This brings us to the filter operator.\nfilter takes a logical vector and uses it to select rows of a data frame. Most commonly, this logical vector is created by performing some sort of tests on the values in the data frame. For example, if we want to select only the male penguins in our data set, we may write:\n\npenguins |&gt; filter(sex == \"male\")\n\n# A tibble: 168 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.3          20.6               190        3650\n 3 Adelie  Torgersen           39.2          19.6               195        4675\n 4 Adelie  Torgersen           38.6          21.2               191        3800\n 5 Adelie  Torgersen           34.6          21.1               198        4400\n 6 Adelie  Torgersen           42.5          20.7               197        4500\n 7 Adelie  Torgersen           46            21.5               194        4200\n 8 Adelie  Biscoe              37.7          18.7               180        3600\n 9 Adelie  Biscoe              38.2          18.1               185        3950\n10 Adelie  Biscoe              38.8          17.2               180        3800\n# ℹ 158 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHere, note the use of the == operator for testing equality. A very common mistake is to use the single equals (assignment) operator inside of filter. Thankfully, dplyr will alert us with an error if we make this mistake:\n\npenguins |&gt; filter(sex = \"male\")\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `sex == \"male\"`?\n\n\nIf we supply multiple tests to filter, we get the intersection: that is, we get rows that pass all tests.\n\npenguins |&gt; filter(sex == \"male\", bill_length_mm &gt; 38)\n\n# A tibble: 156 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.3          20.6               190        3650\n 3 Adelie  Torgersen           39.2          19.6               195        4675\n 4 Adelie  Torgersen           38.6          21.2               191        3800\n 5 Adelie  Torgersen           42.5          20.7               197        4500\n 6 Adelie  Torgersen           46            21.5               194        4200\n 7 Adelie  Biscoe              38.2          18.1               185        3950\n 8 Adelie  Biscoe              38.8          17.2               180        3800\n 9 Adelie  Biscoe              40.6          18.6               183        3550\n10 Adelie  Biscoe              40.5          18.9               180        3950\n# ℹ 146 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIf we want the union-rows that satisfy any of the tests-we have to use the logical operators we previous applied to vectors:\n\npenguins |&gt; filter( (sex == \"male\") | (bill_length_mm &gt; 38))\n\n# A tibble: 292 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           39.3          20.6               190        3650\n 5 Adelie  Torgersen           38.9          17.8               181        3625\n 6 Adelie  Torgersen           39.2          19.6               195        4675\n 7 Adelie  Torgersen           42            20.2               190        4250\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 282 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIt’s a bit clunky, but thankfully, this is somewhat less common than checking that all conditions are satisfied.\ndplyr provides all sorts of useful helpers for creating test statements, e.g., the\n\nbetween\nnear\n\nfunctions.\nEven more useful than these, however, are the slice_*() functions which can be used to perform “top \\(k\\)” type operations. If we want the five largest Adelie penguins, we might try something like:\n\npenguins |&gt; filter(species == \"Adelie\") |&gt; slice_max(body_mass_g, n=5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Biscoe              43.2          19                 197        4775\n2 Adelie  Biscoe              41            20                 203        4725\n3 Adelie  Torgersen           42.9          17.6               196        4700\n4 Adelie  Torgersen           39.2          19.6               195        4675\n5 Adelie  Dream               39.8          19.1               184        4650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nslice_min() works similarly. slice_head and slice_tail get the first and last rows by sort order - in general, I recommend against using these. A key rule of data analysis is that the row order is not semantically meaningful, but it’s good to keep them in the back of your mind just in case. slice_sample can be used to select random subsets of data.\nAnother important subseting function is drop_na which will drop any rows with missing (NA) data.2 This is a good and useful tool, but before you apply it, always ask yourself “why is this data missing?” Understanding the abscence of data is often just as important as understanding the non-missing data. For example, is a student’s SAT score missing on a college application because they i) forgot to list it; ii) never took the SAT; or iii) took the test but chose to omit their score because they did poorly? Proper handling of missing data is often very problem-specific and very hard."
  },
  {
    "objectID": "preassigns/pa04.html#manipulating-and-creating-columns",
    "href": "preassigns/pa04.html#manipulating-and-creating-columns",
    "title": "STA/OPR 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Manipulating and Creating Columns",
    "text": "Manipulating and Creating Columns\nA key task in data analysis is transforming data. As discussed in class, one of the guiding themes of R programming is data integrity and an important way R ensures this is by applying commands to the entire vector. In the data frame context, we apply commands to an entire column. The mutate function is dplyr’s main interface for column creation and manipulation.\nIn general, each argument to mutate takes a name = value pair: the name is the name of a column to be created from value. value can be an arbitrary function of other columns. If name corresponds to an existing column, that column is silently overwritten.\nFor example, if we want to convert penguin bill lengths from millimeters to inches, we might operator:\n\npenguins |&gt; mutate(bill_length_in = bill_length_mm / 25.4)\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_length_in &lt;dbl&gt;\n\n\nHere, note that the bill_length_mm column is retained - and all columns are kept! mutate only creates new columns; it won’t secretly drop them.\nA particularly common operator is changing the name of a column without changing its values. You can use mutate and select(-) for this, but rename provides essentially the same interface and its semantically clearer:\n\npenguins |&gt; rename(mass = body_mass_g)\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm  mass sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt; &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen           39.1          18.7               181  3750 male  \n 2 Adelie  Torgersen           39.5          17.4               186  3800 female\n 3 Adelie  Torgersen           40.3          18                 195  3250 female\n 4 Adelie  Torgersen           NA            NA                  NA    NA &lt;NA&gt;  \n 5 Adelie  Torgersen           36.7          19.3               193  3450 female\n 6 Adelie  Torgersen           39.3          20.6               190  3650 male  \n 7 Adelie  Torgersen           38.9          17.8               181  3625 female\n 8 Adelie  Torgersen           39.2          19.6               195  4675 male  \n 9 Adelie  Torgersen           34.1          18.1               193  3475 &lt;NA&gt;  \n10 Adelie  Torgersen           42            20.2               190  4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;"
  },
  {
    "objectID": "preassigns/pa04.html#operating-with-group-structure",
    "href": "preassigns/pa04.html#operating-with-group-structure",
    "title": "STA/OPR 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Operating with Group Structure",
    "text": "Operating with Group Structure\nWe often want to summarize our data in useful ways: these can be simple summaries like mean (average) or standard deviation or more complex operations (trend lines). In the world of dplyr, the summarize function is used whenever we want to reduce multiple rows to a single data point. Note how this differs from filter: filter drops rows, summarize combines and reduces them.\nFor example, if we want to get the number of male penguins in our data set, we can use the n() function, which counts the number of rows:\n\npenguins |&gt; summarize(number = n())\n\n# A tibble: 1 × 1\n  number\n   &lt;int&gt;\n1    344\n\n\nLook at all those penguins!\nThis is a relatively simple operation, but we can be a bit more complex: e.g., with the mean function:\n\npenguins |&gt; summarize(body_mass_avg = mean(body_mass_g))\n\n# A tibble: 1 × 1\n  body_mass_avg\n          &lt;dbl&gt;\n1            NA\n\n\nWait! Why didn’t that work?\nRecall that the penguins data had some missing (NA) values. When we ask R to compute the average, it can’t! Specifically, depending on the missing values, the mean could be anything, so R returns a missing (unknown) value for the mean as well. Many base R functions have this default behavior and have an optional flag for automatically removing NA values:\n\npenguins |&gt; summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  body_mass_avg\n          &lt;dbl&gt;\n1         4202.\n\n\nHere na.rm=TRUE means to remove all na values before computing the mean.\nThe summarize function is particularly powerful when applied groupwise: e.g., what is the average body mass by species? In dplyr world, this is a two-step operation:\n\npenguins |&gt; \n    group_by(species) |&gt;\n    summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE))\n\n# A tibble: 3 × 2\n  species   body_mass_avg\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Adelie            3701.\n2 Chinstrap         3733.\n3 Gentoo            5076.\n\n\nWe added the group_by operator here. Note that, on its own, group_by doesn’t really do anything:\n\npenguins |&gt; \n    group_by(species)\n\n# A tibble: 344 × 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nWe have added some grouping metadata but the actual data does not get changed until the summarize step.\nWe can also group by more than one element:\n\npenguins |&gt; \n    group_by(species, sex) |&gt;\n    summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex    body_mass_avg\n  &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Adelie    &lt;NA&gt;           3540 \n4 Chinstrap female         3527.\n5 Chinstrap male           3939.\n6 Gentoo    female         4680.\n7 Gentoo    male           5485.\n8 Gentoo    &lt;NA&gt;           4588.\n\n\nNote here that the result is still grouped and that only the last (sex) grouping was removed. That means that any future operations will be automatically grouped by species. If you want to remove all grouping structure, add the ungroup operator at the end:\n\npenguins |&gt; \n    group_by(species, sex) |&gt;\n    summarize(body_mass_avg = mean(body_mass_g, na.rm=TRUE)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n  species   sex    body_mass_avg\n  &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Adelie    &lt;NA&gt;           3540 \n4 Chinstrap female         3527.\n5 Chinstrap male           3939.\n6 Gentoo    female         4680.\n7 Gentoo    male           5485.\n8 Gentoo    &lt;NA&gt;           4588.\n\n\ngroup_by metadata is also useful when summary statistics are computed implicitly by other functions. E.g., if we want to get all penguins that are above average mass for their species, we might try the following:\n\npenguins |&gt; \n    group_by(species) |&gt;\n    filter(body_mass_g &gt;= mean(body_mass_g, na.rm=TRUE)) |&gt;\n    ungroup()\n\n# A tibble: 159 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           39.2          19.6               195        4675\n 4 Adelie  Torgersen           42            20.2               190        4250\n 5 Adelie  Torgersen           38.6          21.2               191        3800\n 6 Adelie  Torgersen           34.6          21.1               198        4400\n 7 Adelie  Torgersen           42.5          20.7               197        4500\n 8 Adelie  Torgersen           46            21.5               194        4200\n 9 Adelie  Biscoe              35.9          19.2               189        3800\n10 Adelie  Biscoe              38.2          18.1               185        3950\n# ℹ 149 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHere, R applies the summarization function mean groupwise for us.\nBefore we close out, let’s put this all together: what species has the largest difference in average body mass between the sexes?\nAnswer: Gentoo penguins have the largest sex difference in average body mass.\nBefore completing the Brightspace submission for this assignment, look up the source for this document on my GitHub (Hint: see the buttons on the sidebar) and see i) how I computed the answer; and ii) how I included it in the rendered text. This will be helpful as you begin preparing your first report."
  },
  {
    "objectID": "preassigns/pa04.html#footnotes",
    "href": "preassigns/pa04.html#footnotes",
    "title": "STA/OPR 9750 Week 4 Pre Assignment: Single-Table dplyr Verbs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRecall a factor is a vector with a fixed set of possible values, often used to represent categorical data. Here, we follow the NY DMV and allow M, F, and X values for sex, but, in general, representation of sex and gender in databases is a tricky problem. See this essay for a list of some of the complexity of real people. (This essay follows in a longer tradition of “the world is much more complicated than you would believe” essays: names, time, addresses. People - and the world we create - are infinitely complex.↩︎\nWe will say much more about R’s missing data model in class this week.↩︎"
  },
  {
    "objectID": "preassigns/pa07.html",
    "href": "preassigns/pa07.html",
    "title": "STA/OPR 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "",
    "text": "Due Date: 2024-10-16 (Wednesday) at 11:45pm\nSubmission: CUNY Brightspace\nThis week we begin to explore statistical visualizations. Visualizations play several interrelated roles in statistical practice: we use visualization to explore new data sets, to see how well models fit to data, and to communicate the results of analyses to new audiences. Compared with the ‘point summary’ tools we have discussed to date, visualizations are far more flexible and more powerful: we can extract novel insights from data visualizations, but we can also deceive ourselves and others.\nAs you review this document, also watch how I iterate and refine each figure until I have something I’m finally happy with. This is quite typical of how working data scientists produce plots: you rarely know exactly what you want, particularly before you begin to explore your data. You should adapt a similar pattern of “take a sad plot and make it better” as you create plots for your mini-projects and, ultimately, your final project."
  },
  {
    "objectID": "preassigns/pa07.html#grammar-of-graphics",
    "href": "preassigns/pa07.html#grammar-of-graphics",
    "title": "STA/OPR 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nComputer graphics is a field of essentially infinite possibility - anything you dream can be represented in the digital domain. At one limit, we have the option to work on a “per pixel” basis, telling the computer exactly what to draw and where. Because this is clearly overwhelmingly monotonous, very little work is actually done at such fine detail and abstraction layers are provided to automate the low-level detail work. We will use an abstraction model known as the Grammar of Graphics.\nDesigned by Leland Wilkinson and popularized by Hadley Wickham, the Grammar of Graphics poses a set of rules for visualizing (tidy) data. It draws its name from the concept of linguistic grammar - the rules that dictate how basic elements (nouns, verbs, and adjectives) may and may not be combined into clear and meaningful sentences The Grammar of Graphics provides specifications for combining different plot elements (legends, data, axes, etc.) into clear and meaningful statistical graphics. Taking the metaphor too far, we might say that the Grammar of Graphics is the Strunk and White to the Ska that are “Infographics.”\nThe grammar of graphics has several interconnected components, which are combined to form a meaningful graphic. We assume that we have a “tidy” data set we want to visualize; recall that, by “tidy”, we mean that our data is\n\nOrganized in a rectangular array\nHomogenously-typed within a column\nOne observation per row\nOne value per cell\n\nGiven this form of tidy data, the grammar of graphics provides us the “parts of speach” necessary to convert tidy data to a visual representation. We’ll only cover the basic components here, leaving more advanced tools for class session:\n\nThe aesthetics are mappings between columns of the data and aspects of the visualization. For instance, “put the grade column on the \\(y\\) axis” or “use color to represent the course ID”.\nScales convert data values to the aesthetics: scales may be quite trivial, e.g. placing continuous values on the \\(x\\) axis in proper order, or more advanced, e.g., binning values and converting them to a sequence of perceptually-ordered colors. )\nGeometric elements or geoms specify how the data are represented on the page through the scales. geoms include basic representations, like points for a scatter plot or lines for a trend plot, as well as more complex objects like boundaries on a map.\nGuides provide interpretational assistance to the viewer. Most guides take the form of legends.\n\nThat’s all a bit abstract, so let’s put it into practice. For now, you shouldn’t worry so much about what each of these really mean; it’s just useful to have a rough sense of what “knob” you want to turn to modify plots."
  },
  {
    "objectID": "preassigns/pa07.html#getting-started-with-ggplot2",
    "href": "preassigns/pa07.html#getting-started-with-ggplot2",
    "title": "STA/OPR 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "Getting started with ggplot2",
    "text": "Getting started with ggplot2\nThe leading implementation of the grammar of graphics is the ggplot2 package in R (gg = Grammar of Graphics). It comes to us from Hadley Wickham and the tidyverse team, who also developed the dplyr and tidyr tools we have been using for the past several weeks.\nLet’s begin by using ggplot2 to explore our penguins data:\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nggplot(penguins)\n\n\n\n\n\n\n\n\nSomewhat underwhelming…\nggplot2, like many of the tools in this course, do exactly what we ask, and no more. Because we have not specified any of the Grammar of Graphics elements, we only get a blank canvas. Let’s now begin by adding an aesthetic to map some of the elements of our data to aspects of our plot.\nSpecifically, suppose we want to see how flipper length correlates with body mass. Let’s make a scatter plot with flipper length on the \\(x\\)-axis and body mass on the \\(y\\)-axis.1\n\nggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g))\n\n\n\n\n\n\n\n\nOk - this is perhaps a bit better. We can see that the columns flipper_lenth_mm and body_mass_g have been placed on the \\(x\\)- and \\(y\\)-axes as we wanted, but we still don’t see anything.\nWe need a geom to actually put “ink to paper”. The simplest geom is a point, useful for making scatter plots.\n\nggplot(penguins, \n       aes(x=flipper_length_mm, y=body_mass_g)) + \n    geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nPretty nifty! Before we go forward, note that ggplot2 adds elements to create ever more complex plots. This is different than dplyr where we “piped” data from one step to the next, refining it along the way.\nHow can we improve the plot above? Before anything else, let’s clean up the \\(x\\) and \\(y\\) axis labels. While the default behavior of showing variable names is helpful for exploratory data analysis, we never want to let variable names “leak” in plots we intend to share with others. We should instead use meaningful (and attractive) axis labels.\n\nggplot(penguins, \n       aes(x=flipper_length_mm, y=body_mass_g)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe can also take advantage of the theme mechanisms of ggplot2 to change the color of the “infrastructure” of our plot. The theme mechanism doesn’t change how the data itself is visualized, but it controls how things like the background, font sizing, etc behave. I tend to prefer the black and white theme over the grey default:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, y=body_mass_g)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdding Color to Depict Species\nOur data set three distinct species and the correlation between flipper and body size may vary across species. Let’s add some color to our plot: since color maps a data element (species) to a graphical aspect (color) we add it to our aesthetic mapping (aes).\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe see here that a few things happened automatically for us:\n\nThe color element was automatically propagated into geom_point. By default, any “top-level” aesthetics are automatically applied to any geom that can handle them.\nA legend was created.\nA color scale was chosen.\n\nThe geom_point help page tells us which aesthetics are required (\\(x, y\\)) and which are optional for the point geom. We couldn’t have gotten away without providing \\(x, y\\) coordinates, but until this point, we were just using the default (black) color.\nTo improve the look of the colors, we can choose a different color scale. I tend to like the colors of the Color Brewer project, though strictly speaking these are designed for use in maps, not scatter plots. You can access these in ggplot2 as follows:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", palette=2)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, I’m using a type=\"qual\" (qualitative) palette because there is no inherent ordering to the penguin types. I like the “bolder” colors of the second palette in this set, but you can adjust the number to try different schemes.\nNext, let’s improve the look of the legend. As before, we see that it is by default titled with the variable name (species). We can provide a proper title instead:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nI still don’t like this legend on the side - it takes too much room, so let’s move it below the image instead. This involves changing a “non-data” element of the plot, so we go through the theme machinery. theme() allows an enormous number of possible changes, but here we want legend.position:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNot so bad!\n\n\nVisualizing Statistical Relationships\nRecall that our goal was to measure the correlation between Body Mass and Flipper Length. We can visualize this correlation on the plot by adding a regression line (recall that for univariate regression like this, the slope of the regression line is \\(\\hat{\\beta} = \\rho_{XY}\\frac{\\sigma_Y}{\\sigma_X}\\)).\nThis is a new geometric element, called a smoother. ggplot2 allows many possible smoothers, but let’s use the lm (linear model) version, which we specify by setting the method argument:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\") + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nBy default, ggplot2 is giving us confidence intervals around the linear trend. These are sometimes useful, but perhaps a bit crowded for now, so let’s turn them off:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\", \n                se=FALSE) + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis is nicer, but maybe still a bit crowded. It would be nicer if we could avoid the “overlaps” of the different species. Here, let’s break out a “small multiples” plot: this is, in essence, a group_by for plotting.\nIn ggplot2 speak, this is called a faceted plot:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\", \n                se=FALSE) + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNot too shabby. But now it’s a bit too hard to tell the lines from the points. Let’s override the color used:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    geom_smooth(method=\"lm\", \n                se=FALSE, \n                color=\"black\") + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThese plots give us some idea of the correlation, but what if we want actual numbers? We can’t do this in “plain” ggplot2, but now we can take advantage of the enormous number of ggplot2 extension packages. It turns out that the ggpmisc package supports what we need, so let’s download and install it:\n\nif(!require(\"ggpmisc\")) install.packages(\"ggpmisc\")\n\nLoading required package: ggpmisc\n\n\nLoading required package: ggpp\n\n\nRegistered S3 methods overwritten by 'ggpp':\n  method                  from   \n  heightDetails.titleGrob ggplot2\n  widthDetails.titleGrob  ggplot2\n\n\n\nAttaching package: 'ggpp'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(ggpmisc)\n\nNow we have access to the various geom_, scale_, etc objects from that package. We can now introduce a new category, stat_, that represents statistical transformations or modeling. Generally, these are applied “automagically” for us, as in geom_smooth, but here we need to build our regression models explicitly:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSee how, even though we’re using functionality from outside ggplot2, the structure of the “grammar” makes it easy for all these tools to work well together.\n\n\nFinal Polish\nWe are almost done, but every figure needs a bit of final polish. Firstly, we should add a title, using the ggtitle function. (R has a built-in title function but that won’t help us here)\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn good academic practice, we should always add a footnote citing the source of our data. The palmerpenguins site has appropriate source information:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\") + \n    labs(caption=\"Data provided by Dr. K. Gorman and the Palmer Station, Antarctica LTER\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAt this point, the bottom of our figure looks a bit crowded. To clear out some space, let’s remove the legend from the bottom, since it simply repeats the facet labels:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\") + \n    labs(caption=\"Data provided by Dr. K. Gorman and the Palmer Station, Antarctica LTER\") + \n    guides(color=\"none\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nCleaner and no loss of information. In designing good scientific graphics, the concept of a “ink-to-information” ratio is useful: if you can remove some ink without removing any (relevant) information, you should generally do so. This makes it easier for the reader to identify the important elements of the plot.\nTo make our point even clearer, it is sometimes useful to add a short “summary” to a plot like this:\n\nggplot(penguins, \n       aes(x=flipper_length_mm, \n           y=body_mass_g, \n           color=species)) + \n    geom_point() + \n    stat_poly_line(se=FALSE, \n                   color=\"black\") +\n    stat_poly_eq() + \n    xlab(\"Flipper Length (mm)\") + \n    ylab(\"Body Mass (g)\") + \n    theme_bw() + \n    scale_color_brewer(type=\"qual\", \n                       palette=2, \n                       name=\"Species\") + \n    theme(legend.position=\"bottom\") + \n    facet_wrap(~species) + \n    ggtitle(\"Correlation of Flipper Length and Body Mass across Penguin Species\", \n            subtitle=\"Flipper Length and Body Mass are positively correlated across species\\nGentoo penguins exhibit the strongest relationship at 70% correlation\") + \n    labs(caption=\"Data provided by Dr. K. Gorman and the Palmer Station, Antarctica LTER\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_line()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_poly_eq()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis is not always a good idea - it requires “hard-coding” the insights for your reader and takes up some space. In scientific writing, I generally prefer to put this sort of summary in a figure caption, while I tend to “say the point” verbally if the figure is destined for a presentation.\nPersonally, I only use this sort of “here is the point” text if I expect a figure to “escape beyond” my presentation and need it to stand fully on its own.\n\n\nConclusions\nggplot2 provides an exceptionally powerful and flexible set of tools for creating statistical visualizations. We will explore it in more depth in class. For now, review the examples above and make sure you see how each plot is created “piecewise” from its various components.\nTo see more about what ggplot2 can do, check out the R Graphics Gallery. If you want to see the specifics of each ggplot2 function, check out the package reference page. To go further with ggplot2, you should also explore its extension gallery."
  },
  {
    "objectID": "preassigns/pa07.html#footnotes",
    "href": "preassigns/pa07.html#footnotes",
    "title": "STA/OPR 9750 Week 7 Pre Assignment: Lots of Plots",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI once had a boss who refused to use the terms \\(x\\) and \\(y\\) axis unless quantities called \\(x\\) and \\(y\\) were actually being plotted. Being Cantabrigian, he insisted I use the terms abscissa and ordinate. I will not inflict such pedantry in this course.↩︎"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "STA/OPR 9750 - In-Class Labs",
    "section": "",
    "text": "Most weeks, the Thursday ‘lecture’ for STA/OPR 9750 will be dedicated to an in-class “lab”. These ungraded labs are an opportunity to see how the concepts introduced in that week’s pre-assignment are used in practice.\nLabs:\n\nLab #01: Rev your Engines! Setting-Up R and RStudio\nLab #02: Getting Down with Markdown\nLab #03: R, These are your first steps…\nLab #04: Single Table Verbs, Group-Aware Filtering\nLab #05: Let us Join our Tables Together\nLab #07: More Thoughts on Plots\nLab #08: TBA\nLab #09: TBA\nLab #11: TBA\nLab #12: TBA\nLab #13: TBA"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA/OPR 9750 - Basic Software Tools for Data Analysis",
    "section": "",
    "text": "Welcome to the course website for STA/OPR 9750 (Fall 2024)!\nSTA/OPR 9750 is an Introduction to R targeted at students in the MS in Business Analytics, MS in Statistics, and MS in Quantitative Methods programs. Though listed as a double course, STA 9750 and OPR 9750 will be taught and graded jointly: students are encouraged to collaborate with classmates in either section.\nThis site hosts the unofficial Course Syllabus, Course Policies, and Course Learning Objectives. Official copies of these documents can be found on CUNY Brightspace. Course pre-assignments, labs, and mini-projects can also be found on this site.\nThis year, STA/OPR 9750 will be taught in a mixture of the flipped-classroom and experiential-learning formats. Roughly, this means that most weeks, students will be asked to complete a small pre-assignment each week to introduce the core concept(s) covered in that week’s lecture. Each class period will be split between a brief lecture covering concepts in more detail and an extended lab activity designed to build familiarity and fluency with that week’s subject matter.\nThere are quite a few moving parts to this course, so this key dates file or the list of upcoming course activities below may be useful:\n\n\n\n\n\n\n\nA CSV file suitable for import into Google Calendar with all assignment deadlines can be found here.\nInstructor: Michael Weylandt"
  },
  {
    "objectID": "slides/slides03.html#mini-project-00",
    "href": "slides/slides03.html#mini-project-00",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "STA/OPR 9750 Mini-Project #00",
    "text": "STA/OPR 9750 Mini-Project #00\n\nMP#00 submitted\n\nA few of you didn’t submit; I’ll follow up directly for VoE\n\nMP#00 peer feedback assignments released (check GitHub)\n\nGive some feedback to your peers\nGet ideas for improving your own site"
  },
  {
    "objectID": "slides/slides03.html#mini-project-01",
    "href": "slides/slides03.html#mini-project-01",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "STA/OPR 9750 Mini-Project #01",
    "text": "STA/OPR 9750 Mini-Project #01\n\nMP#01 released\nStart early\n\nNot too hard if everything is working (post-MP#00)\nTech support takes time"
  },
  {
    "objectID": "slides/slides03.html#graduate-teaching-assistant-gta",
    "href": "slides/slides03.html#graduate-teaching-assistant-gta",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "Graduate Teaching Assistant (GTA)",
    "text": "Graduate Teaching Assistant (GTA)\n\nCharles Ramirez\nTwice Weekly Office Hours (Zoom)\n\nTuesdays 4-5pm\nFridays 12-1pm\n\nWill also help coordinate peer feedback (GitHub), Piazza responses, etc.\nExcellent resource for course project advice!"
  },
  {
    "objectID": "slides/slides03.html#piazza-participation",
    "href": "slides/slides03.html#piazza-participation",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "Piazza Participation",
    "text": "Piazza Participation\n\nAverage time to response &lt;9 hours\n209 posts\n\nThanks to those of you who are helping classmates!"
  },
  {
    "objectID": "slides/slides03.html#course-project",
    "href": "slides/slides03.html#course-project",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "Course Project",
    "text": "Course Project\n\n1 team already registered with me!\nPiazza discussions helping to coordinate other teams"
  },
  {
    "objectID": "slides/slides03.html#upcoming-week",
    "href": "slides/slides03.html#upcoming-week",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nNext Wednesday at 11:45pm:\n\nNext Pre-Assignment\nMP#00 Peer Feedback due"
  },
  {
    "objectID": "slides/slides03.html#faq-vector-index-printout-rules",
    "href": "slides/slides03.html#faq-vector-index-printout-rules",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "FAQ: Vector Index Printout Rules",
    "text": "FAQ: Vector Index Printout Rules\nDefault vector printing:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nEach line gets a new index:\n\nsqrt(1:10)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\n\nMore complex objects have alternate print styles:\n\nmatrix(1:9, nrow=3, ncol=3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nPrint width is controlled by getOption(\"width\")."
  },
  {
    "objectID": "slides/slides03.html#faq-recycling-rules",
    "href": "slides/slides03.html#faq-recycling-rules",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "FAQ: Recycling Rules",
    "text": "FAQ: Recycling Rules\nAlignment by default:\n\nx &lt;- 1:3\ny &lt;- 4:6\nx + y\n\n[1] 5 7 9\n\n\nRecycling by default:\n\nx &lt;- 1\ny &lt;- 4:6\nx + y\n\n[1] 5 6 7\n\n\nRecycle warning when vectors don’t fit together cleanly:\n\nx &lt;- 1:2\ny &lt;- 4:6\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 5 7 7"
  },
  {
    "objectID": "slides/slides03.html#faq-recycling-warning",
    "href": "slides/slides03.html#faq-recycling-warning",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "FAQ: Recycling Warning",
    "text": "FAQ: Recycling Warning\n\nx &lt;- 1:2\ny &lt;- 4:6\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 5 7 7\n\n\nNot a problem per se, but often a sign that something has gone wrong.\n\nscalar + vector is usually safe\n2 vectors of same size is usually safe\nvectors of different size is usually a programming mistake"
  },
  {
    "objectID": "slides/slides03.html#faq-warnings-vs-errors",
    "href": "slides/slides03.html#faq-warnings-vs-errors",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "FAQ: Warnings vs Errors",
    "text": "FAQ: Warnings vs Errors\n\nWarnings: heuristics pointing at typical problem\n\nCode still executed without a problem\nTry to fix these unless you’re certain it’s not a problem\n\nErrors: code failed to execute\n\nYou have to fix these to run your code"
  },
  {
    "objectID": "slides/slides03.html#faq-changing-built-in-functions",
    "href": "slides/slides03.html#faq-changing-built-in-functions",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "FAQ: Changing built-in functions",
    "text": "FAQ: Changing built-in functions\nMost built-in functions can’t / shouldn’t be changed.\nSome allow alternate behavior via additional arguments:\n\nlog(10) # Default is natural (base e) logarithm\n\n[1] 2.302585\n\nlog(10, base=10)\n\n[1] 1\n\n\nIf you want different behavior, write your own function:\n\ncosd &lt;- function(x){\n    ## Cosine in degrees\n    cos(x * pi / 180)\n}\ncosd(90)\n\n[1] 6.123234e-17\n\n\nAlways try ?name to see documentation."
  },
  {
    "objectID": "slides/slides03.html#faq-git-workflow",
    "href": "slides/slides03.html#faq-git-workflow",
    "title": "STA/OPR 9750 - Week 3 Update",
    "section": "FAQ: Git Workflow",
    "text": "FAQ: Git Workflow\nThree key commands:\n\ngit add: add some changes to a ‘box’\ngit commit: seal the ‘box’\ngit push: send the ‘box’ to GitHub\n\nGit pane in RStudio shows uncommited changes, not files.\nIf a file ‘vanishes’ after a commit, that’s good!"
  },
  {
    "objectID": "slides/slides05.html#mini-project-01",
    "href": "slides/slides05.html#mini-project-01",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "STA/OPR 9750 Mini-Project #01",
    "text": "STA/OPR 9750 Mini-Project #01\nSubmission due yesterday at 11:45pm\n\n\\(\\approx 90\\%\\) submitted on time\nSubmit early and submit often\n\nLess “last minute” tech support going forward\n\nUse Piazza and use your peers\n\nVery impressed by Detailed Analyses, Code Folding and Callout Blocks, Fancy gt Tables, Graphics"
  },
  {
    "objectID": "slides/slides05.html#mini-project-01---peer-feedback",
    "href": "slides/slides05.html#mini-project-01---peer-feedback",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "STA/OPR 9750 Mini-Project #01 - Peer Feedback",
    "text": "STA/OPR 9750 Mini-Project #01 - Peer Feedback\nPeer feedback assigned on GitHub this morning\n\n\\(\\approx 4\\) feedbacks each\nTake this seriously: around 20% of this assignment is “meta-review”\nGoal: rigorous constructive critique\n\n\nSubmissions may not map perfectly to rubric - use your best judgement\n\n\nLearn from this! What can you adapt for MP#02?"
  },
  {
    "objectID": "slides/slides05.html#mini-project-02",
    "href": "slides/slides05.html#mini-project-02",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "STA/OPR 9750 Mini-Project #02",
    "text": "STA/OPR 9750 Mini-Project #02\nMP#02 released - Hollywood Movies\n\nDue October 23rd\n\nGitHub post (used for peer feedback) AND Brightspace\nOne Month: don’t wait until the very end\n\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness, not necessarily correctness"
  },
  {
    "objectID": "slides/slides05.html#upcoming-mini-projects",
    "href": "slides/slides05.html#upcoming-mini-projects",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#03: Political Analysis\nMP#04: Retirement Forecasting"
  },
  {
    "objectID": "slides/slides05.html#pre-assignments",
    "href": "slides/slides05.html#pre-assignments",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is October 16th"
  },
  {
    "objectID": "slides/slides05.html#course-project",
    "href": "slides/slides05.html#course-project",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "Course Project",
    "text": "Course Project\n6 teams already formed!\n\nBreakout rooms in teams\n\nTeam/Room 1: GZ + VF + EY + AG + TD\nTeam/Room 2: YZ + HM + TN + NG\nTeam/Room 3: SK + HA + DS\nTeam/Room 4: AC + EL + CL + WP\nTeam/Room 5: CC + AO + HS + MT + DM\nTeam/Room 6: SK + CM + MK + JV (pending confirmation)\n\nAll team commitments due via email 2024-10-02"
  },
  {
    "objectID": "slides/slides05.html#course-support",
    "href": "slides/slides05.html#course-support",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nAsynchronous\n\nPiazza (&lt;40 minute average response time)"
  },
  {
    "objectID": "slides/slides05.html#upcoming-week",
    "href": "slides/slides05.html#upcoming-week",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nNext Wednesday at 11:45pm:\n\nMP#01 peer feedback due\nTeam membership due\n\nNo class on October 3rd"
  },
  {
    "objectID": "slides/slides05.html#october-10---project-proposal-presentations",
    "href": "slides/slides05.html#october-10---project-proposal-presentations",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "October 10 - Project Proposal Presentations",
    "text": "October 10 - Project Proposal Presentations\nOfficial Description\n\n6 minute presentation\nKey topics:\n\nAnimating Question\nTeam Roster\n\nAlso discuss: Possible specific questions, data sources, analytical plan, anticipated challenges\n\nPeer feedback mechanism TBD"
  },
  {
    "objectID": "slides/slides05.html#faq-subqueries",
    "href": "slides/slides05.html#faq-subqueries",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: Subqueries",
    "text": "FAQ: Subqueries\n\n[W]ill we be learning how to perform joins within a subquery?\n\nYou don’t need subqueries in R since it’s an imperative language. Just create a new variable to represent the result of the subquery and use that in the next command.\nSELECT first_name, last_name\nFROM collectors\nWHERE id IN (\n    SELECT collector_id\n    FROM sales\n);\ncollector_ids &lt;- sales |&gt; pull(collector_id)\ncollectors |&gt; filter(id %in% collector_ids) |&gt; select(first_name, last_name)"
  },
  {
    "objectID": "slides/slides05.html#faq-data-integrity",
    "href": "slides/slides05.html#faq-data-integrity",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: Data Integrity",
    "text": "FAQ: Data Integrity\n\n[H]ow can we ensure that the information [resulting from a join] is accurate and not repeated?\n\n\nIf you have a true unique ID, you’re usually safe\nPay attention to all warnings\nManually examine the result of any joins"
  },
  {
    "objectID": "slides/slides05.html#faq-performance-on-large-data-sets",
    "href": "slides/slides05.html#faq-performance-on-large-data-sets",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: Performance on Large Data Sets",
    "text": "FAQ: Performance on Large Data Sets\n\nWill joining large data sets […] affect performance?\n\nSomewhat - larger data sets are always slower.\nBigger danger is “bad joins” creating huge data automatically.\nNote that R is less “smart” than SQL, so won’t optimize execution order for you automatically."
  },
  {
    "objectID": "slides/slides05.html#faq-what-is-the-role-of-pivot_wider",
    "href": "slides/slides05.html#faq-what-is-the-role-of-pivot_wider",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: What is the Role of pivot_wider?",
    "text": "FAQ: What is the Role of pivot_wider?\n\nIs [pivot_wider] just for formatting?\n\n\nlibrary(dplyr); library(tidyr); library(palmerpenguins)\npenguins |&gt; drop_na() |&gt; \n    group_by(sex, species) |&gt; \n    summarize(weight = mean(body_mass_g)) |&gt;\n    pivot_wider(id_cols=species, \n                names_from=sex,\n                values_from=weight) |&gt;\n    mutate(gender_diff = male - female)\n\n# A tibble: 3 × 4\n  species   female  male gender_diff\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie     3369. 4043.        675.\n2 Chinstrap  3527. 3939.        412.\n3 Gentoo     4680. 5485.        805."
  },
  {
    "objectID": "slides/slides05.html#faq-dplyr-joins-vs-sql-joins",
    "href": "slides/slides05.html#faq-dplyr-joins-vs-sql-joins",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: dplyr joins vs SQL joins",
    "text": "FAQ: dplyr joins vs SQL joins\n\nWhat is the difference between dplyr and SQL joins?\n\nNot too much - biggest difference is no INDEX or FOREIGN KEY in R so less guarantees of data integrity."
  },
  {
    "objectID": "slides/slides05.html#faq-when-to-use-anti_join",
    "href": "slides/slides05.html#faq-when-to-use-anti_join",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: When to use anti_join?",
    "text": "FAQ: When to use anti_join?\nRare: looking for unmatched rows. - Useful to find data integrity issues or ‘implicit’ missingness. - I use an anti_join to find students who haven’t submitted an assignment.\nsemi_join appears in MP #02."
  },
  {
    "objectID": "slides/slides05.html#faq-many-to-many-warning",
    "href": "slides/slides05.html#faq-many-to-many-warning",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: many-to-many Warning",
    "text": "FAQ: many-to-many Warning\nTricky to address, but fortunately pretty rare.\n\nSQL explicitly forbids many-to-many\nUsually a sign that a “key” isn’t really unique\n\nCheck for duplicates in x and y tables\nCan occur with “fancy” joins (rolling, inequality)\n\nAdd additional join variables to break “duplication”"
  },
  {
    "objectID": "slides/slides05.html#faq-how-to-check-efficiency",
    "href": "slides/slides05.html#faq-how-to-check-efficiency",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: How to Check Efficiency?",
    "text": "FAQ: How to Check Efficiency?\nNo automatic way. Some rules of thumb:\n\nDon’t create large tables just to filter down\n\nfilter before join when possible\n\nfull_outer join is a bit dangerous\ncross_join is rarely the right answer"
  },
  {
    "objectID": "slides/slides05.html#faq-tidyr-vs-dplyr",
    "href": "slides/slides05.html#faq-tidyr-vs-dplyr",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: tidyr vs dplyr",
    "text": "FAQ: tidyr vs dplyr\n\nIs tidyr more efficient than dplyr?\n\nNope - different packages from the same developers.\nDesigned to work together elegantly."
  },
  {
    "objectID": "slides/slides05.html#faq-rare-joins",
    "href": "slides/slides05.html#faq-rare-joins",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: Rare Joins",
    "text": "FAQ: Rare Joins\n\nPlease explain what cross_join, filter joins, and nest_join are?\n\n\ncross_join: dangerous.\n\nCreates “all pairs” of rows. Useful for ‘design’ problems\n\nfilter joins (anti_, semi_):\n\nHunting down quietly missing data.\nFiltering to sub-samples (see MP#02)\n\nnest_join: beyond this course.\n\nleft_join with extra structure to output."
  },
  {
    "objectID": "slides/slides05.html#faq-how-to-pick-a-join",
    "href": "slides/slides05.html#faq-how-to-pick-a-join",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "FAQ: How to Pick a Join",
    "text": "FAQ: How to Pick a Join\n\nHow do I decide which type of join is most approriate for a given analysis?\n\nTopic of today’s work."
  },
  {
    "objectID": "slides/slides05.html#other-tips",
    "href": "slides/slides05.html#other-tips",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "Other Tips",
    "text": "Other Tips\n\nDisable RStudio’s visual Quarto editor. It’s more trouble than it’s worth. To stop it from opening by default, add editor: source in the header of your qmd files.\nQuarto depends on file structure for organizing content. The main directory (STA9750-2024-FALL) should hold all of your input files. You should never directly put anything in the docs/ folder. That’s where generated output should live.\nWhen I leave &lt;GITHUB_NAME&gt; or similar in instructions, put in your GitHub ID. (And make sure to remove the &lt; and &gt; symbols)"
  },
  {
    "objectID": "slides/slides05.html#diving-deeper-with-dplyr---joins-and-pivots",
    "href": "slides/slides05.html#diving-deeper-with-dplyr---joins-and-pivots",
    "title": "STA/OPR 9750 - Week 5 Update",
    "section": "Diving Deeper with dplyr - Joins and Pivots",
    "text": "Diving Deeper with dplyr - Joins and Pivots\nData Set: nycflights13\nExercise: Lab #05"
  },
  {
    "objectID": "slides/slides06.html#mini-project-01",
    "href": "slides/slides06.html#mini-project-01",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "STA/OPR 9750 Mini-Project #01",
    "text": "STA/OPR 9750 Mini-Project #01\n👏 Thank you to everyone who took part in peer feedback! 👏\nCharles and I have marked all grades - expect on Brightspace soon.\n(Tech issues on our end…)\n\nAlso - let’s thank Charles for going through every post manually with reminders 👏"
  },
  {
    "objectID": "slides/slides06.html#mini-project-01-1",
    "href": "slides/slides06.html#mini-project-01-1",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "STA/OPR 9750 Mini-Project #01",
    "text": "STA/OPR 9750 Mini-Project #01\nSome popular tips:\n\nThe gt makes very nice tables\nQuarto allows “code folding”: useful for hiding long boring code blocks\n\n\n\nShow the code\n1 + 1\n\n\n[1] 2\n\n\nYou can set this globally. You also want to keep echo: true."
  },
  {
    "objectID": "slides/slides06.html#mini-project-02",
    "href": "slides/slides06.html#mini-project-02",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "STA/OPR 9750 Mini-Project #02",
    "text": "STA/OPR 9750 Mini-Project #02\nMP#02 released - Hollywood Movies\n\nDue October 23rd\n\nGitHub post (used for peer feedback) AND Brightspace\n\n\n\nPay attention to the rubric\n\nWriting and presentation are about 50% of your grade\nEvaluated on rigor and thoughtfulness\nUse what you learned from MP#01\nPre-processed data now available as well"
  },
  {
    "objectID": "slides/slides06.html#upcoming-mini-projects",
    "href": "slides/slides06.html#upcoming-mini-projects",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "Upcoming Mini-Projects",
    "text": "Upcoming Mini-Projects\nTentative Topics\n\nMP#03: Political Analysis\nMP#04: Retirement Forecasting"
  },
  {
    "objectID": "slides/slides06.html#pre-assignments",
    "href": "slides/slides06.html#pre-assignments",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "Pre-Assignments",
    "text": "Pre-Assignments\nBrightspace - Wednesdays at 11:45\n\nReading, typically on course website\nBrightspace auto-grades\n\nI have to manually change to completion grading\n\n\nNext pre-assignment is October 16th"
  },
  {
    "objectID": "slides/slides06.html#course-support",
    "href": "slides/slides06.html#course-support",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "Course Support",
    "text": "Course Support\n\nSynchronous\n\nOffice Hours 4x / week\n\nMW Office Hours on Tuesday next week only\n\n\nAsynchronous\n\nPiazza (\\(&lt;30\\) minute average response time)"
  },
  {
    "objectID": "slides/slides06.html#upcoming-week",
    "href": "slides/slides06.html#upcoming-week",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "Upcoming Week",
    "text": "Upcoming Week\nDue Wednesday at 11:45pm:\n\nPre-Assignment #07 (Brightspace)\n\nIntroduction to plotting with ggplot2\n\nProject Proposal Peer Feedback (Vocat)\n\nExpect back:\n\nMP#01 consolidated grades\nProject Proposal instructor feedback"
  },
  {
    "objectID": "slides/slides06.html#course-project-presentations",
    "href": "slides/slides06.html#course-project-presentations",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "Course Project Presentations",
    "text": "Course Project Presentations\n\n\n\n\n\n\n\n\n\n\nTeam\nMembers\n\nTeam\nMembers\n\n\n\n\n1\nTD + VF + AG + EY + GZ\n\n6\nJA + CPV\n\n\n2\nNG + HM + TN + ZL(C)Y + YZ\n\n7\nCD + JCMB + PR + CKZF\n\n\n3\nAC + EL + CL + WP\n\n8\nHH(C)L + EO + GS + FW\n\n\n4\nHA + HA + SK + VL + DS\n\n9\nAA + LC + MAJ\n\n\n5\nCC + DM + AO + HS + MT\n\n10\nHB + SK + MK + CM + JV"
  },
  {
    "objectID": "slides/slides06.html#project-proposal-presentations",
    "href": "slides/slides06.html#project-proposal-presentations",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "Project Proposal Presentations",
    "text": "Project Proposal Presentations\nOfficial Description\n\n6 minute presentation\nKey topics:\n\nAnimating Question\nTeam Roster\n\nAlso discuss: Possible specific questions, data sources, analytical plan, anticipated challenges"
  },
  {
    "objectID": "slides/slides06.html#proposal-peer-feedback",
    "href": "slides/slides06.html#proposal-peer-feedback",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "Proposal Peer Feedback",
    "text": "Proposal Peer Feedback\nProject Peer Feedback Platform: Vocat\nAfter class:\n\nI upload presentation videos\nYou leave comments and scores on at least 1 presentation\n\nTry to comment on “under-discussed” presentations\nI will be looking for constructive feedback\nLess detail than mini-prorjects\n\n\nNew tool - learning experience for all. Bear with us."
  },
  {
    "objectID": "slides/slides06.html#after-proposals",
    "href": "slides/slides06.html#after-proposals",
    "title": "STA/OPR 9750 - Week 6 Update",
    "section": "After Proposals",
    "text": "After Proposals\nSpecial presentation on Baruch data resources"
  },
  {
    "objectID": "preassignments.html",
    "href": "preassignments.html",
    "title": "STA/OPR 9750 - Pre-Assignments",
    "section": "",
    "text": "In lieu of traditional homework, STA/OPR 9750 has weekly pre-assignments designed to achieve several interlocking goals:\n\nProvide initial exposure to that week’s topic before the lecture and lab session\nAllow students with less previous programming experience more time to familiarize themselves with that week’s topic\nAllow students to submit questions to be covered in class\n\nEach Pre-Assignment will be submitted via CUNY Brightspace and due the night before class (Wednesdays at 11:45). These are short assignments, typically only a few questions, so extensions will not be given outside of exceptional circumstances.\n\nPre-Assignments\n\nPre-Assignment for Week #01\nNone.\n\n\nPre-Assignment for Week #02\nDue Dates:\n\nReleased to Students: 2024-08-29\nDue on Brightspace: 2024-09-04 at 11:45pm ET\n\nIn this Pre-Assignment, you will familiarize yourself with the basics of Markdown, an easy way to write and format documents. In class, we will use Markdown based tools to create dynamic data analysis documents seamlessly combining code, text, and graphics.\n\n\nPre-Assignment for Week #03\nDue Dates:\n\nReleased to Students: 2024-09-05\nDue on Brightspace: 2024-09-11 at 11:45pm ET\n\nIn this week’s preassignment, you will familiarize yourself with some basic “calculator math” in R. You will also see how function calls work as we get ready to start some proper R programming.\n\n\nPre-Assignment for Week #04\nDue Dates:\n\nReleased to Students: 2024-09-12\nDue on Brightspace: 2024-09-18 at 11:45pm ET\n\nIn this week’s preassignment, you will review dplyr’s “single-table” verbs. These are functions that take a single data frame and do something, typically returning another data frame. We can divide these into three major groups:\n\nSubsetting rows (filter) and columns (select);\nChanging and creating columns (mutate and less commonly, rename);\noperating with group structure (group_by, summarize)\n\n\n\nPre-Assignment for Week #05\nDue Dates:\n\nReleased to Students: 2024-09-19\nDue on Brightspace: 2024-09-25 at 11:45pm ET\n\nIn this week’s preassignment, you will review dplyr’s most important “multi-table” verbs, the join operators. These are functions that take multiple data frames and combine them together. You will need to use this type of functionality to combine data from different sources together in a principled and organized fashion. You will also learn a bit about the pivot_longer and pivot_wider functions used to change the shape of data frames. These are particularly useful in conjunction with joins: you will often need to reshape two tables to “join” properly (typically, lengthening them with pivot_longer) and then reshape them for downstream presentation (typically with pivot_wider).\n\n\nPre-Assignment for Week #06\nNone.\nThe 2024-10-10 class session will be dedicated to Course Project Proposals.\n\n\nPre-Assignment for Week #07\nDue Dates:\n\nReleased to Students: 2024-10-10\nDue on Brightspace: 2024-10-16 at 11:45pm ET\n\nIn this week’s preassignment, we begin to explore the wonderful world of statistical graphics.\n\n\nPre-Assignment for Week #08\nDue Dates:\n\nReleased to Students: 2024-10-17\nDue on Brightspace: 2024-10-23 at 11:45pm ET\n\nIn this week’s preassignment, we dive deeper into the world of statistical graphics, watching statistical graphics in the hands of a master.\n\n\nPre-Assignment for Week #09\nDue Dates:\n\nReleased to Students: 2024-10-24\nDue on Brightspace: 2024-10-30 at 11:45pm ET TBA\n\n\n\nPre-Assignment for Week #10\nNone.\nThe 2024-11-07 class session will be dedicated to Course Project Mid-Semester Check-Ins.\n\n\nPre-Assignment for Week #11\nDue Dates:\n\nReleased to Students: 2024-11-07\nDue on Brightspace: 2024-11-13 at 11:45pm ET TBA\n\n\n\nPre-Assignment for Week #12\nDue Dates:\n\nReleased to Students: 2024-11-14\nDue on Brightspace: 2024-11-20 at 11:45pm ET\n\nTBA\n\n\nPre-Assignment for Week #13\nDue Dates:\n\nReleased to Students: 2024-11-21\nDue on Brightspace: 2024-12-04 at 11:45pm ET\n\nTBA\n\n\nPre-Assignment for Week #14\nNone.\nThe 2024-12-12 class session will be dedicated to Course Project Final Presentations."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "STA/OPR 9750 - Final Project",
    "section": "",
    "text": "In lieu of exams, STA/OPR 9750 has an end-of-semester project, worth 40% of your final grade. This project is intended to showcase the data analysis technologies covered in this course, including - but not limited to:\nThe project will be graded out of a total 400 points, divided as follows:\nProjects can be completed in groups of 3-5 students.1 All group members are responsible for all portions of the work and will receive the same grade, except on the individual evaluation.\nGroup Membership: By 2024-10-02 at 11:45pm ET, email the instructor with a list of group members, cc-ing all group members. Once the original email is sent, other group members must reply acknowledging their intention to work with this group. After this date, group membership may only be changed for extraordinary circumstances."
  },
  {
    "objectID": "project.html#footnotes",
    "href": "project.html#footnotes",
    "title": "STA/OPR 9750 - Final Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf desired, students can work in pairs or even individually. That “team” is still responsible for a minimum of three specific questions, so you will have to do extra work if you have a team of fewer than 3 people.↩︎\nMore properly, you would want to use Zip Code Tabulation Areas (ZCTAs) for this sort of analysis. The distinction is subtle, but while all ZCTAs have geographic extents, not all zip codes do. For example, there are dedicated zip codes for the IRS and the Department of Defense that have no associated geographic boundaries. Most open data sources will omit this distinction, but if you see it, you should be aware of it.↩︎\nIf students choose to take on multiple specific questions (perhaps because they were in a small group or if a classmate had to drop the course), they may submit multiple individual reports (one per question). If doing so, please modify the GitHub message template to link all reports.↩︎"
  },
  {
    "objectID": "preassigns/pa08.html",
    "href": "preassigns/pa08.html",
    "title": "STA/OPR 9750 Week 8 Pre Assignment: More Plots",
    "section": "",
    "text": "This week, we will dive deeper into the world of ggplot2, with a focus on spatial data visualization and interactive graphics. Before we do so, let’s pause and consolidate everything we’ve done to date.\n\nIf you did not finish last week’s in-class lab, do so now.\nWatch Prof. Di Cook’s lecture “Myth busting and\napophenia in data visualisation: is what you see really there?”. As we discussed in class, plots are an excellent way to explore data, but we always want to be careful that what we think find truly exists. Prof. Cook discusses relationships between effective statistical visualization and effective statistical practice.\n\nAfter finishing Prof. Cook’s lecture, explore the R Graphics Gallery “Best Charts” collection. Pick one chart from this collection and evaluate it with a critical eye:\n\nIs it well styled?\nWhat story is it trying to tell?\nDoes it tell that story effectively?\nDo you believe that story?\nHow could it tell the story more effectively?\n\n\nAfter finishing these, complete the Weekly Pre-Assignment Quiz on Brightspace."
  },
  {
    "objectID": "miniprojects/mini03.html#introduction",
    "href": "miniprojects/mini03.html#introduction",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Introduction",
    "text": "Introduction\nWelcome to Mini-Project #03! In this project, you will write a political fact-check, that most iconic form of our current journalistic era. Specifically, you will investigate the claim that the US Electoral College systematically biases election results away from the vox populi. As you dive in to the world of political data, we’ll also learn a bit more about the mechanics of US federal elections.\nIn this Mini-Project, you will:\n\nIntegrate data from disparate governmental and academic sources\nLearn to work with spatial data formats\nCreate many plots\nUse spatial and animated visualizations to make your argument\n\nNote that - as with all these mini-projects - there isn’t a single “right” answer to the questions posed herein. You may have different views about the relative importance of federalism, direct democratic structures, adherence to the formal structures of the US Constitution, etc. than your classmates. Please make sure to make your argument respectfully and, when we reach the peer-evaluation stage, read and comment respectfully. All grading will be done solely on the quality of the code, the writing, the visualizations, and the argument - not on the political implications of what you may or may not find.\nAlso note that this mini-project is intended to be markedly less demanding than Mini-Project #02. At this point in the course, you should be diving into your Course Project, which should consume the majority of your out-of-class time dedicated to this course for the remainder of the semester."
  },
  {
    "objectID": "miniprojects/mini03.html#background",
    "href": "miniprojects/mini03.html#background",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Background",
    "text": "Background\nThe US Constitution sets the basic rules of electing the President in Section 1 of Article II, which we quote here in part:\n\nEach State shall appoint, in such Manner as the Legislature thereof may direct, a Number of Electors, equal to the whole Number of Senators and Representatives to which the State may be entitled in the Congress: but no Senator or Representative, or Person holding an Office of Trust or Profit under the United States, shall be appointed an Elector.\nThe Electors shall meet in their respective States, and vote by Ballot for two Persons, of whom one at least shall not be an Inhabitant of the same State with themselves. And they shall make a List of all the Persons voted for, and of the Number of Votes for each; which List they shall sign and certify, and transmit sealed to the Seat of the Government of the United States, directed to the President of the Senate. The President of the Senate shall, in the Presence of the Senate and House of Representatives, open all the Certificates, and the Votes shall then be counted. The Person having the greatest Number of Votes shall be the President, if such Number be a Majority of the whole Number of Electors appointed; and if there be more than one who have such Majority, and have an equal Number of Votes, then the House of Representatives shall immediately chuse by Ballot one of them for President; and if no Person have a Majority, then from the five highest on the List the said House shall in like Manner chuse the President. But in chusing the President, the Votes shall be taken by States, the Representation from each State having one Vote; A quorum for this Purpose shall consist of a Member or Members from two thirds of the States, and a Majority of all the States shall be necessary to a Choice. In every Case, after the Choice of the President, the Person having the greatest Number of Votes of the Electors shall be the Vice President. But if there should remain two or more who have equal Votes, the Senate shall chuse from them by Ballot the Vice President.\n\nThough the details have varied over time due to amendment, statue, and technology, this basic outline of this allocation scheme remains unchanged:\n\nEach state gets \\(R + 2\\) electoral college votes, where \\(R\\) is the number of Representatives that state has in the US House of Representatives\nStates can allocate those votes however they wish\nThe president is the candidate who receives a majority of electoral college votes\n\nNotably, the Constitution sets essentially no rules on how the \\(R + 2\\) electoral college votes (ECVs) for a particular state are allocated. At different points in history, different states have elected to use each of the following:\n\nDirect allocation of ECVs by state legislature (no vote)\nAllocation of all ECVs to winner of state-wide popular vote\nAllocation of all ECVs to winner of nation-wide popular vote\n\nAllocation of \\(R\\) ECVs to popular vote winner by congressional district + allocation of remaining \\(2\\) ECVs to the state-wide popular vote winner\n\nCurrently, only Maine and Nebraska use the final option; the other 48 states and the District of Columbia award all \\(R+2\\) ECVs to the winner of their state-wide popular vote. We emphasize here that “statewide winner-take-all” is a choice made by the individual states, not dictated by the US constitution, and that states have the power to change it should they wish.1\nTo my knowledge, no US state uses true proportionate state-wide representation, though I believe such a ECV-allocation scheme would be consistent with the US Constitution. For example, if a state with 5 ECVs had 60,000 votes for Candidate A and 40,000 cast for Candidate B, it could award 3 ECVs to A and 2 to B, regardless of the spatial distribution of those votes within the state."
  },
  {
    "objectID": "miniprojects/mini03.html#student-responsbilities",
    "href": "miniprojects/mini03.html#student-responsbilities",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Student Responsbilities",
    "text": "Student Responsbilities\nRecall our basic analytic workflow and table of student responsibilities:\n\nData Ingest and Cleaning: Given a single data source, read it into R and transform it to a reasonably useful standardized format.\nData Combination and Alignment: Combine multiple data sources to enable insights not possible from a single source.\nDescriptive Statistical Analysis: Take a data table and compute informative summary statistics from both the entire population and relevant subgroups\nData Visualization: Generate insightful data visualizations to spur insights not attainable from point statistics\nInferential Statistical Analysis and Modeling: Develop relevant predictive models and statistical analyses to generate insights about the underlying population and not simply the data at hand.\n\n\nStudents’ Responsibilities in Mini-Project Analyses\n\n\n\n\n\n\n\n\n\nIngest and Cleaning\nCombination and Alignment\nDescriptive Statistical Analysis\nVisualization\n\n\n\nMini-Project #01\n\n\n✓\n\n\n\nMini-Project #02\n\n✓\n✓\n½\n\n\nMini-Project #03\n½\n✓\n✓\n✓\n\n\nMini-Project #04\n✓\n✓\n✓\n✓\n\n\n\nIn this mini-project, you will be working with relatively “clean” electoral data and your main focus should be on the analysis and visualization supporting your fact check. As an analysis of political data, I expect your final submission to have quite a few “red state/blue state” maps.3 Data cleaning and import will play a larger role in Mini-Project #04.\nIn this project, I am no longer providing code to download and read the necessary data files. The data files I have selected for this mini-project are relatively easy to work with and should not provide a significant challenge, particularly after our in-class discussion of Data Import. See the modified rubric below which now includes a grade for data import.\nRubric\nSTA/OPR 9750 Mini-Projects are evaluated using peer grading with meta-review by the course GTAs. Specifically, variants of the following rubric will be used for the mini-projects:\n\nMini-Project Grading Rubric\n\n\n\n\n\n\n\n\n\n\nCourse Element\nExcellent (9-10)\nGreat (7-8)\nGood (5-6)\nAdequate (3-4)\nNeeds Improvement (1-2)\nExtra Credit\n\n\n\nWritten Communication\nReport is well-written and flows naturally. Motivation for key steps is clearly explained to reader without excessive detail. Key findings are highlighted and appropriately given context.\nReport has no grammatical or writing issues. Writing is accessible and flows naturally. Key findings are highlighted, but lack suitable motivation and context.\nReport has no grammatical or writing issues. Key findings are present but insufficiently highlighted.\nWriting is intelligible, but has some grammatical errors. Key findings are obscured.\nReport exhibits significant weakness in written communication. Key points are difficult to discern.\nReport includes extra context beyond instructor provided information.\n\n\nProject Skeleton\nCode completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\nCode completes all instructor-provided tasks satisfactorially.\nResponse to one instructor provided task is skipped, incorrect, or otherwise incomplete.\nResponses to two instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nResponse to three or ore instructor provided tasks are skipped, incorrect, or otherwise incomplete.\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses.\n\n\nFormatting & Display\n\nTables and figures are full ‘publication-quality’.\nReport includes at least one animated visualization designed to effectively communicate findings.\n\n\nTables have well-formatted column names, suitable numbers of digits, and attractive presentation.\nFigures are ‘publication-quality’, with suitable axis labels, well-chosen structure, attractive color schemes, titles, subtitles, and captions, etc.\n\n\nTables are well-formatted, but still have room for improvement.\nFigures are above ‘exploratory-quality’, but do not reach full ‘publication-quality’.\n\n\nTables lack significant ‘polish’ and need improvement in substance (filtering and down-selecting of presented data) or style.\nFigures are suitable to support claims made, but are ‘exploratory-quality’, reflecting minimal effort to customize and ‘polish’ beyond ggplot2 defaults.\n\n\nUnfiltered ‘data dump’ instead of curated table.\nBaseline figures that do not fully support claims made.\n\nReport includes interactive (not just animated) visual elements.\n\n\nCode Quality\n\nCode is (near) flawless.\nCode passes all styler and lintr type analyses without issue.\n\nComments give context of the analysis, not simply defining functions used in a particular line.\nCode has well-chosen variable names and basic comments.\nCode executes properly, but is difficult to read.\nCode fails to execute properly.\nCode takes advantage of advanced Quarto features to improve presentation of results.\n\n\nData Preparation\nData import is fully-automated and efficient, taking care to only download from web-sources if not available locally.\nData is imported and prepared effectively, in an automated fashion with minimal hard-coding of URLs and file paths.\nData is imported and prepared effectively, though source and destination file names are hard-coded.\nData is imported in a manner likely to have errors.\nData is hard-coded and not imported from an external source.\nReport uses additional data sources in a way that creates novel insights.\n\n\n\nNote that this rubric is designed with copious opportunities for extra credit if students go above and beyond the instructor-provided scaffolding. Students pursuing careers in data analytics are strongly encouraged to go beyond the strict ambit of the mini-projects to i) further refine their skills; ii) learn additional techniques that can be used in the final course project; and iii) develop a more impressive professional portfolio.\nBecause students are encouraged to use STA/OPR 9750 mini-projects as the basis for a professional portfolio, the basic skeleton of each project will be released under a fairly permissive usage license. Take advantage of it!\nSubmission Instructions\nAfter completing the analysis, write up your findings, showing all of your code, using a dynamic quarto document and post it to your course repository. The qmd file should be named mp03.qmd so the rendered document can be found at docs/mp03.html in the student’s repository and served at the URL:\n\nhttps://&lt;GITHUB_ID&gt;.github.io/STA9750-2024-FALL/mp03.html\n\nOnce you confirm this website works (substituting &lt;GITHUB_ID&gt; for the actual GitHub username provided to the professor in MP#00 of course), open a new issue at\n\nhttps://github.com/&lt;GITHUB_USERNAME&gt;/STA9750-2024-FALL/issues/new .\n\nTitle the issue STA/OPR 9750 &lt;GITHUB_USERNAME&gt; MiniProject #03 and fill in the following text for the issue:\nHi @michaelweylandt!\n\n\nhttps://&lt;GITHUB_USERNAME&gt;.github.io/STA9750-2024-FALL/mp03.html\nOnce the submission deadline passes, the instructor will tag classmates for peer feedback in this issue thread.\nAdditionally, a PDF export of this report should be submitted on Brightspace. To create a PDF from the uploaded report, simply use your browser’s ‘Print to PDF’ functionality.\nNB: The analysis outline below specifies key tasks you need to perform within your write up. Your peer evaluators will check that you complete these. You are encouraged to do extra analysis, but the bolded Tasks are mandatory.\nNB: Your final submission should look like a report, not simply a list of facts answering questions. Add introductions, conclusions, and your own commentary. You should be practicing both raw coding skills and written communication in all mini-projects. There is little value in data points stated without context or motivation."
  },
  {
    "objectID": "miniprojects/mini03.html#mini-project-03",
    "href": "miniprojects/mini03.html#mini-project-03",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Mini-Project #03",
    "text": "Mini-Project #03\nData I: US House Election Votes from 1976 to 2022\nThe [MIT Election Data Science Lab] collects votes from all biennial congressional races in all 50 states here. Download this data as a CSV file using your web browser. Note that you will need to provide your contact info and agree to cite this data set in your final report.4\nAdditionally, download statewide presidential vote counts from 1976 to 2022 here. As before, it will likely be easiest to download this data by hand using your web browser.\nData II: Congressional Boundary Files 1976 to 2012\nJeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis have created shapefiles for all US congressional districts from 1789 to 2012; they generously make these available here.\n\n\n\n\n\n\nTask 1: Download Congressional Shapefiles 1976-2012\n\n\n\nDownload congressional shapefiles from Lewis et al. for all US Congresses5 from 1976 to 2012.\nYour download code should:\n\nBe fully automated (no “hand-downloading”);\nDownload files with a systematic and interpretable naming convention\nOnly download files as needed out of courtesy for the data provider’s web sever. That is, if you already have a copy of the file, do not re-download it repeatedly.\n\nAs with the other Mini-Projects, make sure you do not store these data files in git. It will be sufficient to include the qmd file with the download code.\n\n\nNote that the shape files are distributed as zip folders, containing several files in a directory structure. We will be interested in the shp files within each zip.\nData III: Congressional Boundary Files 2014 to Present\nTo get district boundaries for more recent congressional elections, we can turn to the US Census Bureau. Unfortunately, these data - while authoritative and highly detailed - are not in quite the same format as our previous congressional boundary files. We can review the US Census Bureau shapefiles online. To download them automatically, I recommend exploring the FTP Archive link near the bottom of the page. In Census-jargon, the CD directory will have shapefiles for Congressional Districts for each year.6\n\n\n\n\n\n\nTask 2: Download Congressional Shapefiles 1976-2012\n\n\n\nDownload congressional shapefiles from the US Census Bureau for all US Congresses from 2014 to 2024.\nYour download code should:\n\nBe fully automated (no “hand-downloading”);\nDownload files with a systematic and interpretable naming convention\nOnly download files as needed out of courtesy for the data provider’s web sever. That is, if you already have a copy of the file, do not re-download it repeatedly.\n\nAs with the other Mini-Projects, make sure you do not store these data files in git. It will be sufficient to include the qmd file with the download code.\n\n\nInitial Exploration of Vote Count Data\n\n\n\n\n\n\nTask 3: Exploration of Vote Count Data\n\n\n\nAnswer the following using the vote count data files from the MIT Election Data Science Lab. You may answer each with a table or plot as you feel is appropriate.\n\nWhich states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\nNew York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent).\nAre there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\n\n\nDo presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state?\nDoes this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\n\n\n\nImporting and Plotting Shape File Data\nAs mentioned above, the shape files you downloaded above are distributed in zip archives, with several files. We only need the shp file within each archive. In this section, we’ll practice extracting the shp file, reading it, and using it to create a plot. The key library we need is the sf (“simple features”) library. It provides the read_sf() function which we can use to read it into R. I download how this works below:\n\nlibrary(ggplot2)\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.4.1\n\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n    download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n              destfile=\"nyc_borough_boundaries.zip\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_sf &lt;- read_sf(fname_shp)\nnyc_sf\n\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS84(DD)\n# A tibble: 5 × 5\n  boro_code boro_name      shape_area shape_leng                        geometry\n      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;              &lt;MULTIPOLYGON [°]&gt;\n1         3 Brooklyn      1934142776.    728147. (((-73.86327 40.58388, -73.863…\n2         5 Staten Island 1623618684.    325910. (((-74.05051 40.56642, -74.050…\n3         1 Manhattan      636646082.    360038. (((-74.01093 40.68449, -74.011…\n4         2 Bronx         1187174772.    463181. (((-73.89681 40.79581, -73.896…\n5         4 Queens        3041418004.    888197. (((-73.82645 40.59053, -73.826…\n\n\n\n\n\n\n\n\nTask 4: Automate Zip File Extraction\n\n\n\nAdapt the code after the ##- symbol above into a function read_shp_from_zip() which takes in a file name, pulls out the .shp file contained there in, and reads it into R using read_sf().\n\n\nThe result of this is a particular sort of data frame. The most important column for us is the geometry column which is of type MULTIPOLYGON. This is, essentially, a list of GPS coordinates which outline a spatial region. Here, each row corresponds to a Borough of NYC. We can pass the geometry column to ggplot2 to make a map:\n\nggplot(nyc_sf, \n       aes(geometry=geometry)) + \n    geom_sf()\n\n\n\n\n\n\n\nHere, we use the sf geom to get the shape outlines. The sf geom plays well with the fill aesthetic.\n\nggplot(nyc_sf, \n       aes(geometry=geometry, \n           fill = shape_area)) + \n    geom_sf()\n\n\n\n\n\n\n\nThis type of plot is called a Chloropleth Map and it is commonly used to depict election results.\n\n\n\n\n\n\nTask 5: Chloropleth Visualization of 2000 Electoral College Results\n\n\n\nUsing the data you downloaded earlier, create a chloropleth visualization of the 2000 electoral college results, coloring each state by the party that won the most votes in that state. Your result should look something like this:\n\nTaken from Wikipedia\nIt is not required, but to make the very best plot, you may want to look up:\n\nHow to “inset” Alaska and Hawaii instead of plotitng their true map locations.\nHow to add labels to a chloropleth in ggplot2\n\nHow to label the small states in the North-East\n\nbut these steps are not required as they are a bit advanced.\n\n\nYou can use the ggplotly function from the plotly package to make your chloropleth interactive:\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\np &lt;- ggplot(nyc_sf, \n       aes(geometry=geometry, \n           fill = shape_area, \n           label = boro_name)) + \n    geom_sf()\n\nggplotly(p)\n\n\n\n\n\n\nThis work ©2024 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "TODO.html",
    "href": "TODO.html",
    "title": "STA 9750",
    "section": "",
    "text": "Course improvements for next STA9750 offering\n\nMini Project #00\n\n\nAdd .gitignore\n\nExclude caching\nExlcude data files\n\nAdd minimal quarto\n\nEnsure students can run Quarto locally\n\nMore git setup\n\nIncrease buffer size to send large figures in MP02 and later\n\n\n\nDisable “Visual” editor mode in RStudio.\nMore Git workflow resources."
  },
  {
    "objectID": "miniprojects/mini03.html#footnotes",
    "href": "miniprojects/mini03.html#footnotes",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Footnotes",
    "text": "Footnotes\n\nI am not aware of “official” reasons from any state on why they select “winner-take-all” allocation. States clearly compete for attention in presidential elections and it seems reasonable to assume that competitive states select “winner-take-all” allocation to attract presidential candidates who will make promises to that state’s voters. By contrast, states whose legislature is dominated by a single party, e.g., New York, may be motivated to award all their votes to the more popular party in that state, denying any ECVs to the other candidate, even if a sizeable minority votes for them. If you find a history of how states select their ECV allocation strategies, I would be interested in reading it.↩︎\nMaking predictions about a counter-factual past.↩︎\nHistorically, the “Republicans Red / Democrats Blue” convention was not particularly strong in American journalism. It become standardized during coverage of the 2000 Presidential Election and subsequent Florida recount battles and has not materially changed since. For purposes of this mini-project, we will apply “Republican Red / Democrat Blue” consistently.↩︎\nWhile it may be possible to automate the browser to automatically fill in this pop-up as part of the download process, that’s beyond the scope of this assignment.↩︎\nIt may be useful to recall that each two year cycle is called “a congress” for district mapping purposes. The 2022 US Election, selecting Representatives to serve 2023-2025, corresponds to the 118th Congress. The upcoming (November 2024) election will select members for the 119th Congress.↩︎\nThe other shapefiles in this FTP archive may be useful for your final projects.↩︎\nThe District of Columbia is very Democratic.↩︎\nThis latter effect is admittedly quite small if we assume political affiliation is unrelated to probability of voting. The relationship between voting likelihood and political leanings is an important one for campaign strategists and actively debated by academics.↩︎"
  },
  {
    "objectID": "miniprojects/mini03.html#mini-project-objectives",
    "href": "miniprojects/mini03.html#mini-project-objectives",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Mini-Project Objectives",
    "text": "Mini-Project Objectives\nIn this project, you will use historical congressional election data to see how the outcome of US presidential elections would have changed under different allocation rules. Like any retrodiction2 task, this analysis has limitations. Notably, if the “rules” had been different, politicians may have run different campaigns and received different vote counts. Still, it is my hope that this is an interesting and informative exercise.\nAs noted above, your final submission should take the form of a “Fact Check”:\n\nTake a statement from a well-known politician or political commentator describing (claimed) bias of the electoral college system\nAnalyze presidential election results under different allocations for presence or abscence of bias (however you define it - see below)\nSummarize your retrodictive findings\nAward a “truthfulness” score to the claim you evaluated. (You may use the scale of an existing political fact-check operation or create your own.)"
  },
  {
    "objectID": "miniprojects/mini03.html#set-up-and-initial-exploration",
    "href": "miniprojects/mini03.html#set-up-and-initial-exploration",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Set-Up and Initial Exploration",
    "text": "Set-Up and Initial Exploration\nData I: US House Election Votes from 1976 to 2022\nThe MIT Election Data Science Lab collects votes from all biennial congressional races in all 50 states here. Download this data as a CSV file using your web browser. Note that you will need to provide your contact info and agree to cite this data set in your final report.4 Make sure to include this citation!\nAdditionally, download statewide presidential vote counts from 1976 to 2022 here. As before, it will likely be easiest to download this data by hand using your web browser.\nData II: Congressional Boundary Files 1976 to 2012\nJeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis have created shapefiles for all US congressional districts from 1789 to 2012; they generously make these available here.\n\n\n\n\n\n\nTask 1: Download Congressional Shapefiles 1976-2012\n\n\n\nDownload congressional shapefiles from Lewis et al. for all US Congresses5 from 1976 to 2012.\nYour download code should:\n\nBe fully automated (no “hand-downloading”);\nDownload files with a systematic and interpretable naming convention\nOnly download files as needed out of courtesy for the data provider’s web sever. That is, if you already have a copy of the file, do not re-download it repeatedly.\n\nAs with the other Mini-Projects, make sure you do not store these data files in git. It will be sufficient to include the qmd file with the download code.\n\n\nNote that the shape files are distributed as zip folders, containing several files in a directory structure. We will be interested in the shp files within each zip.\nData III: Congressional Boundary Files 2014 to Present\nTo get district boundaries for more recent congressional elections, we can turn to the US Census Bureau. Unfortunately, these data - while authoritative and highly detailed - are not in quite the same format as our previous congressional boundary files. We can review the US Census Bureau shapefiles online. To download them automatically, I recommend exploring the FTP Archive link near the bottom of the page. In Census-jargon, the CD directory will have shapefiles for Congressional Districts for each year.6\n\n\n\n\n\n\nTask 2: Download Congressional Shapefiles 1976-2012\n\n\n\nDownload congressional shapefiles from the US Census Bureau for all US Congresses from 2014 to 2024.\nYour download code should:\n\nBe fully automated (no “hand-downloading”);\nDownload files with a systematic and interpretable naming convention\nOnly download files as needed out of courtesy for the data provider’s web sever. That is, if you already have a copy of the file, do not re-download it repeatedly.\n\nAs with the other Mini-Projects, make sure you do not store these data files in git. It will be sufficient to include the qmd file with the download code.\n\n\nInitial Exploration of Vote Count Data\n\n\n\n\n\n\nTask 3: Exploration of Vote Count Data\n\n\n\nAnswer the following using the vote count data files from the MIT Election Data Science Lab. You may answer each with a table or plot as you feel is appropriate.\n\nWhich states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\nNew York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent).\nAre there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\n\n\nDo presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state?\nDoes this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\n\n\n\nImporting and Plotting Shape File Data\nAs mentioned above, the shape files you downloaded above are distributed in zip archives, with several files. We only need the shp file within each archive. In this section, we’ll practice extracting the shp file, reading it, and using it to create a plot. The key library we need is the sf (“simple features”) library. It provides the read_sf() function which we can use to read it into R. I download how this works below:\n\nlibrary(ggplot2)\nlibrary(sf)\n\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n    download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n              destfile=\"nyc_borough_boundaries.zip\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_sf &lt;- read_sf(fname_shp)\nnyc_sf\n\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS84(DD)\n# A tibble: 5 × 5\n  boro_code boro_name      shape_area shape_leng                        geometry\n      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;              &lt;MULTIPOLYGON [°]&gt;\n1         3 Brooklyn      1934142776.    728147. (((-73.86327 40.58388, -73.863…\n2         5 Staten Island 1623618684.    325910. (((-74.05051 40.56642, -74.050…\n3         1 Manhattan      636646082.    360038. (((-74.01093 40.68449, -74.011…\n4         2 Bronx         1187174772.    463181. (((-73.89681 40.79581, -73.896…\n5         4 Queens        3041418004.    888197. (((-73.82645 40.59053, -73.826…\n\n\n\n\n\n\n\n\nTask 4: Automate Zip File Extraction\n\n\n\nAdapt the code after the ##- symbol above into a function read_shp_from_zip() which takes in a file name, pulls out the .shp file contained there in, and reads it into R using read_sf().\n\n\nThe result of this is a particular sort of data frame. The most important column for us is the geometry column which is of type MULTIPOLYGON. This is, essentially, a list of GPS coordinates which outline a spatial region. Here, each row corresponds to a Borough of NYC. We can pass the geometry column to ggplot2 to make a map:\n\nggplot(nyc_sf, \n       aes(geometry=geometry)) + \n    geom_sf()\n\n\n\n\n\n\n\nHere, we use the sf geom to get the shape outlines. The sf geom plays well with the fill aesthetic.\n\nggplot(nyc_sf, \n       aes(geometry=geometry, \n           fill = shape_area)) + \n    geom_sf()\n\n\n\n\n\n\n\nThis type of plot is called a Chloropleth Map and it is commonly used to depict election results.\n\n\n\n\n\n\nTask 5: Chloropleth Visualization of 2000 Electoral College Results\n\n\n\nUsing the data you downloaded earlier, create a chloropleth visualization of the 2000 electoral college results, coloring each state by the party that won the most votes in that state. Your result should look something like this:\n\nTaken from Wikipedia\nIt is not required, but to make the very best plot, you may want to look up:\n\nHow to “inset” Alaska and Hawaii instead of plotitng their true map locations.\nHow to add labels to a chloropleth in ggplot2\n\nHow to label the small states in the North-East\n\nbut these steps are not required as they are a bit advanced.\n\n\n\n\n\n\n\n\nTask 6: Advanced Chloropleth Visualization of Electoral College Results\n\n\n\nModify your previous code to make either an animated version showing election results over time.\n\n\nThe following example may be useful for you:\n\n## Animated Chloropleth using gganimate\n\n## Add some time \"structure\" to our data for \n## demonstration purposes only\nnyc_sf_repeats &lt;- bind_rows(\n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 1), \n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 2), \n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 3), \n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 4), \n    nyc_sf |&gt; mutate(value = rnorm(5), \n                     frame = 5))\n\nlibrary(gganimate)\nggplot(nyc_sf_repeats, \n       aes(geometry=geometry, \n           fill = value)) + \n    geom_sf() + \n    transition_time(frame)\n\n\n\n\n\n\n\nNow that we have finished exploring our data and building some tools for plots, we are ready to dig into our main question."
  },
  {
    "objectID": "miniprojects/mini03.html#comparing-the-effects-of-ecv-allocation-rules",
    "href": "miniprojects/mini03.html#comparing-the-effects-of-ecv-allocation-rules",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Comparing the Effects of ECV Allocation Rules",
    "text": "Comparing the Effects of ECV Allocation Rules\nGo through the historical voting data and assign each state’s ECVs according to various strategies:\n\nState-Wide Winner-Take-All\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nState-Wide Proportional\nNational Proportional\n\nBased on these allocation strategies, compare the winning presidential candidate with the actual historical winner.\nWhat patterns do you see? Are the results generally consistent or are one or more methods systematically more favorable to one party?\nFor the district-level winner-take-all, you may assume that the presidential candidate of the same party as the congressional representative wins that election.\n\n\n\n\n\n\nTask 7: Evaluating Fairness of ECV Allocation Schemes\n\n\n\nWrite a fact check evaluating the fairness of the different ECV electoral allocation schemes.\nTo do so, you should first determine which allocation scheme you consider “fairest”. You should then see which schemes give different results, if they ever do. To make your fact check more compelling, select one election where the ECV scheme had the largest impact–if one exists–and explain how the results would have been different under a different ECV scheme.\nAs you perform your analysis, you may assume that the District of Columbia has three ECVs, which are allocated to the Democratic candidate under all schemes except possibly national popular vote.7\n\n\nThroughout all of this, note that we are not varying the \\(R+2\\) ECV allocation scheme specified by the constitution. Our concern here is only what individual states can do to address “fairness” in presidential elections. If we allow the possibility of constitutional amendment, the possibilities are endless. The \\(R+2\\) rule has several interesting effects; some are well-known, such as the Senate’s equal treatment of small and large states, while others are less well-known, including the fact that congressional representation is based on population, not counts of voters.8"
  },
  {
    "objectID": "miniprojects/mini03.html#extra-credit-opportunity",
    "href": "miniprojects/mini03.html#extra-credit-opportunity",
    "title": "STA/OPR 9750 Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Extra Credit Opportunity",
    "text": "Extra Credit Opportunity\n\n\n\n\n\n\nExtra Credit Opportunity\n\n\n\nFor extra credit, extend your analysis to 2024 electoral results. You will have to find a reliable source of 2024 state- or district-wide vote counts. If the 2024 election is close, this may not be easy to do between the election and the date this mini-project is due.\n\n\n\nThis work ©2024 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  }
]